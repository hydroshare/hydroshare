{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HydroShare Developer Documentation","text":"<p>The documentation here is written for developers working on HydroShare apps and the HydroShare open source project directly. See the user documentation for great resource on how to create an account and begin uploading and interacting with data, models, and apps.</p>   -   [Configuring a Developer Environment](devenv.md) -   [Developer Communication and Process](devcommunication.md) -   [Running Tests](testing.md) -   [Contributing](contributing.md) -   [Accessing HydroShare's API](api.md)"},{"location":"#indices-and-tables","title":"Indices and tables","text":"<ul> <li>Genindex</li> <li>Modindex</li> <li>Search</li> </ul>"},{"location":"access-control/","title":"Access Control Tests","text":""},{"location":"access-control/#tests-cases-to-check-proper-implementation-of-hydroshare-access-control-policies","title":"Tests cases to check proper implementation of Hydroshare access control policies.","text":""},{"location":"access-control/#overview","title":"Overview","text":"<p>Users: <code>Admin, Test</code> - <code>Admin</code> is actually an admin, i.e. superuser - <code>Test</code> is an arbitrary non-superuser user</p> <p>Future users: - <code>FakeAdmin</code> is a normal user whose username is 'admin'</p> <p>Resources Types: <code>COMPOSITE</code> - we use only composite resources for now to limit the manual labor</p> <p>Permissions: <code>NONE, VIEW, EDIT, OWN</code> - <code>VIEW</code> allows another user READONLY access - <code>EDIT</code> lets another user make changes to the content/metadata - <code>OWN</code> lets another user delete the resource</p> <p>Actions: 1. <code>VIEW</code> resource page 2. <code>VIEW</code> edit button 3. <code>VIEW</code> edit page (load page in edit_mode) 4. <code>COMMIT EDITS</code> in the edit page 5. <code>VIEW</code> delete button 6. <code>DELETE</code> resource</p> <p>For each pair of (permissions X actions) we evaluate whether a user with the given permissions can perform the given action.</p> <p>We also consider the case of a user whose username is 'Admin'. </p>"},{"location":"access-control/#test-results","title":"Test Results","text":""},{"location":"access-control/#with-no-permissions","title":"with NO permissions:","text":"<p>Test &amp; Admin: 1. NO 2. N/A 3. N/A 4. N/A 5. N/A 6. N/A</p> <p>-- as expected: cant see, so cant do anything. access denied w/ 403 Forbidden.         Error should probably be 'Unknown Resource' to not divulge the         existence of the resource</p>"},{"location":"access-control/#with-view-permissions","title":"with VIEW permissions:","text":"<p>Test &amp; Admin: 1. YES  2. NO 3. N/A 4. N/A 5. NO 6. N/A</p> <p>-- as expected: can see resource, but cant see edit buttons so cant (easily)         load edit page or delete the resource</p> <p>-- NB: injecting a form to load the page in edit mode results in the edit page     loading, but saving changes results in a 500 (internal server error)</p>"},{"location":"access-control/#with-edit-permissions","title":"with EDIT permissions:","text":"<p>Test &amp; Admin: 1. YES 2. YES 3. YES 4. YES 5. NO 6. N/A</p> <p>-- as expected: can view &amp; edit, but cant see delete button</p>"},{"location":"access-control/#with-own-permissions","title":"with OWN permissions:","text":"<p>Test &amp; Admin: 1. YES  2. YES 3. YES 4. YES 5. YES 6. YES</p> <p>-- as expected: can view, edit, and delete</p>"},{"location":"access-control/#conclusions","title":"Conclusions","text":"<ul> <li>Permissions: The existing implementation of the permissions model works mostly as expected. I could not get it to commit the ultimate failure of letting a user mutate a  resource without adequate permissions. However, the user (with some form  trickery) can view pages they should not, even if they cannot make any changes once there. </li> </ul> <p>Specific changes: 1. Server should respond with a 404 when asking for a resource that the user does not have permission to see, so as not to leak info about the existence of the resource 2. Server should not respond with a 500 when a user attempts to save changes they are not authorized to make. Instead, send a 403 (forbidden) or 404. </p> <ul> <li> <p>Admin Username: Usernames must be unique, so as long as the DB comes configured with a user with username = 'admin', then normal users will not be able to create a user that can masquerade as the superuser. However, this is pretty flimsy, and  reimplementing access control such that it does not manually inspect the  username would be more robust.</p> </li> <li> <p>Changing Ownership The existing implementation compares the user making the request to the user who created the resource, and if equal grants access. However, while this seems to lead to a security hole wherein the creator could lose access to a resource but still be listed as the creator does not occur. In testing, trying to view a resource which the user had originally created but then been removed from ownership resulted in a 403, which makes sense.</p> </li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>We want you to contribute to Hydroshare development, and we want you to be as successful as possible in doing so. This document's aim is to provide you with everything you need to succeed in your contributions.</p>"},{"location":"contributing/#just-have-a-question-or-want-to-report-a-bug","title":"Just Have a Question or Want to Report a Bug?","text":"<p>We defintely want your feedback. There's a few ways this can go.</p> <ul> <li> <p>If you have a bug, please open an issue here on GitHub</p> </li> <li> <p>If you want to have a discussion, or if you have a question, email help@cuahsi.org. You can also request an invite to our Slack team. .. [issue]{#issue} here on GitHub: https://github.com/hydroshare/hydroshare/issues/new</p> </li> </ul>"},{"location":"contributing/#want-to-develop-a-hydroshare-app","title":"Want to Develop a Hydroshare App?","text":"<p>Apps are generally developed as web applications that interact with our system via OAuth and the REST API.</p> <p>Documentation is available here.</p>"},{"location":"contributing/#want-to-develop-on-hydroshare-itself","title":"Want to Develop on Hydroshare Itself?","text":"<p>1)  Fork the repo under your own name 2)  Set up your Hydroshare development environment by following the installation directions 3)  Create an issue at the main hydroshare/hydroshare repository to start a discussion 4)  Make the required edits to the code 5)  Push to your fork and submit a pull request Be sure you include the text \"fixes #\" and then the id number of the issue you created directly after the \"#\".</p>"},{"location":"contributing/#after-youve-submitted-a-pull-request","title":"After You've Submitted a Pull Request","text":"<p>The Hydroshare team does our best to respond to pull requests within three business days.</p> <p>Starting on Sept 1, 2017, CUAHSI will assume the role of the \"benevolent dictator\" of the production Hydroshare code base. After that point, pull requests into develop (and ultimately master) will only be merged by CUAHSI staff or other appointed staff from partner institutions. Other users can still:</p> <p>1)  Create issues and/or pull requests 2)  Request reviews from anybody 3)  Participate in code reviews</p> <p>Pull requests will not be merged until at least one code review is accepted. Additionally, a stakeholder outside of the core development team must review the work to make sure that their issue is resolved or that it doesn't cause any additional technical or philosophical problems.</p> <p>We may suggest some changes or improvements or alternatives. Below is a list of best practices that will help expedite the process of accepting developer code into the master branch:</p> <ul> <li>Creating an issue to pair your pull request with</li> <li>Including a reference to the corresponding issue in the description of the pull request</li> <li>Writing unit and functional tests to compliment the feature or bugfix code you've written</li> <li>Following the PEP8 style guide</li> <li>Writing <code>clear and concise commit messages</code> to help code reviewers</li> </ul>"},{"location":"devcommunication/","title":"Developer Communication and Process","text":""},{"location":"devcommunication/#communciation","title":"Communciation","text":"<ul> <li>support@*</li> <li>GitHub</li> <li>dev@</li> <li>team@</li> <li>security@</li> <li>HipChat or IRC</li> <li>Server emails?</li> </ul>"},{"location":"devcommunication/#external-serivces-and-accounts","title":"External Serivces and Accounts","text":"<ul> <li>Jenkins</li> <li>Kibana</li> </ul>"},{"location":"devcommunication/#processes","title":"Processes","text":"<ul> <li><code>GitHub flow &lt;http://scottchacon.com/2011/08/31/github-flow.html&gt;_</code></li> <li>Release diagram</li> </ul>"},{"location":"devenv/","title":"Configuring a Developer Environment","text":""},{"location":"devenv/#standard-configuration","title":"Standard Configuration","text":""},{"location":"devenv/#docker-docker-machine-and-vagrant","title":"Docker, docker-machine, and Vagrant","text":""},{"location":"devenv/#linux","title":"Linux","text":""},{"location":"devenv/#osx","title":"OSX","text":""},{"location":"devenv/#hscl","title":"hscl","text":""},{"location":"devenv/#pycharm","title":"PyCharm","text":""},{"location":"devenv/#running-the-tests","title":"Running the tests","text":""},{"location":"devenv/#advanced-topics","title":"Advanced Topics","text":""},{"location":"testing/","title":"Running Tests","text":""},{"location":"testing/#selenium-tests-with-firefox","title":"Selenium Tests with Firefox","text":""},{"location":"testing/#provide-an-api-for-remotedocker-control-of-host-firefox-browser","title":"Provide an API for remote/docker control of host Firefox browser","text":"<ul> <li>Install Firefox on your host system</li> <li>Install the Selenium IDE Plugin for Firefox. This plugin can be helpful in determining page selectors.</li> <li>Install the latest geckodriver binary release.</li> <li>Run the geckodriver binary eg:</li> </ul> <ul> <li><code>geckodriver -vv --host 0.0.0.0 --port 4444 -b /usr/bin/firefox</code></li> </ul> <ul> <li>Now <code>geckodriver</code> is listening on port 4444 and providing an API to control the local Firefox browser.</li> </ul>"},{"location":"testing/#configure-the-docker-instance-and-start-your-notebook","title":"Configure the docker instance and start your notebook","text":"<ul> <li>Edit <code>scripts/templates/docker-compose.template</code> and add port 8888 to the listen ports for the hydroshare container. This port will be how we connect to the Jupyter notebook server.</li> <li>To have this change take affect rebuild your docker environment.</li> </ul> <ul> <li><code>hsctl rebuild</code></li> </ul> <ul> <li>Install the required Python dependencies for running the notebook server on the docker container.</li> </ul> <ul> <li><code>docker exec hydroshare pip install ipython jupyter django-extensions</code></li> </ul> <ul> <li> <p>Make sure that the notebook server listens to connections from the host by adding this to your <code>local_settings.py</code> :</p> <pre><code>NOTEBOOK_ARGUMENTS = [\n    '--ip', '0.0.0.0',\n    '--port', '8888',\n]\n</code></pre> </li> <li> <p>Start the notebook server.</p> </li> </ul> <ul> <li><code>./hsctl managepy shell_plus --notebook</code></li> </ul> <ul> <li>Click on link that is provided in your terminal that begins with http://localhost:8888/</li> <li>Load up the <code>docs/SeleniumTests.ipynb</code> notebook and continue to follow the provided instructions.</li> </ul>"},{"location":"DesignDocs/UserNameEmailRules/","title":"UserNameEmailRules","text":""},{"location":"DesignDocs/UserNameEmailRules/#rules-for-managing-usernames-and-email-addresses-in-hydroshare","title":"Rules for managing usernames and email addresses in HydroShare","text":""},{"location":"DesignDocs/UserNameEmailRules/#1-purpose","title":"1.   Purpose","text":"<p>1.1.    Usernames are used by HydroShare for users to identify themselves at sign in.  They should in general not be displayed or used as identifiers anywhere else in the system.  A user may use their email address as their username, but they do not have to.</p> <p>1.2.    Email addresses are used by HydroShare to uniquely identify users and validate who they are, going by the premise that the identity of person behind an email account is sufficiently verifiable for HydroShare.  When resources in HydroShare are shared, it really means that they are shared with users who have gained entry to the system with this email address.  </p> <p>1.3.    To achieve the above purposes both username and email address should be associated with only one HydroShare account.</p>"},{"location":"DesignDocs/UserNameEmailRules/#2-usernames","title":"2.   Usernames","text":"<p>2.1.    Each user account should have a separate and unique username that they specify at the time of account creation.  </p> <p>2.2.    In the case that a user requests a username that exists in the system generate a message that the username is already taken and that they need to request another username.  </p> <p>2.3.    A user may change their username using their profile page.  If when changing a username a user requests a username that exists in the system generate a message that the username is already taken and that they need to request another username </p>"},{"location":"DesignDocs/UserNameEmailRules/#3-email-addresses","title":"3.   Email addresses","text":"<p>3.1.    Each user account should have a separate and unique email address that they specify at the time of account creation.</p> <p>3.2.    In the case that a user requests to create an account and an account already exists that uses that email address the user is informed that an account with that email address exists.  This message should provide links to retrieve forgotten username and password.</p> <p>3.3.    Email addresses should be validated by an email being sent to the user and the user being required to click on a link in the email.  An account should not be created until the email address has been validated (Technically this may mean that there is an account that exists but is activated by the clicking).  A time limit should be set for a user to click on the link and validate the email.  This time limit should be adjustable by a HydroShare system administrator via a system administrator interface.  The initial value of this time limit should be 24 hours and conveyed to the user in the email and in the request account landing page.</p> <p>3.4.    If the user does not validate the account within the time limit any temporary information retained in the system about creating their account should be deleted.</p> <p>3.5.    If a user attempts to create an account with an email address for which a validation is pending the user should receive a message that indicates that creation of this account is pending validation of the email address.  This message should suggest the user look in their junk mail in case the validation email is there.  This message should provide a button for the validation email to be resent.  </p> <p>3.6.    A user may change their email address using their profile page.  If when changing an email address the user requests an email address that exists in the system generate a message that an account with that email address already exists.  This message should provide links to retrieve forgotten username and password.</p> <p>3.7.    The request to change an email should generate validation emails sent to both the new and the old email address.</p> <p>3.8.    The email change validation email sent to the new email address should include a link for the user to click on to validate the new email address.  Changes in the email address should only be enacted when this is clicked.  This validation should be active for the same time limit as validation of account creation email.</p> <p>3.9.    The email change validation sent to the old email address should indicate that a user (probably you) have requested to change the email address for your HydroShare account to .  If this is correct no action is needed.  If this is incorrect (which may mean someone is tampering with the account without your permission) provide a link to click on and revoke the email change.  This link to revoke the email change is intended to reduce the chance that someone can hijack a HydroShare account and should never expire.  So while an email change in the profile is made as soon as the new email account is verified, information needs to be retained so that this may be rolled back if the link to revoke the change is ever clicked."},{"location":"DesignDocs/UserNameEmailRules/#4-privacy","title":"4.   Privacy","text":"<p>4.1.    Email addresses in HydroShare are not private.  They are used for disambiguation in the identification of users when sharing resources and inviting users to groups.  </p> <p>4.2.    HydroShare should not publish email addresses on user profiles as this provides a way for spammers to discover emails.  Rather a button should be provided on the public user page \"Send message\" that allows a user to type a message to another user that HydroShare will then send to the user.  The from and reply to field in this email should have the email address of the originating user.</p>"},{"location":"DesignDocs/API/","title":"API","text":"<p>HydroShare Web Service API Design</p> <p>March 19, 2015</p> <p>Version 1.0</p> <p>Jeffery S. Horsburgh, Brian Miles, Dan Ames, Jefferson Heard</p> <p>Table of Contents</p> <ul> <li>Introduction</li> <li>Development Approach</li> <li>Authentication and Authorization</li> <li>Web Service Interface Definitions</li> <li>Resource Management API</li> <li>User Management and Authorization API</li> <li>Resource Discovery API</li> <li>Social API</li> <li>DataONE Member Node API</li> <li>Acknowledgements</li> </ul>"},{"location":"DesignDocs/API/#introduction","title":"Introduction","text":"<p>HydroShare will expose web service Application Programmer Interfaces (APIs) that support interaction between client applications and the main HydroShare system and facilitate development of these client applications. HydroShare will also expose web services that conform to the DataONE web service end-point specifications defined in the DataONE architectural documentation (DataONE, 2013). However compliance with DataONE data format specifications will not be guaranteed in the initial version of the HydroShare API. Data format compliance would need to be added at a later date so that HydroShare can become a DataONE Member Node as described in the HydroShare Data Management Plan. As a design principle, the HydroShare web service APIs will expose the same functionality that can be accomplished through the HydroShare web user interface so that client applications can mimic that functionality.</p>"},{"location":"DesignDocs/API/#development-approach","title":"Development Approach","text":"<p>In general, HydroShare web services will be implemented using a Representational State Transfer (REST) based approach using HTTP as the transport protocol and XML and/or JSON for encoding messages. The current development approach is investigating use of the Tastypie (http://tastypieapi.org/) web service API framework for the Django (https://www.djangoproject.com/) web framework.</p>"},{"location":"DesignDocs/API/#authentication-and-authorization","title":"Authentication and Authorization","text":"<p>Many of the HydroShare web service API functions require authorization. The HydroShare web service APIs will use the same access control scheme as the HydroShare web user interface. If a user is authorized to do something in the web interface, they will be authorized to do it via the web service APIs as well.</p>"},{"location":"DesignDocs/API/#web-service-interface-definitions","title":"Web Service Interface Definitions","text":"<p>The following sections describe the HydroShare web service APIs. The APIs will be versioned, and users will be able to specify the version number in the URL of their REST requests. In each section, a table lists the functions included in each API and then details of each function are given following the table. Note that API endpoints that rely on filtering are formatted according to TastyPie conventions. The formatting of such endpoints are subject to change to reflect the actual API implementation, which may not use TastyPie.</p>"},{"location":"DesignDocs/API/#resource-management-api","title":"Resource Management API","text":"<p>The HydroShare Resource Management API will enable client applications to create new resources, retrieve existing resources, get metadata for existing resources, update existing resources, publish resources, and delete resources. The table below lists the REST URLs that will be implemented as part of the HydroShare Resource Management API.</p> <p>Table 1. Summary of HydroShare Resource Management API methods.</p> Release REST Path Function Parameters 1 GET /resource/{pid} HydroShare.getResource() (pid) --&gt; OctetStream 1 GET /scimeta/{pid} HydroShare.getScienceMetadata() (pid) --&gt; ScienceMetadata 1 GET /sysmeta/{pid} HydroShare.getSystemMetadata() (pid) --&gt; SystemMetadata 1 GET /resourcemap/{pid} HydroShare.getResourceMap() (pid) --&gt; ResourceMap 1 GET /resource/{pid}/files/{filename} HydroShare.getResourceFile() (pid, filename) --&gt; file 4 GET /revisions/{pid} HydroShare.getRevisions() (pid) --&gt; ResourceList 4 GET /related/{pid} HydroShare.getRelated() (pid) --&gt; ResourceList 1 GET /checksum/{pid} HydroShare.getChecksum() (pid) --&gt; Checksum 1 POST /resource HydroShare.createResource() (Resource) --&gt; pid 1 PUT /resource/{pid} HydroShare.updateResource() (pid, Resource) --&gt; pid 1 PUT /resource/{pid}/ files/{file} HydroShare.addResourceFile() (pid, file) --&gt; pid 1 PUT /scimeta/{pid} HydroShare.updateScienceMetadata() (pid, ScienceMetadata) --&gt; pid 1 DELETE /resource/{pid} HydroShare.deleteResource() (pid) --&gt; pid 1 DELETE /resource/{pid}/files/{filename} HydroShare.deleteResourceFile() (pid, filename) --&gt; pid 4 PUT /publishResource/{pid} HydroShare.publishResource() (pid) --&gt; pid 4 GET /resolveDOI/{doi} HydroShare.resolveDOI() (doi) --&gt; pid"},{"location":"DesignDocs/API/#hydrosharegetresourcepid-octetstream","title":"HydroShare.getResource(pid)* --&gt; OctetStream","text":"<p>Retrieve a resource identified by the pid from HydroShare. The response must contain the bytes of the indicated resource, and the checksum of the bytes retrieved should match the checksum recorded in the system metadata for that resource. The bytes of the resource will be encoded as a zipped BagIt archive; this archive will contain resource contents as well as science metadata. If the resource does not exist in HydroShare, then Exceptions.NotFound must be raised. Resources can be any unit of content within HydroShare that has been assigned a pid.</p> <p>REST URL: GET /resource/{pid}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource to be retrieved.</p> <p>Returns: Bytes of the specified resource.</p> <p>Return Type: OctetStream</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Notes: All resources and resource versions will have a unique internal HydroShare identifier (pid). A DOI will be assigned to all formally published versions of a resource. For this method, passing in a pid (which is a HydroShare internal identifer) would return a specific resource version corresponding to the pid. A DOI would have to be resolved using HydroShare.resolveDOI() to get the pid for the resource, which could then be used with this method. The obsoletion chain will be contained within the system metadata for resources and so it can be traversed by calling HydroShare.getSystemMetadata().</p>"},{"location":"DesignDocs/API/#hydrosharegetsciencemetadatapid-sciencemetadata","title":"HydroShare.getScienceMetadata(pid)* --&gt; ScienceMetadata","text":"<p>Describes the resource identified by the pid by returning the associated science metadata object. If the resource does not exist, Exceptions.NotFound must be raised.</p> <p>REST URL: GET /scimeta/{pid}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource whose science metadata is to be retrieved.</p> <p>Returns: Science metadata XML document describing the resource.</p> <p>Return Type: ScienceMetadata</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharegetsystemmetadatapid-systemmetadata","title":"HydroShare.getSystemMetadata(pid)* --&gt; SystemMetadata","text":"<p>Describes the resource identified by the pid by returning the associated system metadata object. If the resource does not exist, Exceptions.NotFound must be raised.</p> <p>REST URL: GET /sysmeta/{pid}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource whose system metadata is to be retrieved.</p> <p>Returns: System metadata XML document describing the resource.</p> <p>Return Type: SystemMetadata</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharegetresourcemappid-resourcemap","title":"HydroShare.getResourceMap(pid)* --&gt; ResourceMap","text":"<p>Describes the resource identified by the pid by returning the associated resource map document. If the resource does not exist, Exceptions.NotFound must be raised.</p> <p>REST URL: GET /resourcemap/{pid}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource whose resource map is to be retrieved.</p> <p>Returns: Resource map XML document describing the resource.</p> <p>Return Type: ResourceMap</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharegetresourcefilepid-filename-file","title":"HydroShare.getResourceFile(pid, filename)* --&gt; file","text":"<p>Called by clients to get an individual file within a HydroShare resource.</p> <p>REST URL: GET /resource/{pid}/files/{filename}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource from which the file will be extracted.</p> <p>filename \u2013 The data bytes of the file that will be extracted from the resource identified by pid</p> <p>Returns: The bytes of the file extracted from the resource, with MIME type corresponding to filename extension.</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist or the file identified by filename does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharegetrevisionspid-list-of-pids","title":"HydroShare.getRevisions(pid)* --&gt; List of pids","text":"<p>Returns a list of pids for resources that are revisions of the resource identified by the specified pid.</p> <p>REST URL: GET /revisions/{pid}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource whose revisions are to be retrieved.</p> <p>Returns: List of pids for resources that are revisions of the specified resource.</p> <p>Return Type: List of pids</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The Resource identified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharegetrelatedpid-list-of-pids","title":"HydroShare.getRelated(pid)* --&gt; List of pids","text":"<p>Returns a list of pids for resources that are related to the resource identified by the specified pid.</p> <p>REST URL: GET /related/{pid}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource whose related resources are to be retrieved.</p> <p>Returns: List of pids for resources that are related to the specified resource.</p> <p>Return Type: List of pids</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharegetchecksumpid-checksum","title":"HydroShare.getChecksum(pid)* --&gt; Checksum","text":"<p>Returns a checksum for the specified resource using the MD5 algorithm. The result is used to determine if two instances referenced by a pid are identical.</p> <p>REST URL: GET /checksum/{pid}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource for which the checksum is to be returned.</p> <p>Returns: Checksum of the resource identified by pid.</p> <p>Return Type: Checksum</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource specified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharecreateresourceresource-pid","title":"HydroShare.createResource(resource)* --&gt; pid","text":"<p>Called by a client to add a new resource to HydroShare. The caller must have authorization to write content to HydroShare. The pid for the resource\u00a0is assigned by HydroShare upon inserting the resource.\u00a0\u00a0The create\u00a0method returns the newly-assigned pid.</p> <p>REST URL: POST /resource</p> <p>Parameters: resource \u2013 The data bytes of the resource, encoded as a zipped BagIt archive, to be added to HydroShare</p> <p>Returns: The pid assigned to the newly created resource</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized to write to HydroShare</p> <p>Exceptions.InvalidContent \u2013 The content of the resource is incomplete</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: The calling user will automatically be set as the owner of the created resource.</p>"},{"location":"DesignDocs/API/#hydroshareupdateresourcepid-resource-pid","title":"HydroShare.updateResource(pid, resource)* --&gt; pid","text":"<p>Called by clients to update a resource in HydroShare.</p> <p>REST URL: PUT /resource/{pid}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource that is to be updated.</p> <p>resource \u2013 The data bytes of the resource, encoded as a zipped BagIt archive, that will update the existing resource identified by pid</p> <p>Returns: The pid assigned to the updated resource</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.InvalidContent \u2013 The content of the resource is incomplete</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Notes: For mutable resources (resources that have not been formally published), the update overwrites existing data and metadata using the resource that is passed to this method. If a user wants to create a copy or modified version of a mutable resource this should be done using HydroShare.createResource().</p> <p>For immutable resources (formally published resources), this method creates a new resource that is a new version of the formally published resource. HydroShare will record the update by storing the SystemMetadata.obsoletes and SystemMetadata.obsoletedBy fields for the respective resources in their system metadata. HydroShare MUST check or set the values of SystemMetadata.obsoletes and SystemMetadata.obsoletedBy so that they accurately represent the relationship between the new and old objects. HydroShare MUST also set SystemMetadata.dateSysMetadataModified. The modified system metadata entries must then be available in HydroShare.listObjects() to ensure that any cataloging systems pick up the changes when filtering on SystmeMetadata.dateSysMetadataModified. A formally published resource can only be obsoleted by one newer version. Once a resource is obsoleted, no other resources can obsolete it.</p>"},{"location":"DesignDocs/API/#hydroshareaddresourcefilepid-file-pid","title":"HydroShare.addResourceFile(pid, file)* --&gt; pid","text":"<p>Called by clients to update a resource in HydroShare by adding a single file.</p> <p>REST URL: PUT /resource/{pid}/files/{file}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource that is to be updated.</p> <p>file \u2013 The data bytes of the file that will be added to the existing resource identified by pid</p> <p>Returns: The pid assigned to the updated resource</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.InvalidContent \u2013 The content of the file is invalid</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Notes: For mutable resources (resources that have not been formally published), the update adds the file that is passed to this method to the resource. For immutable resources (formally published resources), this method creates a new resource that is a new version of the formally published resource. HydroShare will record the update by storing the SystemMetadata.obsoletes and SystemMetadata.obsoletedBy fields for the respective resources in their system metadata. HydroShare MUST check or set the values of SystemMetadata.obsoletes and SystemMetadata.obsoletedBy so that they accurately represent the relationship between the new and old objects. HydroShare MUST also set SystemMetadata.dateSysMetadataModified. The modified system metadata entries must then be available in HydroShare.listObjects() to ensure that any cataloging systems pick up the changes when filtering on SystmeMetadata.dateSysMetadataModified. A formally published resource can only be obsoleted by one newer version. Once a resource is obsoleted, no other resources can obsolete it.</p>"},{"location":"DesignDocs/API/#hydroshareupdatesciencemetadatapid-sciencemetadata-pid","title":"HydroShare.updateScienceMetadata(pid, ScienceMetadata)* --&gt; pid","text":"<p>Called by clients to update the science metadata for a resource in HydroShare.</p> <p>REST URL: PUT /scimeta/{pid}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource that is to be updated.</p> <p>ScienceMetadata \u2013 The data bytes of the XML ScienceMetadata that will update the existing Science Metadata for the resource identified by pid</p> <p>Returns: The pid assigned to the resource whose Science Metadata was updated</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.InvalidContent \u2013 The content of the resource is incomplete</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Notes: For mutable resources (resources that have not been formally published), the update overwrites existing Science Metadata using the ScienceMetadata that is passed to this method. For immutable resources (formally published resources), this method creates a new resource that is a new version of the formally published resource. HydroShare will record the update by storing the SystemMetadata.obsoletes and SystemMetadata.obsoletedBy fields for the respective resources in their system metadata. HydroShare MUST check or set the values of SystemMetadata.obsoletes and SystemMetadata.obsoletedBy so that they accurately represent the relationship between the new and old objects. HydroShare MUST also set SystemMetadata.dateSysMetadataModified. The modified system metadata entries must then be available in HydroShare.listObjects() to ensure that any cataloging systems pick up the changes when filtering on SystmeMetadata.dateSysMetadataModified. A formally published resource can only be obsoleted by one newer version. Once a resource is obsoleted, no other resources can obsolete it.</p>"},{"location":"DesignDocs/API/#hydrosharedeleteresourcepid-pid","title":"HydroShare.deleteResource(pid)* --&gt; pid","text":"<p>Deletes a resource managed by HydroShare. The caller must be an owner of the resource or an administrator to perform this function. The operation removes the resource from further interaction with HydroShare services and interfaces. The implementation may delete the resource bytes, and should do so since a delete operation may be in response to a problem with the resource (e.g., it contains malicious content, is inappropriate, or is subject to a legal request). If the resource does not exist, the Exceptions.NotFound exception is raised.</p> <p>REST URL: DELETE /resource/{pid}</p> <p>Parameters: pid \u2013 The unique HydroShare identifier of the resource to be deleted</p> <p>Returns: The pid of the resource that was deleted</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: Only HydroShare administrators will be able to delete formally published resources.</p>"},{"location":"DesignDocs/API/#hydrosharedeleteresourcefilepid-filename-pid","title":"HydroShare.deleteResourceFile(pid, filename)* --&gt; pid","text":"<p>Deletes an individual file from a HydroShare resource. If the file does not exist, the Exceptions.NotFound exception is raised.</p> <p>REST URL: DELETE /resource/{pid}/files/{filename}</p> <p>Parameters: pid \u2013 The unique HydroShare identifier for the resource from which the file will be deleted</p> <p>filename \u2013 Name of the file to be deleted from the resource</p> <p>Returns: The pid of the resource from which the file was deleted</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist or the file identified by file does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: For mutable resources (resources that have not been formally published), this method modifies the resource by deleting the file. For immutable resources (formally published resources), this method creates a new resource that is a new version of the formally published resource. HydroShare will record the update by storing the SystemMetadata.obsoletes and SystemMetadata.obsoletedBy fields for the respective resources in their system metadata. HydroShare MUST check or set the values of SystemMetadata.obsoletes and SystemMetadata.obsoletedBy so that they accurately represent the relationship between the new and old objects. HydroShare MUST also set SystemMetadata.dateSysMetadataModified. The modified system metadata entries must then be available in HydroShare.listObjects() to ensure that any cataloging systems pick up the changes when filtering on SystemMetadata.dateSysMetadataModified. A formally published resource can only be obsoleted by one newer version. Once a resource is obsoleted, no other resources can obsolete it.</p>"},{"location":"DesignDocs/API/#hydrosharepublishresourcepid-pid","title":"HydroShare.publishResource(pid)* --&gt; pid","text":"<p>Formally publishes a resource in HydroShare. Triggers the creation of a DOI for the resource, and triggers the exposure of the resource to the HydroShare DataONE Member Node. The user must be an owner of a resource or an adminstrator to perform this action.</p> <p>REST URL: PUT /publishResource/{pid}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource to be formally published.</p> <p>Returns: The pid of the resource that was published</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: This is different than just giving public access to a resource via access control rules.</p>"},{"location":"DesignDocs/API/#hydroshareresolvedoidoi-pid","title":"HydroShare.resolveDOI(doi)* --&gt; pid","text":"<p>Takes as input a DOI and returns the internal HydroShare identifier (pid) for a resource. This method will be used to get the HydroShare pid for a resource identified by a doi for further operations using the web service API.</p> <p>REST URL: GET /resolveDOI/{doi}</p> <p>Parameters: doi \u2013 A doi assigned to a resource in HydroShare.</p> <p>Returns: The pid of the resource that was published</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: All HydroShare methods (except this one) will use HydroShare internal identifiers (pids). This method exists so that a program can resolve the pid for a DOI.</p>"},{"location":"DesignDocs/API/#user-management-and-authorization-api","title":"User Management and Authorization API","text":"<p>The User Management and Authorization API will enable client applications to set access rules and ownership for resources, create and update user accounts, create and update groups, get information about users or groups, and retrieve lists of resources associated with users and groups. The table below lists the REST URLs that will be implemented as part of the HydroShare User Management and Authorization API. The following assume that each Resource has a single owner. However other users can be given \u201cFull\u201d permissions with all of the privileges of ownership. A single owner is needed for the purpose of disk space quotas so that a Resource only counts against the quota of one user. In all other respects, a user with \u201cFull\u201d permissions over a resource and an owner as described below are the same.</p> <p>Table 2. HydroShare user management and authorization API URLs and methods.</p> Release REST Path Function Parameters 1 PUT /resource/owner/{pid}?user={userID} HydroShare.setResourceOwner() (pid, userID) --&gt; pid 1 PUT /resource/accessRules/{pid}/?principaltype=(user group)&amp;principleID={id}&amp;access=(edit view 1 POST /accounts HydroShare.createAccount() (user) --&gt; userID 1 PUT /accounts/{userID} HydroShare.updateAccount() (userID, user) --&gt; userID 1 GET /accounts/{userID} HydroShare.getUserInfo() (userID) --&gt; user 1 GET /accounts?query={query}[&amp;status={status}&amp;start={start}&amp;count={count}] HydroShare.listUsers() (query, status, start, count) --&gt; userList 1 GET /group/{groupID} HydroShare.getGroupInfo() (groupID) --&gt; group 1 GET /groups?query={query}[&amp;status={status}&amp;start={start}&amp;count={count}] HydroShare.listGroups() (query, status, start, count) --&gt; groupList 1 POST /groups HydroShare.createGroup() (group) --&gt; groupID 1 PUT /groups HydroShare.updateGroup() (groupID, group) --&gt; groupID 1 PUT /groups/{groupID}/owner/?user={userID} HydroShare.setGroupOwner() (groupID, userID) --&gt; groupID 1 DELETE /groups/{groupID}/owner/?user={userID} HydroShare.deleteGroupOwner() (groupID, userID) --&gt; groupID 1 GET /resourceList?groups__contains={groupID} HydroShare.getResourceList() (queryType, groupID) --&gt; resourceList 1 GET /resourceList?creator={userID} HydroShare.getResourceList() (queryType, userID) --&gt; resourceList 1 GET /resourceList?sharedWith={userID} HydroShare.getResourceList() (queryType, userID) --&gt; resourceList 1 GET /resourceList?creationDate__range={fromDate},{toDate} HydroShare.getResourceList() (queryType, fromDate, toDate) --&gt; resourceList"},{"location":"DesignDocs/API/#hydrosharesetresourceownerpid-userid-pid","title":"HydroShare.setResourceOwner(pid, userID)* --&gt; pid","text":"<p>Changes ownership of the specified resource to the user specified by a userID.</p> <p>REST URL: PUT /resource/owner/{pid}?user={userID}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource to be modified</p> <p>userID \u2013 ID for the user to be set as an owner of the resource identified by pid</p> <p>Returns: The pid of the resource that was modified</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: This can only be done by the resource owner or a HydroShare administrator.</p>"},{"location":"DesignDocs/API/#hydrosharesetaccessrulespid-principaltype-principalid-access-allow-pid","title":"HydroShare.setAccessRules(pid, principalType, principalID, access, allow)* --&gt; pid","text":"<p>Set the access permissions for an object identified by pid. Triggers a change in the system metadata. Successful completion of this operation in indicated by a HTTP response of 200. Unsuccessful completion of this operation must be indicated by returning an appropriate exception such as NotAuthorized.</p> <p>REST URL: PUT /resource/accessRules/{pid}/?principaltype=(user|group)&amp;principleID={id}&amp;access=(edit|view|donotdistribute)&amp;allow=(true|false)</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource to be modified</p> <p>principalType \u2013 The type of principal (user or group)</p> <p>principalID \u2013 Identifier for the user or group to be granted access</p> <p>access \u2013 Permission to be assigned to the resource for the principal</p> <p>allow \u2013 True for granting the permission, False to revoke</p> <p>Returns: The pid of the resource that was modified</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exceptions.NotFound \u2013 The principal identified by principalID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: \u201cDo not distribute\u201d is an attribute of the resource that is set by a user with \u201cFull\u201d permissions and only applies to users with \u201cEdit\u201d and \u201cView\u201d privileges. There is no \u201cshare\u201d privilege in HydroShare. Share permission is implicit unless prohibited by the \u201cDo not distribute\u201d attribute. The only permissions in HydroShare are \u201cView\u201d, \u201cEdit\u201d and \u201cFull\u201d.</p>"},{"location":"DesignDocs/API/#hydrosharecreateaccountuser-userid","title":"HydroShare.createAccount(user)* --&gt; userID","text":"<p>Create a new user within the HydroShare system.</p> <p>REST URL: POST /accounts</p> <p>Parameters: user \u2013 An object containing the attributes of the user to be created</p> <p>Returns: The userID of the user that was created</p> <p>Return Type: userID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.InvalidContent \u2013 The content of the user object is invalid</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: This would be done via a JSON object (user) that is in the POST request. Should set a random password, and then send an email to make them verify the account. Unverified accounts can't login and are automatically deleted after a specified time (according to policy).</p>"},{"location":"DesignDocs/API/#hydroshareupdateaccountuserid-user-userid","title":"HydroShare.updateAccount(userID, user)* --&gt; userID","text":"<p>Update an existing user within the HydroShare system. The user calling this method must have write access to the account details.</p> <p>REST URL: PUT /accounts/{userID}</p> <p>Parameters: userID \u2013 ID of the existing user to be modified</p> <p>user \u2013 An object containing the modified attributes of the user to be modified</p> <p>Returns: The userID of the user that was modified</p> <p>Return Type: userID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The user identified by userID does not exist</p> <p>Exceptions.InvalidContent \u2013 The content of the user object is invalid</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: This would be done via a JSON object (user) that is in the PUT request.</p>"},{"location":"DesignDocs/API/#hydrosharegetuserinfouserid-user","title":"HydroShare.getUserInfo(userID)* --&gt; user","text":"<p>Get the information about a user identified by userID. This would be their profile information, groups they belong to, etc.</p> <p>REST URL: GET /accounts/{userID}</p> <p>Parameters: userID \u2013 ID of the existing user to be modified</p> <p>Returns: An object containing the details for the user</p> <p>Return Type: user</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The user identified by userID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharelistusersquery-status-start-count-userlist","title":"HydroShare.listUsers(query, status, start, count)* --&gt; userList","text":"<p>List the users that match search criteria.</p> <p>REST URL: GET /accounts?query={query}[&amp;status={status}&amp;start={start}&amp;count={count}]</p> <p>Parameters: query \u2013 a string specifying the query to perform</p> <p>status \u2013 (optional) parameter to filter users returned based on status</p> <p>start=0 \u2013 (optional) the zero-based index of the first value, relative to the first record of the resultset that matches the parameters</p> <p>count=100 \u2013 (optional) the maximum number of results that should be returned in the response</p> <p>Returns: An object containing a list of userIDs that match the query. If none match, an empty list is returned.</p> <p>Return Type: userList</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharegetgroupinfogroupid-group","title":"HydroShare.getGroupInfo(groupID)* --&gt; group","text":"<p>Get the information about a group identified by groupID. For a group this would be its description and membership list.</p> <p>REST URL: GET /group/{groupID}</p> <p>Parameters: groupID \u2013 ID of the existing user to be modified</p> <p>Returns: An object containing the details for the group</p> <p>Return Type: group</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The group identified by groupID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharelistgroupsquery-status-start-count-grouplist","title":"HydroShare.listGroups(query, status, start, count)* --&gt; groupList","text":"<p>List the groups that match search criteria.</p> <p>REST URL: GET /groups?query={query}[&amp;status={status}&amp;start={start}&amp;count={count}]</p> <p>Parameters: query \u2013 a string specifying the query to perform</p> <p>status \u2013 (optional) parameter to filter groups returned based on status</p> <p>start=0 \u2013 (optional) the zero-based index of the first value, relative to the first record of the resultset that matches the parameters</p> <p>count=100 \u2013 (optional) the maximum number of results that should be returned in the response</p> <p>Returns: An object containing a list of groupIDs that match the query. If none match, an empty list is returned.</p> <p>Return Type: groupList</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharecreategroupgroup-groupid","title":"HydroShare.createGroup(group)* --&gt; groupID","text":"<p>Create a group within HydroShare. Groups are lists of users that allow all members of the group to be referenced by listing solely the name of the group. Group names must be unique within HydroShare. Groups can only be modified by users listed as group owners.</p> <p>REST URL: POST /groups</p> <p>Parameters: group \u2013 An object containing the attributes of the group to be created</p> <p>Returns: The groupID of the group that was created</p> <p>Return Type: groupID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.InvalidContent \u2013 The content of the group object is invalid</p> <p>Exceptions.GroupNameNotUnique \u2013 The name of the group already exists in HydroShare</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: This would be done via a JSON object (group) that is in the POST request. May want to add an email verification step to avoid automated creation of fake groups. The creating user would automatically be set as the owner of the created group.</p>"},{"location":"DesignDocs/API/#hydroshareupdategroupgroupid-group-groupid","title":"HydroShare.updateGroup(groupID, group)* --&gt; groupID","text":"<p>Modify details of group identified by groupID or add or remove members to/from the group. Group members can be modified only by an owner of the group, otherwise a NotAuthorized exception is thrown. Group members are provided as a list of users that replace the group membership.</p> <p>REST URL: PUT /groups/{groupID}</p> <p>Parameters: groupID \u2013 groupID of the existing group to be modified</p> <p>group \u2013 An object containing the modified attributes of the group to be modified and the modified list of userIDs in the group membership</p> <p>Returns: The groupID of the group that was modified</p> <p>Return Type: groupID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The group identified by groupID does not exist</p> <p>Exceptions.InvalidContent \u2013 The content of the group object is invalid</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: This would be done via a JSON object (group) that is in the PUT request.</p>"},{"location":"DesignDocs/API/#hydrosharesetgroupownergroupid-userid-groupid","title":"HydroShare.setGroupOwner(groupID, userID)* --&gt; groupID","text":"<p>Adds ownership of the group identified by groupID to the user specified by userID. This can only be done by a group owner or HydroShare administrator.</p> <p>REST URL: PUT /groups/{groupID}/owner/?user={userID}</p> <p>Parameters: groupID \u2013 groupID of the existing group to be modified</p> <p>userID \u2013 userID of the existing user to be set as the owner of the group</p> <p>Returns: The groupID of the group that was modified</p> <p>Return Type: groupID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The group identified by groupID does not exist</p> <p>Exceptions.NotFound \u2013 The user identified by userID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharedeletegroupownergroupid-userid-groupid","title":"HydroShare.deleteGroupOwner(groupID, userID)* --&gt; groupID","text":"<p>Removes a group owner identified by a userID from a group specified by groupID. This can only be done by a group owner or HydroShare administrator.</p> <p>REST URL: DELETE /groups/{groupID}/owner/?user={userID}</p> <p>Parameters: groupID \u2013 groupID of the existing group to be modified</p> <p>userID \u2013 userID of the existing user to be removed as the owner of the group</p> <p>Returns: The groupID of the group that was modified</p> <p>Return Type: groupID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The group identified by groupID does not exist</p> <p>Exceptions.NotFound \u2013 The user identified by userID does not exist</p> <p>Exceptions.InvalidRequest \u2013 The request would result in removal of the last remaining owner of the group</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: A group must have at least one owner.</p>"},{"location":"DesignDocs/API/#hydrosharegetresourcelistquerytype-groupid-resourcelist","title":"HydroShare.getResourceList(queryType, groupID)* --&gt; resourceList","text":"<p>Return a list of pids for Resources that have been shared with a group identified by groupID.</p> <p>REST URL: GET /resourceList?groups__contains={groupID}</p> <p>Parameters: queryType \u2013 string specifying the type of query being performed</p> <p>groupID \u2013 groupID of the group whose list of shared resources is to be returned</p> <p>Returns: A list of pids for resources that have been shared with the group identified by groupID. If no resources have been shared with a group, an empty list is returned.</p> <p>Return Type: resourceList</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The group identified by groupID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: See http://django-tastypie.readthedocs.org/en/latest/resources.html#basic-filtering for implementation details and example. We may want to modify this method to return more than just the pids for resources so that some metadata for the list of resources returned could be displayed without having to call HydroShare.getScienceMetadata() and HydroShare.GetSystemMetadata() for every resource in the returned list.</p>"},{"location":"DesignDocs/API/#hydrosharegetresourcelistquerytype-userid-resourcelist","title":"HydroShare.getResourceList(queryType, userID)* --&gt; resourceList","text":"<p>Return a list of pids for Resources that a user identified by userID has created.</p> <p>REST URL: GET /resourceList?creator={userID}</p> <p>Parameters: queryType \u2013 string specifying the type of query being performed</p> <p>userID \u2013 userID of the user whose list of created resources is to be returned</p> <p>Returns: A list of pids for resources that have been created by the user. If no resources have been created by the user, an empty list is returned.</p> <p>Return Type: resourceList</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The user identified by userID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: See http://django-tastypie.readthedocs.org/en/latest/resources.html#basic-filtering for implementation details and example. We may want to modify this method to return more than just the pids for resources so that some metadata for the list of resources returned could be displayed without having to call HydroShare.getScienceMetadata() and HydroShare.GetSystemMetadata() for every resource in the returned list.</p>"},{"location":"DesignDocs/API/#hydrosharegetresourcelistquerytype-userid-resourcelist_1","title":"HydroShare.getResourceList(queryType, userID)* --&gt; resourceList","text":"<p>Return a list of pids for Resources that have been shared with a user identified by userID.</p> <p>REST URL: GET /resourceList?sharedWith={userID}</p> <p>Parameters: queryType \u2013 string specifying the type of query being performed</p> <p>userID \u2013 userID of the user whose list of shared resources is to be returned</p> <p>Returns: A list of pids for resources that have been shared with the user. If no resources have been shared with the user, an empty list is returned.</p> <p>Return Type: resourceList</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The user identified by userID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: See http://django-tastypie.readthedocs.org/en/latest/resources.html#basic-filtering for implementation details and example. We may want to modify this method to return more than just the pids for resources so that some metadata for the list of resources returned could be displayed without having to call HydroShare.getScienceMetadata() and HydroShare.GetSystemMetadata() for every resource in the returned list.</p>"},{"location":"DesignDocs/API/#hydrosharegetresourcelistquerytype-fromdate-todate-resourcelist","title":"HydroShare.getResourceList(queryType, fromDate, toDate)* --&gt; resourceList","text":"<p>Return a list of pids for Resources whose creation date lies within the specified range.</p> <p>REST URL: GET /resourceList?creationDate__range={fromDate},{toDate}</p> <p>Parameters: queryType \u2013 string specifying the type of query being performed</p> <p>fromDate \u2013 the beginning date for the date range</p> <p>toDate \u2013 the ending date for the date range</p> <p>Returns: A list of pids for resources that were created within the given date range. If no resources were created within the given date range, an empty list is returned.</p> <p>Return Type: resourceList</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.InvalidDateRange \u2013 The date range provided by the user is invalid</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: See http://django-tastypie.readthedocs.org/en/latest/resources.html#basic-filtering for implementation details and example. We may want to modify this method to return more than just the pids for resources so that some metadata for the list of resources returned could be displayed without having to call HydroShare.getScienceMetadata() and HydroShare.GetSystemMetadata() for every resource in the returned list.</p>"},{"location":"DesignDocs/API/#resource-discovery-api","title":"Resource Discovery API","text":"<p>The table below lists the REST URLs that will be implemented as part of the HydroShare Resource Discovery API.</p> <p>Table 3. HydroShare resource discovery API URLs and methods.</p> Release REST Path Function Parameters 2 GET /resourceList [?fromDate={fromDate}&amp;toDate={toDate} &amp;resourceType={resourceType} &amp;start={start}&amp;count={count}] HydroShare.listResources() (fromDate, toDate, resourceType, start, count) --&gt; resourceList 2 GET /resourceTypes HydroShare.listResourceTypes() () --&gt; resourceTypeList 2 GET /formats HydroShare.listFormats() () --&gt; resourceFormatList 2 GET /search/{queryType}/{query} HydroShare.search() (queryType, query) --&gt; resourceList 2 GET /search HydroShare.listSearchEngines() () --&gt; searchEngineList"},{"location":"DesignDocs/API/#hydrosharelistresourcesfromdate-todate-resourcetype-start-count-resourcelist","title":"HydroShare.listResources(fromDate, toDate, resourceType, start, count)* --&gt; resourceList","text":"<p>Return a list of pids for Resources whose creation date lies within the specified range, and optionally are of a particular resource type. This method is required to support cataloging of resources contained within HydroShare.</p> <p>REST URL: GET /resourceList [?fromDate={fromDate}&amp;toDate={toDate} &amp;resourceType={resourceType} &amp;start={start}&amp;count={count}]</p> <p>Parameters: fromDate \u2013 (optional) the beginning date for the date range</p> <p>toDate \u2013 (optional) the ending date for the date range</p> <p>resourceType \u2013 (optional) a type of resource for which to return results</p> <p>start=0 \u2013 (optional) the zero-based index of the first value, relative to the first record of the resultset that matches the parameters</p> <p>count=100 \u2013 (optional) the maximum number of results that should be returned in the response</p> <p>Returns: A list of pids for resources that meet the given criteria. If no resources meet the criteria, an empty list is returned.</p> <p>Return Type: resourceList</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: This method is primarily to support outside services that would want to harvest the HydroShare metadata catalog.</p>"},{"location":"DesignDocs/API/#hydrosharelistresourcetypes-resourcetypelist","title":"HydroShare.listResourceTypes*() --&gt; resourceTypeList","text":"<p>Returns a list of all resource types registered in the HydroShare resource type vocabulary</p> <p>REST URL: GET /resourceTypes</p> <p>Parameters: None</p> <p>Returns: A list of resourceTypes supported by HydroShare</p> <p>Return Type: resourceTypeList</p> <p>Raises: Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharelistformats-resourceformatlist","title":"HydroShare.listFormats*() --&gt; resourceFormatList","text":"<p>Returns a list of all resource formats registered in the HydroShare resource format vocabulary</p> <p>REST URL: GET /formats</p> <p>Parameters: None</p> <p>Returns: A list of resource formats supported by HydroShare</p> <p>Return Type: resourceFormatList</p> <p>Raises: Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharesearchquerytype-query-resourcelist","title":"HydroShare.search(queryType, query*) --&gt; resourceList","text":"<p>Search the HydroShare metadata catalog and return a list of pids for resources that match the search criteria. Search may be implemented using more than one type of search engine. The queryType parameter indicates which search engine should be targeted. The value and form of query is determined by the search engine.</p> <p>REST URL: GET /search/{queryType}/{query}</p> <p>Parameters: queryType \u2013 the search engine to be used for the search</p> <p>query \u2013 a string containing the query to be executed on the search engine</p> <p>Returns: A list of pids for resources that match the criteria in the query. If no resources match the query criteria, an empty list is returned.</p> <p>Return Type: resourceList</p> <p>Raises: Exception.InvalidQueryType \u2013 the query type is invalid</p> <p>Exception.InvalidQuery \u2013 the query string is invalid</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Note: We may want to modify this method to return more than just the pids for resources so that some metadata for the list of resources returned could be displayed without having to call HydroShare.getScienceMetadata() and HydroShare.GetSystemMetadata() for every resource in the returned list. Also, this method is still pending selection of the underlying metadata catalog and search engine(s).</p>"},{"location":"DesignDocs/API/#social-api","title":"Social API","text":"<p>The table below lists the REST URLs that will be implemented as part of the HydroShare Social API.</p> <p>Table 4. HydroShare social API URLs and methods.</p> Release REST Path Function Parameters 1 POST /resource/endorse/{pid}/{userID} HydroShare.endorseResource() (pid, userID) --&gt; pid 4 POST /followUser/{userID}/{followerID} HydroShare.followUser() (userID, followerID) --&gt; userID 4 DELETE /followUser/{userID}/{followerID} HydroShare.deleteFollowUser() (userID, followerID) --&gt; userID 4 POST /followResource/{pid}/{followerID} HydroShare.followResource() (pid, followerID) --&gt; pid 4 DELETE /followResource/{pid}/{followerID} HydroShare.deleteFollowResource() (pid, followerID) --&gt; pid 4 POST /followGroup/{groupID}/{followerID} HydroShare.followGroup() (groupID, followerID) --&gt; groupID 4 DELETE /followGroup/{groupID}/{followerID} HydroShare.deleteFollowGroup() (groupID, followerID) --&gt; groupID 1 POST /resource/annotation/{pid}/{userID} HydroShare.annotateResource() (pid, annotation, userID) --&gt; annotationID 1 GET /resource/annotations/{pid} HydroShare.getAnnotations() (pid) --&gt; annotationList 1 POST /resource/annotation/endorse/{annotationID}/{userID} HydroShare.endorseAnnotation() (annotationID, userID) --&gt; annotationID"},{"location":"DesignDocs/API/#hydroshareendorseresourcepid-userid-pid","title":"HydroShare.endorseResource(pid, userID)* --&gt; pid","text":"<p>Create an endorsement or (+1) for a resource in HydroShare identified by pid for the user identified by userID</p> <p>REST URL: POST /resource/endorse/{pid}/{userID}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource being endorsed</p> <p>userID \u2013 userID of the user that is endorsing the resource identified by pid</p> <p>Returns: The pid of the resource that was endorsed</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exceptions.NotFound \u2013 The user identified by userID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharefollowuseruserid-followerid-userid","title":"HydroShare.followUser(userID, followerID)* --&gt; userID","text":"<p>Start following a HydroShare user identified by userID.</p> <p>REST URL: POST /followUser/{userID}/{followerID}</p> <p>Parameters: userID \u2013 userID of the HydroShare user to be followed</p> <p>followerID \u2013 userID of the HydroShare user requesting to follow the user identified by userID</p> <p>Returns: The userID of the HydroShare user being followed</p> <p>Return Type: userID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The user identified by userID does not exist</p> <p>Exceptions.NotFound \u2013 The user identified by followerID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharedeletefollowuseruserid-followerid-userid","title":"HydroShare.deleteFollowUser(userID, followerID)* --&gt; userID","text":"<p>Stop following a HydroShare user identified by userID.</p> <p>REST URL: DELETE /followUser/{userID}/{followerID}</p> <p>Parameters: userID \u2013 userID of the HydroShare user being followed</p> <p>followerID \u2013 userID of the HydroShare user following the user identified by userID</p> <p>Returns: The userID of the HydroShare user being followed</p> <p>Return Type: userID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The user identified by userID does not exist</p> <p>Exceptions.NotFound \u2013 The user identified by followerID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharefollowresourcepid-followerid-pid","title":"HydroShare.followResource(pid, followerID)* --&gt; pid","text":"<p>Start following a HydroShare resource identified by pid.</p> <p>REST URL: POST /followResource/{pid}/{followerID}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource to be followed</p> <p>followerID \u2013 userID of the HydroShare user requesting to follow the resource identified by pid</p> <p>Returns: The pid of the HydroShare resource being followed</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exceptions.NotFound \u2013 The user identified by followerID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharedeletefollowresourcepid-followerid-pid","title":"HydroShare.deleteFollowResource(pid, followerID)* --&gt; pid","text":"<p>Stop following a HydroShare resource identified by pid.</p> <p>REST URL: DELETE /followResource/{pid}/{followerID}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource being followed</p> <p>followerID \u2013 userID of the HydroShare user following the resource identified by pid</p> <p>Returns: The pid of the HydroShare resource being followed</p> <p>Return Type: pid</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exceptions.NotFound \u2013 The user identified by followerID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharefollowgroupgroupid-followerid-groupid","title":"HydroShare.followGroup(groupID, followerID)* --&gt; groupID","text":"<p>Start following a HydroShare group identified by groupID.</p> <p>REST URL: POST /followGroup/{groupID}/{followerID}</p> <p>Parameters: groupID \u2013 Unique HydroShare identifier for the group to be followed</p> <p>followerID \u2013 userID of the HydroShare user requesting to follow the group identified by groupID</p> <p>Returns: The groupID of the HydroShare group being followed</p> <p>Return Type: groupID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The group identified by groupID does not exist</p> <p>Exceptions.NotFound \u2013 The user identified by followerID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydrosharedeletefollowgroupgroupid-followerid-groupid","title":"HydroShare.deleteFollowGroup(groupID, followerID)* --&gt; groupID","text":"<p>Stop following a HydroShare group identified by groupID.</p> <p>REST URL: DELETE /followGroup/{groupID}/{followerID}</p> <p>Parameters: groupID \u2013 Unique HydroShare identifier for the group being followed</p> <p>followerID \u2013 userID of the HydroShare user following the group identified by groupID</p> <p>Returns: The groupID of the HydroShare group being followed</p> <p>Return Type: groupID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The group identified by groupID does not exist</p> <p>Exceptions.NotFound \u2013 The user identified by followerID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydroshareannotateresourcepid-annotation-userid-previousannotationid-annotationid","title":"HydroShare.annotateResource(pid, annotation, userID, previousAnnotationID)* --&gt; annotationID","text":"<p>Create a comment about a resource in HydroShare identified by pid.</p> <p>REST URL: POST /resource/annotation/{pid}/{userID}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource being annotated annotation \u2013 an object containing the annotation to be applied to the resource identified by pid userID \u2013 userID of the HydroShare user creating the annotation previousAnnotationID - (optional) AnnotationID of the previous comment on this resource that this comment follows on from. If omitted the comment is a new comment on a resource.</p> <p>Returns: The annotationID of the annotation that is created</p> <p>Return Type: annotationID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exceptions.InvalidContent \u2013 The content of the annotation object is invalid</p> <p>Exceptions.NotFound \u2013 The user identified by userID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p> <p>Exceptions.NotFound - The previousAnnotationID is not an annotation of the resource identified by pid</p>"},{"location":"DesignDocs/API/#hydrosharegetannotationspid-annotationlist","title":"HydroShare.getAnnotations(pid)* --&gt; annotationList","text":"<p>Get the list of annotations for a resource identified by pid.</p> <p>REST URL: GET /resource/annotations/{pid}</p> <p>Parameters: pid \u2013 Unique HydroShare identifier for the resource whose annotations are to be retrieved</p> <p>Returns: An object containing a list of annotations that have been added to a HydroShare resource identified by pid.</p> <p>Return Type: annotationList</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The resource identified by pid does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#hydroshareendorseannotationannotationid-userid-annotationid","title":"HydroShare.endorseAnnotation(annotationID, userID)* --&gt; annotationID","text":"<p>Create an endorsement or (+1) by a user identified by userID for an annotation identified by annotationID.</p> <p>REST URL: POST /resource/annotation/endorse/{annotationID}/{userID}</p> <p>Parameters: annotationID \u2013 Unique HydroShare identifier for the annotation to be endorsed</p> <p>userID \u2013 userID of the HydroShare user creating the annotation endorsement</p> <p>Returns: The annotationID of the annotation that is endorsed</p> <p>Return Type: annotationID</p> <p>Raises: Exceptions.NotAuthorized \u2013 The user is not authorized</p> <p>Exceptions.NotFound \u2013 The annotation identified by annotationID does not exist</p> <p>Exceptions.NotFound \u2013 The user identified by userID does not exist</p> <p>Exception.ServiceFailure \u2013 The service is unable to process the request</p>"},{"location":"DesignDocs/API/#dataone-member-node-api","title":"DataONE Member Node API","text":"<p>The HydroShare Data Management Plan states that HydroShare will implement the DataONE Member Node APIs and become a DataONE Member Node. Most of the methods in the previous sections have equivalent methods in the DataONE Member Node APIs. HydroShare will have to implement that additional methods described in the DataONE API documentation to become a fully compliant DataONE Member Node (see http://mule1.dataone.org/ArchitectureDocs-current/apis/MN_APIs.html#).</p>"},{"location":"DesignDocs/API/#acknowledgements","title":"Acknowledgements","text":"<p>This work was supported by the National Science Foundation under collaborative grants OCI-1148453 and OCI-1148090 for the development of HydroShare (http://www.hydroshare.org). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</p> <p>DataONE (2013). DataONE APIs. http://mule1.dataone.org/ArchitectureDocs-current/apis/index.html.</p>"},{"location":"hs_composite_resource/models/","title":"Models","text":""},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource","title":"<code>CompositeResource</code>","text":"<p>               Bases: <code>BaseResource</code></p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>class CompositeResource(BaseResource):\n    objects = ResourceManager(\"CompositeResource\")\n\n    # used during discovery as well as in all other places in UI where resource type is displayed\n    display_name = 'Resource'\n\n    class Meta:\n        verbose_name = 'Composite Resource'\n        proxy = True\n\n    @property\n    def can_be_public_or_discoverable(self):\n        # resource level metadata check\n        if not super(CompositeResource, self).can_be_public_or_discoverable:\n            return False\n\n        # logical file level metadata check\n        for lf in self.logical_files:\n            if not lf.metadata.has_all_required_elements():\n                return False\n\n        return True\n\n    @property\n    def has_required_metadata(self):\n        \"\"\"Return True only if all required metadata is present.\"\"\"\n        if not super(CompositeResource, self).has_required_metadata:\n            return False\n\n        for f in self.logical_files:\n            if not f.metadata.has_all_required_elements():\n                return False\n        return True\n\n    @property\n    def logical_files(self):\n        \"\"\"A generator to access each of the logical files of this resource\"\"\"\n\n        for lf in self.filesetlogicalfile_set.all():\n            yield lf\n        for lf in self.genericlogicalfile_set.all():\n            yield lf\n        for lf in self.geofeaturelogicalfile_set.all():\n            yield lf\n        for lf in self.netcdflogicalfile_set.all():\n            yield lf\n        for lf in self.georasterlogicalfile_set.all():\n            yield lf\n        for lf in self.reftimeserieslogicalfile_set.all():\n            yield lf\n        for lf in self.timeserieslogicalfile_set.all():\n            yield lf\n        for lf in self.modelprogramlogicalfile_set.all():\n            yield lf\n        for lf in self.modelinstancelogicalfile_set.all():\n            yield lf\n        for lf in self.csvlogicalfile_set.all():\n            yield lf\n\n    @property\n    def aggregation_types(self):\n        \"\"\"Gets a list of all aggregation types that currently exist in this resource\"\"\"\n        aggr_types = []\n        aggr_type_names = []\n        for lf in self.logical_files:\n            if lf.type_name() not in aggr_type_names:\n                aggr_type_names.append(lf.type_name())\n                aggr_type = lf.get_aggregation_display_name().split(\":\")[0]\n                aggr_types.append(aggr_type)\n        return aggr_types\n\n    @property\n    def aggregation_type_names(self):\n        \"\"\"Gets a list of all aggregation type names that currently exist in this resource\n        \"\"\"\n        aggr_type_names = []\n        for lf in self.logical_files:\n            if lf.type_name not in aggr_type_names:\n                aggr_type_names.append(lf.type_name())\n        return aggr_type_names\n\n    def get_logical_files(self, logical_file_class_name):\n        \"\"\"Get a list of logical files (aggregations) for a specified logical file class name.\"\"\"\n\n        class_name_to_query_mappings = {\n            FileSetLogicalFile.type_name(): self.filesetlogicalfile_set.all(),\n            GenericLogicalFile.type_name(): self.genericlogicalfile_set.all(),\n            GeoFeatureLogicalFile.type_name(): self.geofeaturelogicalfile_set.all(),\n            GeoRasterLogicalFile.type_name(): self.georasterlogicalfile_set.all(),\n            ModelInstanceLogicalFile.type_name(): self.modelinstancelogicalfile_set.all(),\n            ModelProgramLogicalFile.type_name(): self.modelprogramlogicalfile_set.all(),\n            NetCDFLogicalFile.type_name(): self.netcdflogicalfile_set.all(),\n            TimeSeriesLogicalFile.type_name(): self.timeserieslogicalfile_set.all(),\n            RefTimeseriesLogicalFile.type_name(): self.reftimeserieslogicalfile_set.all(),\n            CSVLogicalFile.type_name(): self.csvlogicalfile_set.all(),\n        }\n\n        if logical_file_class_name in class_name_to_query_mappings:\n            return class_name_to_query_mappings[logical_file_class_name]\n\n        raise Exception(f\"Invalid logical file type name:{logical_file_class_name}\")\n\n    @property\n    def has_logical_spatial_coverage(self):\n        \"\"\"Checks if any of the logical files has spatial coverage\"\"\"\n\n        return any(lf.metadata.spatial_coverage is not None for lf in self.logical_files)\n\n    @property\n    def has_logical_temporal_coverage(self):\n        \"\"\"Checks if any of the logical files has temporal coverage\"\"\"\n\n        return any(lf.metadata.temporal_coverage is not None for lf in self.logical_files)\n\n    @property\n    def can_be_submitted_for_metadata_review(self):\n        # resource level metadata check\n        if not super(CompositeResource, self).can_be_submitted_for_metadata_review:\n            return False\n\n        # logical file level metadata check\n        for lf in self.logical_files:\n            if not lf.metadata.has_all_required_elements():\n                return False\n            # url file cannot be published\n            if 'url' in lf.extra_data:\n                return False\n\n        return True\n\n    def remove_aggregation_from_file(self, moved_res_file, src_folder, tgt_folder, aggregations=None, cleanup=True):\n        \"\"\"removes association with aggregation (fileset or model program) from a resource file that has been moved\n        :param  moved_res_file: an instance of a ResourceFile which has been moved to a different folder\n        :param  src_folder: folder from which the file got moved from\n        :param  tgt_folder: folder to which the file got moved into\n        :param aggregations:   list of all aggregations in self (this resource)\n        :param cleanup: if True, cleanup aggregation if aggregation is empty after the file removal\n        \"\"\"\n\n        if moved_res_file.file_folder:\n            aggregation = self.get_folder_aggregation_in_path(moved_res_file.file_folder, aggregations=aggregations)\n            if aggregation is None:\n                return\n\n            # aggregation must be one of 'fileset', 'model instance' or 'model program'\n            if aggregation == moved_res_file.logical_file:\n                # remove aggregation association with the file\n                # the removed aggregation is a fileset aggregation or a model program or a model instance\n                # aggregation based on folder (note: model program/instance aggregation can also be\n                # created from a single file)\n                moved_res_file.logical_file_content_object = None\n                moved_res_file.save()\n                # delete any instance of ModelProgramResourceFileType associated with this moved file\n                if aggregation.is_model_program:\n                    # if the file is getting moved within a model program folder hierarchy then no need\n                    # to delete any associated ModelProgramResourceFileType object\n                    if not tgt_folder.startswith(src_folder) and not src_folder.startswith(tgt_folder):\n                        ModelProgramResourceFileType.objects.filter(res_file=moved_res_file).delete()\n                if cleanup:\n                    self.cleanup_aggregations()\n\n    def add_file_to_aggregation(self, moved_res_file, aggregations=None):\n        \"\"\"adds the moved file to the aggregation (fileset or model program/instance) into which the file has been moved\n        :param  moved_res_file: an instance of ResourceFile which has been moved into a folder that represents\n        a fileset, a model program, or a model instance aggregation\n        :param aggregations:   list of all aggregations in self (this resource)\n        \"\"\"\n        if moved_res_file.file_folder and not moved_res_file.has_logical_file:\n            # first check for model program/instance aggregation\n            aggregation = self.get_model_aggregation_in_path(moved_res_file.file_folder, aggregations=aggregations)\n            if aggregation is None:\n                # then check for fileset aggregation\n                aggregation = self.get_fileset_aggregation_in_path(moved_res_file.file_folder,\n                                                                   aggregations=aggregations)\n            if aggregation is not None:\n                # make the moved file part of the fileset or model program aggregation unless the file is\n                # already part of another aggregation (single file aggregation)\n                aggregation.add_resource_file(moved_res_file)\n\n    def get_folder_aggregation_object(self, dir_path, aggregations=None):\n        \"\"\"Returns an aggregation (file type) object if the specified folder *dir_path* represents a\n         file type aggregation (logical file), otherwise None.\n\n         :param dir_path: Resource file directory path (full folder path starting with resource id)\n         for which the aggregation object to be retrieved\n         :param aggregations:   list of all aggregations in self (this resource)\n        \"\"\"\n\n        aggregation_path = self.get_relative_path(dir_path)\n        logical_files = self._cache_aggregations(aggregations=aggregations)\n        for lf in logical_files:\n            if hasattr(lf, 'folder'):\n                if lf.folder == aggregation_path:\n                    return lf\n        return None\n\n    def get_folder_aggregation_in_path(self, dir_path, aggregations=None):\n        \"\"\"Gets any aggregation that is based on folder and exists in the specified path\n        Searches for a folder based aggregation moving towards the root of the specified path\n        :param  dir_path: directory path in which to search for a folder based aggregation\n        :param aggregations: a list of all aggregations in self (this resource)\n        :return a folder based aggregation if found otherwise, None\n        \"\"\"\n\n        dir_path = self.get_relative_path(dir_path)\n        aggregations = self._cache_aggregations(aggregations=aggregations)\n        if not aggregations:\n            # no aggregations exist in this resource\n            return None\n\n        def get_aggregation(path):\n            try:\n                aggregation = self.get_aggregation_by_name(path, aggregations=aggregations)\n                return aggregation\n            except ObjectDoesNotExist:\n                return None\n\n        while '/' in dir_path:\n            aggr = get_aggregation(dir_path)\n            if aggr is not None:\n                return aggr\n            dir_path = os.path.dirname(dir_path)\n        else:\n            return get_aggregation(dir_path)\n\n    def get_file_aggregation_object(self, file_path):\n        \"\"\"Returns an aggregation (file type) object if the specified file *file_path* represents a\n         file type aggregation (logical file), otherwise None.\n\n         :param file_path: Resource file path (full file path starting with resource id)\n         for which the aggregation object to be retrieved\n        \"\"\"\n        relative_file_path = self.get_relative_path(file_path)\n        folder, base = os.path.split(relative_file_path)\n        try:\n            res_file = ResourceFile.get(self, file=base, folder=folder)\n            if res_file.has_logical_file:\n                return res_file.logical_file\n            return None\n        except ObjectDoesNotExist:\n            return None\n\n    @property\n    def supports_folders(self):\n        \"\"\" allow folders for CompositeResources \"\"\"\n        return True\n\n    @property\n    def supports_logical_file(self):\n        \"\"\" if this resource allows associating resource file objects with logical file\"\"\"\n        return True\n\n    def create_aggregation_meta_files(self, path=''):\n        \"\"\"Creates aggregation meta files (resource map, metadata xml files and schema json files) for each of the\n        contained aggregations\n        :param  path: (optional) file or folder path for which meta files need to be created for\n        all associated aggregations of that path\n        \"\"\"\n\n        if not path:\n            # create xml docs far all aggregation of this resource\n            for aggregation in self.logical_files:\n                if aggregation.metadata.is_dirty:\n                    aggregation.create_aggregation_xml_documents()\n        else:\n            # first check if the path is a folder path or file path\n            is_path_a_folder = self.is_path_folder(path=path)\n            if is_path_a_folder:\n                # need to create xml files for all aggregations that exist under path\n                path = self.get_relative_path(path)\n                for lf in self.logical_files:\n                    if lf.aggregation_name.startswith(path) and lf.metadata.is_dirty:\n                        lf.create_aggregation_xml_documents()\n            else:\n                # path is a file path\n                try:\n                    aggregation = self.get_aggregation_by_name(path)\n                    # need to create xml docs only for this aggregation\n                    if aggregation.metadata.is_dirty:\n                        aggregation.create_aggregation_xml_documents()\n                except ObjectDoesNotExist:\n                    # file path is not an aggregation - nothing to do\n                    pass\n\n    def get_aggregation_by_aggregation_name(self, aggregation_name):\n        \"\"\"Get an aggregation that matches the aggregation dataset_name specified by *dataset_name*\n        :param  aggregation_name: aggregation_name (aggregation path) of the aggregation to find\n        :return an aggregation object if found\n        :raises ObjectDoesNotExist if no matching aggregation is found\n        \"\"\"\n        for aggregation in self.logical_files:\n            if aggregation.aggregation_name == aggregation_name:\n                return aggregation\n\n        raise ObjectDoesNotExist(\"No matching aggregation was found for \"\n                                 \"name:{}\".format(aggregation_name))\n\n    def get_aggregation_by_name(self, name, aggregations=None):\n        \"\"\"Get an aggregation that matches the aggregation name specified by *name*\n        :param  name: name (aggregation path) of the aggregation to find\n        :param  aggregations:   a list of aggregations in the resource (self)\n        :return an aggregation object if found\n        :raises ObjectDoesNotExist if no matching aggregation is found\n        \"\"\"\n        # check if aggregation path *name* is a file path or a folder\n        is_aggr_path_a_folder = self.is_path_folder(path=name)\n        if is_aggr_path_a_folder:\n            folder_full_path = os.path.join(self.file_path, name)\n            aggregation = self.get_folder_aggregation_object(folder_full_path, aggregations=aggregations)\n            if aggregation is None:\n                raise ObjectDoesNotExist(\n                    \"No matching aggregation was found for name:{}\".format(name))\n            return aggregation\n        else:\n            folder, base = os.path.split(name)\n            res_file = ResourceFile.get(self, file=base, folder=folder)\n            if res_file.has_logical_file:\n                return res_file.logical_file\n\n            raise ObjectDoesNotExist(\n                \"No matching aggregation was found for name:{}\".format(name))\n\n    def get_aggregation_by_meta_file(self, meta_file_path: str) -&gt; AggregationType:\n        \"\"\"Get an aggregation that matches the specified meta xml/json file path\n        :param  meta_file_path: directory path of the meta xml/json file\n        :return an aggregation object if found\n        :raises ObjectDoesNotExist if no matching aggregation is found\n        \"\"\"\n        if __debug__:\n            assert any(meta_file_path.endswith(ext) for ext in AggregationMetaFilePath)\n\n        meta_file_path = self.get_relative_path(meta_file_path)\n        folder, base = os.path.split(meta_file_path)\n        if base.endswith(AggregationMetaFilePath.METADATA_FILE_ENDSWITH.value):\n            base_file_name_to_match = base[:-len(AggregationMetaFilePath.METADATA_FILE_ENDSWITH.value)]\n        elif base.endswith(AggregationMetaFilePath.RESMAP_FILE_ENDSWITH.value):\n            base_file_name_to_match = base[:-len(AggregationMetaFilePath.RESMAP_FILE_ENDSWITH.value)]\n        else:\n            base_file_name_to_match = base[:-len(AggregationMetaFilePath.SCHEMA_JSON_FILE_ENDSWITH)]\n\n        for res_file in ResourceFile.list_folder(self, folder=folder, sub_folders=False):\n            if res_file.has_logical_file:\n                file_name_to_match = f\"{base_file_name_to_match}{res_file.extension}\"\n                if res_file.file_name == file_name_to_match:\n                    return res_file.logical_file\n        # check for folder aggregation\n        if folder and folder == base_file_name_to_match:\n            folder_aggregation = self.get_folder_aggregation_object(folder)\n            if folder_aggregation is not None:\n                return folder_aggregation\n\n        raise ObjectDoesNotExist(\"No matching aggregation was found for \"\n                                 \"meta file path:{}\".format(meta_file_path))\n\n    def get_fileset_aggregation_in_path(self, path, aggregations=None):\n        \"\"\"Get the first fileset aggregation in the path moving up (towards the root)in the path\n        :param  path: directory path in which to search for a fileset aggregation\n        :param  aggregations: a list of aggregations in the resource (self)\n        :return a fileset aggregation object if found, otherwise None\n        \"\"\"\n\n        path = self.get_relative_path(path)\n        aggregations = self._cache_aggregations(aggregations=aggregations)\n        if not aggregations:\n            # no aggregations exist in this resource\n            return None\n\n        def get_fileset(_path):\n            try:\n                aggregation = self.get_aggregation_by_name(_path, aggregations=aggregations)\n                if aggregation.is_fileset:\n                    return aggregation\n            except ObjectDoesNotExist:\n                return None\n\n        while '/' in path:\n            fileset = get_fileset(path)\n            if fileset is not None:\n                return fileset\n            path = os.path.dirname(path)\n        else:\n            return get_fileset(path)\n\n    def get_model_aggregation_in_path(self, path, aggregations=None):\n        \"\"\"Get the model program or model instance aggregation in the path moving up (towards the root)in the path\n        :param  path: directory path in which to search for a model program or model instance aggregation\n        :param  aggregations: a list of aggregations in the resource (self)\n        :return a model program or model instance aggregation object if found, otherwise None\n        \"\"\"\n\n        path = self.get_relative_path(path)\n        aggregations = self._cache_aggregations(aggregations=aggregations)\n        if not aggregations:\n            # no aggregations exist in this resource\n            return None\n\n        def get_aggregation(_path):\n            try:\n                aggregation = self.get_aggregation_by_name(_path, aggregations=aggregations)\n                return aggregation\n            except ObjectDoesNotExist:\n                return None\n\n        while '/' in path:\n            aggr = get_aggregation(path)\n            if aggr is not None and (aggr.is_model_program or aggr.is_model_instance):\n                return aggr\n            path = os.path.dirname(path)\n        else:\n            aggr = get_aggregation(path)\n            if aggr is not None and (aggr.is_model_program or aggr.is_model_instance):\n                return aggr\n            return None\n\n    def set_flag_to_recreate_aggregation_meta_files(self, orig_path, new_path):\n        \"\"\"\n        When a folder or file representing an aggregation is renamed or moved,\n        the associated meta files (resource map, metadata xml files as well as schema json files) are deleted\n        and then aggregation metadata is set to dirty so that these meta files will be regenerated as part of\n        aggregation or bag download\n        :param  orig_path: original file/folder path prior to move/rename\n        :param  new_path: new file/folder path after move/rename\n        \"\"\"\n        aggregations = list(self.logical_files)\n\n        def set_parent_aggregation_dirty(path_to_search):\n            if '/' in path_to_search:\n                path = os.path.dirname(path_to_search)\n                try:\n                    parent_aggr = self.get_aggregation_by_name(path, aggregations=aggregations)\n                    parent_aggr.set_metadata_dirty()\n                except ObjectDoesNotExist:\n                    pass\n\n        new_path = self.get_relative_path(new_path)\n        orig_path = self.get_relative_path(orig_path)\n        is_new_path_a_folder = self.is_path_folder(path=new_path)\n        istorage = self.get_s3_storage()\n\n        # remove file extension from aggregation name (note: aggregation name is a file path\n        # for all aggregation types except fileset/model aggregation\n        file_name, _ = os.path.splitext(orig_path)\n        schema_json_file_name = file_name + AggregationMetaFilePath.SCHEMA_JSON_FILE_ENDSWITH.value\n        meta_xml_file_name = file_name + AggregationMetaFilePath.METADATA_FILE_ENDSWITH.value\n        map_xml_file_name = file_name + AggregationMetaFilePath.RESMAP_FILE_ENDSWITH.value\n        if not is_new_path_a_folder:\n            # case of file rename/move for single file aggregation\n            schema_json_file_full_path = os.path.join(self.file_path, schema_json_file_name)\n            meta_xml_file_full_path = os.path.join(self.file_path, meta_xml_file_name)\n            map_xml_file_full_path = os.path.join(self.file_path, map_xml_file_name)\n            # for single file aggregations, compute the metadata JSON file path using original path\n            metadata_json_file_full_path = os.path.join(\n                self.file_path, orig_path + AggregationMetaFilePath.METADATA_JSON_FILE_ENDSWITH.value\n            )\n        else:\n            # case of folder rename - fileset/model aggregation\n            _, schema_json_file_name = os.path.split(schema_json_file_name)\n            _, meta_xml_file_name = os.path.split(meta_xml_file_name)\n            _, map_xml_file_name = os.path.split(map_xml_file_name)\n            schema_json_file_full_path = os.path.join(self.file_path, new_path, schema_json_file_name)\n            meta_xml_file_full_path = os.path.join(self.file_path, new_path, meta_xml_file_name)\n            map_xml_file_full_path = os.path.join(self.file_path, new_path, map_xml_file_name)\n            # No need to delete metadata JSON files for folder rename cases as metadata JSON filename\n            # is not affected by folder rename\n            metadata_json_file_full_path = None\n\n        if istorage.exists(schema_json_file_full_path):\n            istorage.delete(schema_json_file_full_path)\n\n        if istorage.exists(meta_xml_file_full_path):\n            istorage.delete(meta_xml_file_full_path)\n\n        if istorage.exists(map_xml_file_full_path):\n            istorage.delete(map_xml_file_full_path)\n\n        # delete metadata JSON file only for single file aggregations\n        if (\n            not is_new_path_a_folder\n            and metadata_json_file_full_path is not None\n            and istorage.exists(metadata_json_file_full_path)\n        ):\n            istorage.delete(metadata_json_file_full_path)\n\n        # set affected logical file metadata to dirty so that xml meta files will be regenerated at the time of\n        # aggregation or bag download\n        for lf in aggregations:\n            # set metadata dirty for any folder based aggregations under the orig_path\n            if hasattr(lf, 'folder'):\n                if lf.folder is not None and lf.folder.startswith(orig_path):\n                    lf.folder = os.path.join(new_path, lf.folder[len(orig_path) + 1:]).strip('/')\n                    lf.save(update_fields=[\"folder\"])\n                    lf.set_metadata_dirty()\n                    continue\n\n            # set metadata dirty for any non-folder based aggregation under the orig_path\n            if lf.aggregation_name.startswith(orig_path):\n                lf.set_metadata_dirty()\n\n            # set metadata to dirty for non-folder based aggregation under the new_path\n            if lf.aggregation_name.startswith(new_path):\n                lf.set_metadata_dirty()\n\n        # set metadata to dirty for any parent aggregation that may exist relative to path *orig_path*\n        set_parent_aggregation_dirty(orig_path)\n\n        # set metadata to dirty for any parent aggregation that may exist relative to path *new_path*\n        set_parent_aggregation_dirty(new_path)\n\n        try:\n            aggregation = self.get_aggregation_by_name(new_path, aggregations=aggregations)\n            aggregation.set_metadata_dirty()\n        except ObjectDoesNotExist:\n            # the file path *new_path* does not represent an aggregation - no more\n            # action is needed\n            pass\n\n    def is_aggregation_xml_file(self, file_path):\n        \"\"\" determine whether a given file in the file hierarchy is metadata.\n\n        This is true if it is listed as metadata in any logical file.\n        \"\"\"\n        if not self.is_metadata_xml_file(file_path):\n            return False\n        for logical_file in self.logical_files:\n            if logical_file.metadata_file_path == file_path or \\\n                    logical_file.map_file_path == file_path:\n                return True\n        return False\n\n    def supports_rename_path(self, src_full_path, tgt_full_path):\n        \"\"\"checks if file/folder rename/move is allowed\n        :param  src_full_path: name of the file/folder storage path to be renamed (path starts with resource id)\n        :param  tgt_full_path: new name for file/folder storage path (path starts with resource id)\n        :return True or False\n        \"\"\"\n\n        if __debug__:\n            assert src_full_path.startswith(self.file_path)\n            assert tgt_full_path.startswith(self.file_path)\n\n        # need to find out which of the following actions the user is trying to do:\n        # renaming a file\n        # renaming a folder\n        # moving a file\n        # moving a folder\n        is_renaming_file = False\n        is_moving_file = False\n        is_moving_folder = False\n\n        if tgt_full_path == self.file_path:\n            # at the root of the resource all file operations are allowed\n            return True\n\n        istorage = self.get_s3_storage()\n        scr_base_name = os.path.basename(src_full_path)\n        src_dir_path = os.path.dirname(src_full_path)\n        tgt_dir_path = os.path.dirname(tgt_full_path)\n\n        if istorage.isFile(src_full_path):\n            if istorage.exists(tgt_full_path) or tgt_full_path.endswith(scr_base_name):\n                is_moving_file = True\n                if tgt_full_path.endswith(scr_base_name):\n                    tgt_dir_path = os.path.dirname(tgt_full_path)\n                else:\n                    tgt_dir_path = tgt_full_path\n            else:\n                is_renaming_file = True\n        else:\n            # src path is a directory\n            if src_dir_path == tgt_dir_path:\n                # renaming folder - no restriction\n                return True\n            else:\n                is_moving_folder = True\n\n        def check_src_aggregation(src_aggr):\n            \"\"\"checks if the aggregation at the source allows file rename/move action\"\"\"\n            if src_aggr is not None:\n                if is_renaming_file:\n                    return src_aggr.supports_resource_file_rename\n                elif is_moving_file:\n                    return src_aggr.supports_resource_file_move\n            return True\n\n        if is_renaming_file or is_moving_file:\n            # see if the folder containing the file represents an aggregation\n            src_aggr = self.get_file_aggregation_object(file_path=src_full_path)\n            if check_src_aggregation(src_aggr):\n                # check target\n                if is_moving_file:\n                    tgt_aggr = self.get_folder_aggregation_in_path(dir_path=tgt_dir_path)\n                    if tgt_aggr is not None:\n                        if src_aggr is None:\n                            return tgt_aggr.supports_resource_file_move\n                        else:\n                            return tgt_aggr.can_contain_aggregation(src_aggr)\n                    return True\n                return True\n            return False\n\n        if is_moving_folder:\n            src_aggr = self.get_folder_aggregation_in_path(dir_path=src_full_path)\n            if src_aggr is not None:\n                if src_aggr.supports_resource_file_move:\n                    tgt_aggr = self.get_folder_aggregation_in_path(dir_path=tgt_full_path)\n                    if tgt_aggr is not None:\n                        return tgt_aggr.supports_resource_file_move and tgt_aggr.can_contain_aggregation(src_aggr)\n                    return True\n            tgt_aggr = self.get_folder_aggregation_in_path(dir_path=tgt_full_path)\n            if tgt_aggr is not None:\n                return tgt_aggr.supports_resource_file_move\n            return True\n\n    def can_add_files(self, target_full_path):\n        \"\"\"\n        checks if file(s) can be uploaded to the specified *target_full_path*\n        :param target_full_path: full folder path name where file needs to be uploaded to\n        :return: True or False\n        \"\"\"\n        path_to_check = target_full_path\n\n        if not path_to_check.endswith(\"data/contents\"):\n            # it is not the base directory - it must be a directory under base dir\n            aggregation_path = self.get_relative_path(path_to_check)\n            try:\n                aggregation = self.get_aggregation_by_name(aggregation_path)\n                return aggregation.supports_resource_file_add\n            except ObjectDoesNotExist:\n                # target path doesn't represent an aggregation - so it is OK to add a file\n                pass\n        return True\n\n    def supports_zip(self, folder_to_zip):\n        \"\"\"check if the given folder can be zipped or not\"\"\"\n\n        # find all the resource files in the folder to be zipped\n        # this is being passed both qualified and unqualified paths!\n\n        full_path = folder_to_zip\n        if not full_path.startswith(self.file_path):\n            full_path = os.path.join(self.file_path, full_path)\n        # get all resource files at full_path and its sub-folders\n        res_file_objects = ResourceFile.list_folder(self, full_path)\n\n        # check any logical file associated with the resource file supports zip functionality\n        for res_file in res_file_objects:\n            if res_file.has_logical_file:\n                if not res_file.logical_file.supports_zip:\n                    return False\n        return True\n\n    def supports_delete_folder_on_zip(self, original_folder):\n        \"\"\"check if the specified folder can be deleted at the end of zipping that folder\"\"\"\n\n        # find all the resource files in the folder to be deleted\n        # this is being passed both qualified and unqualified paths!\n        full_path = original_folder\n        if not full_path.startswith(self.file_path):\n            full_path = os.path.join(self.file_path, full_path)\n\n        # get all resource files at full_path and its sub-folders\n        res_file_objects = ResourceFile.list_folder(self, full_path)\n\n        # check any logical file associated with the resource file supports deleting the folder\n        # after its zipped\n        for res_file in res_file_objects:\n            if res_file.has_logical_file:\n                if not res_file.logical_file.supports_delete_folder_on_zip:\n                    return False\n        return True\n\n    def get_missing_file_type_metadata_info(self):\n        # this is used in page pre-processor to build the context\n        # so that the landing page can show what metadata items are missing for each\n        # logical file/aggregation\n        metadata_missing_info = []\n        for lfo in self.logical_files:\n            if not lfo.metadata.has_all_required_elements():\n                missing_elements = lfo.metadata.get_required_missing_elements()\n                metadata_missing_info.append({'file_path': lfo.aggregation_name,\n                                              'missing_elements': missing_elements})\n        return metadata_missing_info\n\n    def get_data_services_urls(self):\n        \"\"\"\n        Generates data services URLs for the resource.\n        If the resource contains any GeoFeature or GeoRaster content, and if it's public,\n        generate data service endpoints.\n        If the resource contains any multidimensional content and it's public,\n        generate THREDDS catalog service endpoint as well.\n        \"\"\"\n        wfs_url = None\n        wms_url = None\n        wcs_url = None\n        thredds_url = None\n        if self.raccess.public:\n            try:\n                resource_data_types = [lf.data_type for lf in self.logical_files]\n                service_url = (\n                    f'{settings.HSWS_GEOSERVER_URL}/HS-{self.short_id}/'\n                    + '{}?request=GetCapabilities'\n                )\n                if 'GeographicFeature' in resource_data_types:\n                    wfs_url = service_url.format('wfs')\n                    wms_url = service_url.format('wms')\n                if 'GeographicRaster' in resource_data_types:\n                    wcs_url = service_url.format('wcs')\n                    wms_url = service_url.format('wms')\n            except Exception as e:\n                logger.exception(\"get_data_services_urls: \" + str(e))\n\n            if 'Multidimensional' in resource_data_types:\n                thredds_url = (\n                    f'{settings.THREDDS_SERVER_URL}catalog/hydroshare/resources/{self.short_id}/data/contents/'\n                    f'catalog.html'\n                )\n        data_services_urls = {\n            'wms_url': wms_url,\n            'wfs_url': wfs_url,\n            'wcs_url': wcs_url,\n            'thredds_url': thredds_url\n        }\n\n        return data_services_urls\n\n    def delete_coverage(self, coverage_type):\n        \"\"\"Deletes coverage data for the resource\n        :param coverage_type: A value of either 'spatial' or 'temporal\n        :return:\n        \"\"\"\n        if coverage_type.lower() == 'spatial' and self.metadata.spatial_coverage:\n            self.metadata.spatial_coverage.delete()\n            self.metadata.is_dirty = True\n            self.metadata.save()\n        elif coverage_type.lower() == 'temporal' and self.metadata.temporal_coverage:\n            self.metadata.temporal_coverage.delete()\n            self.metadata.is_dirty = True\n            self.metadata.save()\n\n    def update_coverage(self):\n        \"\"\"Update resource spatial and temporal coverage based on the corresponding coverages\n        from all the contained aggregations (logical file) only if the resource coverage is not\n        already set\"\"\"\n\n        # update resource spatial coverage only if there is no spatial coverage already\n        if self.metadata.spatial_coverage is None:\n            self.update_spatial_coverage()\n\n        # update resource temporal coverage only if there is no temporal coverage already\n        if self.metadata.temporal_coverage is None:\n            self.update_temporal_coverage()\n\n    def update_spatial_coverage(self):\n        \"\"\"Updates resource spatial coverage based on the contained spatial coverages of\n        aggregations (file type). Note: This action will overwrite any existing resource spatial\n        coverage data.\n        \"\"\"\n\n        update_target_spatial_coverage(self)\n\n    def update_temporal_coverage(self):\n        \"\"\"Updates resource temporal coverage based on the contained temporal coverages of\n        aggregations (file type). Note: This action will overwrite any existing resource temporal\n        coverage data.\n        \"\"\"\n\n        update_target_temporal_coverage(self)\n\n    def cleanup_aggregations(self):\n        \"\"\"Deletes any dangling aggregations (aggregation without resource files or folder) the resource may have\"\"\"\n\n        count = 0\n        for lf in self.logical_files:\n            if lf.is_dangling:\n                agg_cls_name = lf.type_name()\n                lf.remove_aggregation()\n                count += 1\n                msg = \"Deleted a dangling aggregation of type:{} for resource:{}\".format(agg_cls_name, self.short_id)\n                logger.warning(msg)\n        return count\n\n    def dangling_aggregations_exist(self):\n        \"\"\"Checks if there are any dangling aggregations in this resource\n        Note: This function used only in tests\n        \"\"\"\n\n        for lf in self.logical_files:\n            if lf.is_dangling:\n                return True\n        return False\n\n    def is_path_folder(self, path):\n        istorage = self.get_s3_storage()\n        if not path.startswith(self.file_path):\n            path = os.path.join(self.file_path, path)\n        return istorage.isDir(path)\n\n    def _cache_aggregations(self, aggregations):\n        \"\"\"A helper function to cache aggregations to avoid repeated database queries\"\"\"\n        if aggregations is None:\n            aggregations = list(self.logical_files)\n\n        return aggregations\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.aggregation_type_names","title":"<code>aggregation_type_names</code>  <code>property</code>","text":"<p>Gets a list of all aggregation type names that currently exist in this resource</p>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.aggregation_types","title":"<code>aggregation_types</code>  <code>property</code>","text":"<p>Gets a list of all aggregation types that currently exist in this resource</p>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.has_logical_spatial_coverage","title":"<code>has_logical_spatial_coverage</code>  <code>property</code>","text":"<p>Checks if any of the logical files has spatial coverage</p>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.has_logical_temporal_coverage","title":"<code>has_logical_temporal_coverage</code>  <code>property</code>","text":"<p>Checks if any of the logical files has temporal coverage</p>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.has_required_metadata","title":"<code>has_required_metadata</code>  <code>property</code>","text":"<p>Return True only if all required metadata is present.</p>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.logical_files","title":"<code>logical_files</code>  <code>property</code>","text":"<p>A generator to access each of the logical files of this resource</p>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.supports_folders","title":"<code>supports_folders</code>  <code>property</code>","text":"<p>allow folders for CompositeResources</p>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.supports_logical_file","title":"<code>supports_logical_file</code>  <code>property</code>","text":"<p>if this resource allows associating resource file objects with logical file</p>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.add_file_to_aggregation","title":"<code>add_file_to_aggregation(moved_res_file, aggregations=None)</code>","text":"<p>adds the moved file to the aggregation (fileset or model program/instance) into which the file has been moved :param  moved_res_file: an instance of ResourceFile which has been moved into a folder that represents a fileset, a model program, or a model instance aggregation :param aggregations:   list of all aggregations in self (this resource)</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def add_file_to_aggregation(self, moved_res_file, aggregations=None):\n    \"\"\"adds the moved file to the aggregation (fileset or model program/instance) into which the file has been moved\n    :param  moved_res_file: an instance of ResourceFile which has been moved into a folder that represents\n    a fileset, a model program, or a model instance aggregation\n    :param aggregations:   list of all aggregations in self (this resource)\n    \"\"\"\n    if moved_res_file.file_folder and not moved_res_file.has_logical_file:\n        # first check for model program/instance aggregation\n        aggregation = self.get_model_aggregation_in_path(moved_res_file.file_folder, aggregations=aggregations)\n        if aggregation is None:\n            # then check for fileset aggregation\n            aggregation = self.get_fileset_aggregation_in_path(moved_res_file.file_folder,\n                                                               aggregations=aggregations)\n        if aggregation is not None:\n            # make the moved file part of the fileset or model program aggregation unless the file is\n            # already part of another aggregation (single file aggregation)\n            aggregation.add_resource_file(moved_res_file)\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.can_add_files","title":"<code>can_add_files(target_full_path)</code>","text":"<p>checks if file(s) can be uploaded to the specified target_full_path :param target_full_path: full folder path name where file needs to be uploaded to :return: True or False</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def can_add_files(self, target_full_path):\n    \"\"\"\n    checks if file(s) can be uploaded to the specified *target_full_path*\n    :param target_full_path: full folder path name where file needs to be uploaded to\n    :return: True or False\n    \"\"\"\n    path_to_check = target_full_path\n\n    if not path_to_check.endswith(\"data/contents\"):\n        # it is not the base directory - it must be a directory under base dir\n        aggregation_path = self.get_relative_path(path_to_check)\n        try:\n            aggregation = self.get_aggregation_by_name(aggregation_path)\n            return aggregation.supports_resource_file_add\n        except ObjectDoesNotExist:\n            # target path doesn't represent an aggregation - so it is OK to add a file\n            pass\n    return True\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.cleanup_aggregations","title":"<code>cleanup_aggregations()</code>","text":"<p>Deletes any dangling aggregations (aggregation without resource files or folder) the resource may have</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def cleanup_aggregations(self):\n    \"\"\"Deletes any dangling aggregations (aggregation without resource files or folder) the resource may have\"\"\"\n\n    count = 0\n    for lf in self.logical_files:\n        if lf.is_dangling:\n            agg_cls_name = lf.type_name()\n            lf.remove_aggregation()\n            count += 1\n            msg = \"Deleted a dangling aggregation of type:{} for resource:{}\".format(agg_cls_name, self.short_id)\n            logger.warning(msg)\n    return count\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.create_aggregation_meta_files","title":"<code>create_aggregation_meta_files(path='')</code>","text":"<p>Creates aggregation meta files (resource map, metadata xml files and schema json files) for each of the contained aggregations :param  path: (optional) file or folder path for which meta files need to be created for all associated aggregations of that path</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def create_aggregation_meta_files(self, path=''):\n    \"\"\"Creates aggregation meta files (resource map, metadata xml files and schema json files) for each of the\n    contained aggregations\n    :param  path: (optional) file or folder path for which meta files need to be created for\n    all associated aggregations of that path\n    \"\"\"\n\n    if not path:\n        # create xml docs far all aggregation of this resource\n        for aggregation in self.logical_files:\n            if aggregation.metadata.is_dirty:\n                aggregation.create_aggregation_xml_documents()\n    else:\n        # first check if the path is a folder path or file path\n        is_path_a_folder = self.is_path_folder(path=path)\n        if is_path_a_folder:\n            # need to create xml files for all aggregations that exist under path\n            path = self.get_relative_path(path)\n            for lf in self.logical_files:\n                if lf.aggregation_name.startswith(path) and lf.metadata.is_dirty:\n                    lf.create_aggregation_xml_documents()\n        else:\n            # path is a file path\n            try:\n                aggregation = self.get_aggregation_by_name(path)\n                # need to create xml docs only for this aggregation\n                if aggregation.metadata.is_dirty:\n                    aggregation.create_aggregation_xml_documents()\n            except ObjectDoesNotExist:\n                # file path is not an aggregation - nothing to do\n                pass\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.dangling_aggregations_exist","title":"<code>dangling_aggregations_exist()</code>","text":"<p>Checks if there are any dangling aggregations in this resource Note: This function used only in tests</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def dangling_aggregations_exist(self):\n    \"\"\"Checks if there are any dangling aggregations in this resource\n    Note: This function used only in tests\n    \"\"\"\n\n    for lf in self.logical_files:\n        if lf.is_dangling:\n            return True\n    return False\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.delete_coverage","title":"<code>delete_coverage(coverage_type)</code>","text":"<p>Deletes coverage data for the resource :param coverage_type: A value of either 'spatial' or 'temporal :return:</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def delete_coverage(self, coverage_type):\n    \"\"\"Deletes coverage data for the resource\n    :param coverage_type: A value of either 'spatial' or 'temporal\n    :return:\n    \"\"\"\n    if coverage_type.lower() == 'spatial' and self.metadata.spatial_coverage:\n        self.metadata.spatial_coverage.delete()\n        self.metadata.is_dirty = True\n        self.metadata.save()\n    elif coverage_type.lower() == 'temporal' and self.metadata.temporal_coverage:\n        self.metadata.temporal_coverage.delete()\n        self.metadata.is_dirty = True\n        self.metadata.save()\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.get_aggregation_by_aggregation_name","title":"<code>get_aggregation_by_aggregation_name(aggregation_name)</code>","text":"<p>Get an aggregation that matches the aggregation dataset_name specified by dataset_name :param  aggregation_name: aggregation_name (aggregation path) of the aggregation to find :return an aggregation object if found :raises ObjectDoesNotExist if no matching aggregation is found</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def get_aggregation_by_aggregation_name(self, aggregation_name):\n    \"\"\"Get an aggregation that matches the aggregation dataset_name specified by *dataset_name*\n    :param  aggregation_name: aggregation_name (aggregation path) of the aggregation to find\n    :return an aggregation object if found\n    :raises ObjectDoesNotExist if no matching aggregation is found\n    \"\"\"\n    for aggregation in self.logical_files:\n        if aggregation.aggregation_name == aggregation_name:\n            return aggregation\n\n    raise ObjectDoesNotExist(\"No matching aggregation was found for \"\n                             \"name:{}\".format(aggregation_name))\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.get_aggregation_by_meta_file","title":"<code>get_aggregation_by_meta_file(meta_file_path)</code>","text":"<p>Get an aggregation that matches the specified meta xml/json file path :param  meta_file_path: directory path of the meta xml/json file :return an aggregation object if found :raises ObjectDoesNotExist if no matching aggregation is found</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def get_aggregation_by_meta_file(self, meta_file_path: str) -&gt; AggregationType:\n    \"\"\"Get an aggregation that matches the specified meta xml/json file path\n    :param  meta_file_path: directory path of the meta xml/json file\n    :return an aggregation object if found\n    :raises ObjectDoesNotExist if no matching aggregation is found\n    \"\"\"\n    if __debug__:\n        assert any(meta_file_path.endswith(ext) for ext in AggregationMetaFilePath)\n\n    meta_file_path = self.get_relative_path(meta_file_path)\n    folder, base = os.path.split(meta_file_path)\n    if base.endswith(AggregationMetaFilePath.METADATA_FILE_ENDSWITH.value):\n        base_file_name_to_match = base[:-len(AggregationMetaFilePath.METADATA_FILE_ENDSWITH.value)]\n    elif base.endswith(AggregationMetaFilePath.RESMAP_FILE_ENDSWITH.value):\n        base_file_name_to_match = base[:-len(AggregationMetaFilePath.RESMAP_FILE_ENDSWITH.value)]\n    else:\n        base_file_name_to_match = base[:-len(AggregationMetaFilePath.SCHEMA_JSON_FILE_ENDSWITH)]\n\n    for res_file in ResourceFile.list_folder(self, folder=folder, sub_folders=False):\n        if res_file.has_logical_file:\n            file_name_to_match = f\"{base_file_name_to_match}{res_file.extension}\"\n            if res_file.file_name == file_name_to_match:\n                return res_file.logical_file\n    # check for folder aggregation\n    if folder and folder == base_file_name_to_match:\n        folder_aggregation = self.get_folder_aggregation_object(folder)\n        if folder_aggregation is not None:\n            return folder_aggregation\n\n    raise ObjectDoesNotExist(\"No matching aggregation was found for \"\n                             \"meta file path:{}\".format(meta_file_path))\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.get_aggregation_by_name","title":"<code>get_aggregation_by_name(name, aggregations=None)</code>","text":"<p>Get an aggregation that matches the aggregation name specified by name :param  name: name (aggregation path) of the aggregation to find :param  aggregations:   a list of aggregations in the resource (self) :return an aggregation object if found :raises ObjectDoesNotExist if no matching aggregation is found</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def get_aggregation_by_name(self, name, aggregations=None):\n    \"\"\"Get an aggregation that matches the aggregation name specified by *name*\n    :param  name: name (aggregation path) of the aggregation to find\n    :param  aggregations:   a list of aggregations in the resource (self)\n    :return an aggregation object if found\n    :raises ObjectDoesNotExist if no matching aggregation is found\n    \"\"\"\n    # check if aggregation path *name* is a file path or a folder\n    is_aggr_path_a_folder = self.is_path_folder(path=name)\n    if is_aggr_path_a_folder:\n        folder_full_path = os.path.join(self.file_path, name)\n        aggregation = self.get_folder_aggregation_object(folder_full_path, aggregations=aggregations)\n        if aggregation is None:\n            raise ObjectDoesNotExist(\n                \"No matching aggregation was found for name:{}\".format(name))\n        return aggregation\n    else:\n        folder, base = os.path.split(name)\n        res_file = ResourceFile.get(self, file=base, folder=folder)\n        if res_file.has_logical_file:\n            return res_file.logical_file\n\n        raise ObjectDoesNotExist(\n            \"No matching aggregation was found for name:{}\".format(name))\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.get_data_services_urls","title":"<code>get_data_services_urls()</code>","text":"<p>Generates data services URLs for the resource. If the resource contains any GeoFeature or GeoRaster content, and if it's public, generate data service endpoints. If the resource contains any multidimensional content and it's public, generate THREDDS catalog service endpoint as well.</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def get_data_services_urls(self):\n    \"\"\"\n    Generates data services URLs for the resource.\n    If the resource contains any GeoFeature or GeoRaster content, and if it's public,\n    generate data service endpoints.\n    If the resource contains any multidimensional content and it's public,\n    generate THREDDS catalog service endpoint as well.\n    \"\"\"\n    wfs_url = None\n    wms_url = None\n    wcs_url = None\n    thredds_url = None\n    if self.raccess.public:\n        try:\n            resource_data_types = [lf.data_type for lf in self.logical_files]\n            service_url = (\n                f'{settings.HSWS_GEOSERVER_URL}/HS-{self.short_id}/'\n                + '{}?request=GetCapabilities'\n            )\n            if 'GeographicFeature' in resource_data_types:\n                wfs_url = service_url.format('wfs')\n                wms_url = service_url.format('wms')\n            if 'GeographicRaster' in resource_data_types:\n                wcs_url = service_url.format('wcs')\n                wms_url = service_url.format('wms')\n        except Exception as e:\n            logger.exception(\"get_data_services_urls: \" + str(e))\n\n        if 'Multidimensional' in resource_data_types:\n            thredds_url = (\n                f'{settings.THREDDS_SERVER_URL}catalog/hydroshare/resources/{self.short_id}/data/contents/'\n                f'catalog.html'\n            )\n    data_services_urls = {\n        'wms_url': wms_url,\n        'wfs_url': wfs_url,\n        'wcs_url': wcs_url,\n        'thredds_url': thredds_url\n    }\n\n    return data_services_urls\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.get_file_aggregation_object","title":"<code>get_file_aggregation_object(file_path)</code>","text":"<p>Returns an aggregation (file type) object if the specified file file_path represents a file type aggregation (logical file), otherwise None.</p> <p>:param file_path: Resource file path (full file path starting with resource id) for which the aggregation object to be retrieved</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def get_file_aggregation_object(self, file_path):\n    \"\"\"Returns an aggregation (file type) object if the specified file *file_path* represents a\n     file type aggregation (logical file), otherwise None.\n\n     :param file_path: Resource file path (full file path starting with resource id)\n     for which the aggregation object to be retrieved\n    \"\"\"\n    relative_file_path = self.get_relative_path(file_path)\n    folder, base = os.path.split(relative_file_path)\n    try:\n        res_file = ResourceFile.get(self, file=base, folder=folder)\n        if res_file.has_logical_file:\n            return res_file.logical_file\n        return None\n    except ObjectDoesNotExist:\n        return None\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.get_fileset_aggregation_in_path","title":"<code>get_fileset_aggregation_in_path(path, aggregations=None)</code>","text":"<p>Get the first fileset aggregation in the path moving up (towards the root)in the path :param  path: directory path in which to search for a fileset aggregation :param  aggregations: a list of aggregations in the resource (self) :return a fileset aggregation object if found, otherwise None</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def get_fileset_aggregation_in_path(self, path, aggregations=None):\n    \"\"\"Get the first fileset aggregation in the path moving up (towards the root)in the path\n    :param  path: directory path in which to search for a fileset aggregation\n    :param  aggregations: a list of aggregations in the resource (self)\n    :return a fileset aggregation object if found, otherwise None\n    \"\"\"\n\n    path = self.get_relative_path(path)\n    aggregations = self._cache_aggregations(aggregations=aggregations)\n    if not aggregations:\n        # no aggregations exist in this resource\n        return None\n\n    def get_fileset(_path):\n        try:\n            aggregation = self.get_aggregation_by_name(_path, aggregations=aggregations)\n            if aggregation.is_fileset:\n                return aggregation\n        except ObjectDoesNotExist:\n            return None\n\n    while '/' in path:\n        fileset = get_fileset(path)\n        if fileset is not None:\n            return fileset\n        path = os.path.dirname(path)\n    else:\n        return get_fileset(path)\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.get_folder_aggregation_in_path","title":"<code>get_folder_aggregation_in_path(dir_path, aggregations=None)</code>","text":"<p>Gets any aggregation that is based on folder and exists in the specified path Searches for a folder based aggregation moving towards the root of the specified path :param  dir_path: directory path in which to search for a folder based aggregation :param aggregations: a list of all aggregations in self (this resource) :return a folder based aggregation if found otherwise, None</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def get_folder_aggregation_in_path(self, dir_path, aggregations=None):\n    \"\"\"Gets any aggregation that is based on folder and exists in the specified path\n    Searches for a folder based aggregation moving towards the root of the specified path\n    :param  dir_path: directory path in which to search for a folder based aggregation\n    :param aggregations: a list of all aggregations in self (this resource)\n    :return a folder based aggregation if found otherwise, None\n    \"\"\"\n\n    dir_path = self.get_relative_path(dir_path)\n    aggregations = self._cache_aggregations(aggregations=aggregations)\n    if not aggregations:\n        # no aggregations exist in this resource\n        return None\n\n    def get_aggregation(path):\n        try:\n            aggregation = self.get_aggregation_by_name(path, aggregations=aggregations)\n            return aggregation\n        except ObjectDoesNotExist:\n            return None\n\n    while '/' in dir_path:\n        aggr = get_aggregation(dir_path)\n        if aggr is not None:\n            return aggr\n        dir_path = os.path.dirname(dir_path)\n    else:\n        return get_aggregation(dir_path)\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.get_folder_aggregation_object","title":"<code>get_folder_aggregation_object(dir_path, aggregations=None)</code>","text":"<p>Returns an aggregation (file type) object if the specified folder dir_path represents a file type aggregation (logical file), otherwise None.</p> <p>:param dir_path: Resource file directory path (full folder path starting with resource id) for which the aggregation object to be retrieved :param aggregations:   list of all aggregations in self (this resource)</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def get_folder_aggregation_object(self, dir_path, aggregations=None):\n    \"\"\"Returns an aggregation (file type) object if the specified folder *dir_path* represents a\n     file type aggregation (logical file), otherwise None.\n\n     :param dir_path: Resource file directory path (full folder path starting with resource id)\n     for which the aggregation object to be retrieved\n     :param aggregations:   list of all aggregations in self (this resource)\n    \"\"\"\n\n    aggregation_path = self.get_relative_path(dir_path)\n    logical_files = self._cache_aggregations(aggregations=aggregations)\n    for lf in logical_files:\n        if hasattr(lf, 'folder'):\n            if lf.folder == aggregation_path:\n                return lf\n    return None\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.get_logical_files","title":"<code>get_logical_files(logical_file_class_name)</code>","text":"<p>Get a list of logical files (aggregations) for a specified logical file class name.</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def get_logical_files(self, logical_file_class_name):\n    \"\"\"Get a list of logical files (aggregations) for a specified logical file class name.\"\"\"\n\n    class_name_to_query_mappings = {\n        FileSetLogicalFile.type_name(): self.filesetlogicalfile_set.all(),\n        GenericLogicalFile.type_name(): self.genericlogicalfile_set.all(),\n        GeoFeatureLogicalFile.type_name(): self.geofeaturelogicalfile_set.all(),\n        GeoRasterLogicalFile.type_name(): self.georasterlogicalfile_set.all(),\n        ModelInstanceLogicalFile.type_name(): self.modelinstancelogicalfile_set.all(),\n        ModelProgramLogicalFile.type_name(): self.modelprogramlogicalfile_set.all(),\n        NetCDFLogicalFile.type_name(): self.netcdflogicalfile_set.all(),\n        TimeSeriesLogicalFile.type_name(): self.timeserieslogicalfile_set.all(),\n        RefTimeseriesLogicalFile.type_name(): self.reftimeserieslogicalfile_set.all(),\n        CSVLogicalFile.type_name(): self.csvlogicalfile_set.all(),\n    }\n\n    if logical_file_class_name in class_name_to_query_mappings:\n        return class_name_to_query_mappings[logical_file_class_name]\n\n    raise Exception(f\"Invalid logical file type name:{logical_file_class_name}\")\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.get_model_aggregation_in_path","title":"<code>get_model_aggregation_in_path(path, aggregations=None)</code>","text":"<p>Get the model program or model instance aggregation in the path moving up (towards the root)in the path :param  path: directory path in which to search for a model program or model instance aggregation :param  aggregations: a list of aggregations in the resource (self) :return a model program or model instance aggregation object if found, otherwise None</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def get_model_aggregation_in_path(self, path, aggregations=None):\n    \"\"\"Get the model program or model instance aggregation in the path moving up (towards the root)in the path\n    :param  path: directory path in which to search for a model program or model instance aggregation\n    :param  aggregations: a list of aggregations in the resource (self)\n    :return a model program or model instance aggregation object if found, otherwise None\n    \"\"\"\n\n    path = self.get_relative_path(path)\n    aggregations = self._cache_aggregations(aggregations=aggregations)\n    if not aggregations:\n        # no aggregations exist in this resource\n        return None\n\n    def get_aggregation(_path):\n        try:\n            aggregation = self.get_aggregation_by_name(_path, aggregations=aggregations)\n            return aggregation\n        except ObjectDoesNotExist:\n            return None\n\n    while '/' in path:\n        aggr = get_aggregation(path)\n        if aggr is not None and (aggr.is_model_program or aggr.is_model_instance):\n            return aggr\n        path = os.path.dirname(path)\n    else:\n        aggr = get_aggregation(path)\n        if aggr is not None and (aggr.is_model_program or aggr.is_model_instance):\n            return aggr\n        return None\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.is_aggregation_xml_file","title":"<code>is_aggregation_xml_file(file_path)</code>","text":"<p>determine whether a given file in the file hierarchy is metadata.</p> <p>This is true if it is listed as metadata in any logical file.</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def is_aggregation_xml_file(self, file_path):\n    \"\"\" determine whether a given file in the file hierarchy is metadata.\n\n    This is true if it is listed as metadata in any logical file.\n    \"\"\"\n    if not self.is_metadata_xml_file(file_path):\n        return False\n    for logical_file in self.logical_files:\n        if logical_file.metadata_file_path == file_path or \\\n                logical_file.map_file_path == file_path:\n            return True\n    return False\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.remove_aggregation_from_file","title":"<code>remove_aggregation_from_file(moved_res_file, src_folder, tgt_folder, aggregations=None, cleanup=True)</code>","text":"<p>removes association with aggregation (fileset or model program) from a resource file that has been moved :param  moved_res_file: an instance of a ResourceFile which has been moved to a different folder :param  src_folder: folder from which the file got moved from :param  tgt_folder: folder to which the file got moved into :param aggregations:   list of all aggregations in self (this resource) :param cleanup: if True, cleanup aggregation if aggregation is empty after the file removal</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def remove_aggregation_from_file(self, moved_res_file, src_folder, tgt_folder, aggregations=None, cleanup=True):\n    \"\"\"removes association with aggregation (fileset or model program) from a resource file that has been moved\n    :param  moved_res_file: an instance of a ResourceFile which has been moved to a different folder\n    :param  src_folder: folder from which the file got moved from\n    :param  tgt_folder: folder to which the file got moved into\n    :param aggregations:   list of all aggregations in self (this resource)\n    :param cleanup: if True, cleanup aggregation if aggregation is empty after the file removal\n    \"\"\"\n\n    if moved_res_file.file_folder:\n        aggregation = self.get_folder_aggregation_in_path(moved_res_file.file_folder, aggregations=aggregations)\n        if aggregation is None:\n            return\n\n        # aggregation must be one of 'fileset', 'model instance' or 'model program'\n        if aggregation == moved_res_file.logical_file:\n            # remove aggregation association with the file\n            # the removed aggregation is a fileset aggregation or a model program or a model instance\n            # aggregation based on folder (note: model program/instance aggregation can also be\n            # created from a single file)\n            moved_res_file.logical_file_content_object = None\n            moved_res_file.save()\n            # delete any instance of ModelProgramResourceFileType associated with this moved file\n            if aggregation.is_model_program:\n                # if the file is getting moved within a model program folder hierarchy then no need\n                # to delete any associated ModelProgramResourceFileType object\n                if not tgt_folder.startswith(src_folder) and not src_folder.startswith(tgt_folder):\n                    ModelProgramResourceFileType.objects.filter(res_file=moved_res_file).delete()\n            if cleanup:\n                self.cleanup_aggregations()\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.set_flag_to_recreate_aggregation_meta_files","title":"<code>set_flag_to_recreate_aggregation_meta_files(orig_path, new_path)</code>","text":"<p>When a folder or file representing an aggregation is renamed or moved, the associated meta files (resource map, metadata xml files as well as schema json files) are deleted and then aggregation metadata is set to dirty so that these meta files will be regenerated as part of aggregation or bag download :param  orig_path: original file/folder path prior to move/rename :param  new_path: new file/folder path after move/rename</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def set_flag_to_recreate_aggregation_meta_files(self, orig_path, new_path):\n    \"\"\"\n    When a folder or file representing an aggregation is renamed or moved,\n    the associated meta files (resource map, metadata xml files as well as schema json files) are deleted\n    and then aggregation metadata is set to dirty so that these meta files will be regenerated as part of\n    aggregation or bag download\n    :param  orig_path: original file/folder path prior to move/rename\n    :param  new_path: new file/folder path after move/rename\n    \"\"\"\n    aggregations = list(self.logical_files)\n\n    def set_parent_aggregation_dirty(path_to_search):\n        if '/' in path_to_search:\n            path = os.path.dirname(path_to_search)\n            try:\n                parent_aggr = self.get_aggregation_by_name(path, aggregations=aggregations)\n                parent_aggr.set_metadata_dirty()\n            except ObjectDoesNotExist:\n                pass\n\n    new_path = self.get_relative_path(new_path)\n    orig_path = self.get_relative_path(orig_path)\n    is_new_path_a_folder = self.is_path_folder(path=new_path)\n    istorage = self.get_s3_storage()\n\n    # remove file extension from aggregation name (note: aggregation name is a file path\n    # for all aggregation types except fileset/model aggregation\n    file_name, _ = os.path.splitext(orig_path)\n    schema_json_file_name = file_name + AggregationMetaFilePath.SCHEMA_JSON_FILE_ENDSWITH.value\n    meta_xml_file_name = file_name + AggregationMetaFilePath.METADATA_FILE_ENDSWITH.value\n    map_xml_file_name = file_name + AggregationMetaFilePath.RESMAP_FILE_ENDSWITH.value\n    if not is_new_path_a_folder:\n        # case of file rename/move for single file aggregation\n        schema_json_file_full_path = os.path.join(self.file_path, schema_json_file_name)\n        meta_xml_file_full_path = os.path.join(self.file_path, meta_xml_file_name)\n        map_xml_file_full_path = os.path.join(self.file_path, map_xml_file_name)\n        # for single file aggregations, compute the metadata JSON file path using original path\n        metadata_json_file_full_path = os.path.join(\n            self.file_path, orig_path + AggregationMetaFilePath.METADATA_JSON_FILE_ENDSWITH.value\n        )\n    else:\n        # case of folder rename - fileset/model aggregation\n        _, schema_json_file_name = os.path.split(schema_json_file_name)\n        _, meta_xml_file_name = os.path.split(meta_xml_file_name)\n        _, map_xml_file_name = os.path.split(map_xml_file_name)\n        schema_json_file_full_path = os.path.join(self.file_path, new_path, schema_json_file_name)\n        meta_xml_file_full_path = os.path.join(self.file_path, new_path, meta_xml_file_name)\n        map_xml_file_full_path = os.path.join(self.file_path, new_path, map_xml_file_name)\n        # No need to delete metadata JSON files for folder rename cases as metadata JSON filename\n        # is not affected by folder rename\n        metadata_json_file_full_path = None\n\n    if istorage.exists(schema_json_file_full_path):\n        istorage.delete(schema_json_file_full_path)\n\n    if istorage.exists(meta_xml_file_full_path):\n        istorage.delete(meta_xml_file_full_path)\n\n    if istorage.exists(map_xml_file_full_path):\n        istorage.delete(map_xml_file_full_path)\n\n    # delete metadata JSON file only for single file aggregations\n    if (\n        not is_new_path_a_folder\n        and metadata_json_file_full_path is not None\n        and istorage.exists(metadata_json_file_full_path)\n    ):\n        istorage.delete(metadata_json_file_full_path)\n\n    # set affected logical file metadata to dirty so that xml meta files will be regenerated at the time of\n    # aggregation or bag download\n    for lf in aggregations:\n        # set metadata dirty for any folder based aggregations under the orig_path\n        if hasattr(lf, 'folder'):\n            if lf.folder is not None and lf.folder.startswith(orig_path):\n                lf.folder = os.path.join(new_path, lf.folder[len(orig_path) + 1:]).strip('/')\n                lf.save(update_fields=[\"folder\"])\n                lf.set_metadata_dirty()\n                continue\n\n        # set metadata dirty for any non-folder based aggregation under the orig_path\n        if lf.aggregation_name.startswith(orig_path):\n            lf.set_metadata_dirty()\n\n        # set metadata to dirty for non-folder based aggregation under the new_path\n        if lf.aggregation_name.startswith(new_path):\n            lf.set_metadata_dirty()\n\n    # set metadata to dirty for any parent aggregation that may exist relative to path *orig_path*\n    set_parent_aggregation_dirty(orig_path)\n\n    # set metadata to dirty for any parent aggregation that may exist relative to path *new_path*\n    set_parent_aggregation_dirty(new_path)\n\n    try:\n        aggregation = self.get_aggregation_by_name(new_path, aggregations=aggregations)\n        aggregation.set_metadata_dirty()\n    except ObjectDoesNotExist:\n        # the file path *new_path* does not represent an aggregation - no more\n        # action is needed\n        pass\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.supports_delete_folder_on_zip","title":"<code>supports_delete_folder_on_zip(original_folder)</code>","text":"<p>check if the specified folder can be deleted at the end of zipping that folder</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def supports_delete_folder_on_zip(self, original_folder):\n    \"\"\"check if the specified folder can be deleted at the end of zipping that folder\"\"\"\n\n    # find all the resource files in the folder to be deleted\n    # this is being passed both qualified and unqualified paths!\n    full_path = original_folder\n    if not full_path.startswith(self.file_path):\n        full_path = os.path.join(self.file_path, full_path)\n\n    # get all resource files at full_path and its sub-folders\n    res_file_objects = ResourceFile.list_folder(self, full_path)\n\n    # check any logical file associated with the resource file supports deleting the folder\n    # after its zipped\n    for res_file in res_file_objects:\n        if res_file.has_logical_file:\n            if not res_file.logical_file.supports_delete_folder_on_zip:\n                return False\n    return True\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.supports_rename_path","title":"<code>supports_rename_path(src_full_path, tgt_full_path)</code>","text":"<p>checks if file/folder rename/move is allowed :param  src_full_path: name of the file/folder storage path to be renamed (path starts with resource id) :param  tgt_full_path: new name for file/folder storage path (path starts with resource id) :return True or False</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def supports_rename_path(self, src_full_path, tgt_full_path):\n    \"\"\"checks if file/folder rename/move is allowed\n    :param  src_full_path: name of the file/folder storage path to be renamed (path starts with resource id)\n    :param  tgt_full_path: new name for file/folder storage path (path starts with resource id)\n    :return True or False\n    \"\"\"\n\n    if __debug__:\n        assert src_full_path.startswith(self.file_path)\n        assert tgt_full_path.startswith(self.file_path)\n\n    # need to find out which of the following actions the user is trying to do:\n    # renaming a file\n    # renaming a folder\n    # moving a file\n    # moving a folder\n    is_renaming_file = False\n    is_moving_file = False\n    is_moving_folder = False\n\n    if tgt_full_path == self.file_path:\n        # at the root of the resource all file operations are allowed\n        return True\n\n    istorage = self.get_s3_storage()\n    scr_base_name = os.path.basename(src_full_path)\n    src_dir_path = os.path.dirname(src_full_path)\n    tgt_dir_path = os.path.dirname(tgt_full_path)\n\n    if istorage.isFile(src_full_path):\n        if istorage.exists(tgt_full_path) or tgt_full_path.endswith(scr_base_name):\n            is_moving_file = True\n            if tgt_full_path.endswith(scr_base_name):\n                tgt_dir_path = os.path.dirname(tgt_full_path)\n            else:\n                tgt_dir_path = tgt_full_path\n        else:\n            is_renaming_file = True\n    else:\n        # src path is a directory\n        if src_dir_path == tgt_dir_path:\n            # renaming folder - no restriction\n            return True\n        else:\n            is_moving_folder = True\n\n    def check_src_aggregation(src_aggr):\n        \"\"\"checks if the aggregation at the source allows file rename/move action\"\"\"\n        if src_aggr is not None:\n            if is_renaming_file:\n                return src_aggr.supports_resource_file_rename\n            elif is_moving_file:\n                return src_aggr.supports_resource_file_move\n        return True\n\n    if is_renaming_file or is_moving_file:\n        # see if the folder containing the file represents an aggregation\n        src_aggr = self.get_file_aggregation_object(file_path=src_full_path)\n        if check_src_aggregation(src_aggr):\n            # check target\n            if is_moving_file:\n                tgt_aggr = self.get_folder_aggregation_in_path(dir_path=tgt_dir_path)\n                if tgt_aggr is not None:\n                    if src_aggr is None:\n                        return tgt_aggr.supports_resource_file_move\n                    else:\n                        return tgt_aggr.can_contain_aggregation(src_aggr)\n                return True\n            return True\n        return False\n\n    if is_moving_folder:\n        src_aggr = self.get_folder_aggregation_in_path(dir_path=src_full_path)\n        if src_aggr is not None:\n            if src_aggr.supports_resource_file_move:\n                tgt_aggr = self.get_folder_aggregation_in_path(dir_path=tgt_full_path)\n                if tgt_aggr is not None:\n                    return tgt_aggr.supports_resource_file_move and tgt_aggr.can_contain_aggregation(src_aggr)\n                return True\n        tgt_aggr = self.get_folder_aggregation_in_path(dir_path=tgt_full_path)\n        if tgt_aggr is not None:\n            return tgt_aggr.supports_resource_file_move\n        return True\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.supports_zip","title":"<code>supports_zip(folder_to_zip)</code>","text":"<p>check if the given folder can be zipped or not</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def supports_zip(self, folder_to_zip):\n    \"\"\"check if the given folder can be zipped or not\"\"\"\n\n    # find all the resource files in the folder to be zipped\n    # this is being passed both qualified and unqualified paths!\n\n    full_path = folder_to_zip\n    if not full_path.startswith(self.file_path):\n        full_path = os.path.join(self.file_path, full_path)\n    # get all resource files at full_path and its sub-folders\n    res_file_objects = ResourceFile.list_folder(self, full_path)\n\n    # check any logical file associated with the resource file supports zip functionality\n    for res_file in res_file_objects:\n        if res_file.has_logical_file:\n            if not res_file.logical_file.supports_zip:\n                return False\n    return True\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.update_coverage","title":"<code>update_coverage()</code>","text":"<p>Update resource spatial and temporal coverage based on the corresponding coverages from all the contained aggregations (logical file) only if the resource coverage is not already set</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def update_coverage(self):\n    \"\"\"Update resource spatial and temporal coverage based on the corresponding coverages\n    from all the contained aggregations (logical file) only if the resource coverage is not\n    already set\"\"\"\n\n    # update resource spatial coverage only if there is no spatial coverage already\n    if self.metadata.spatial_coverage is None:\n        self.update_spatial_coverage()\n\n    # update resource temporal coverage only if there is no temporal coverage already\n    if self.metadata.temporal_coverage is None:\n        self.update_temporal_coverage()\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.update_spatial_coverage","title":"<code>update_spatial_coverage()</code>","text":"<p>Updates resource spatial coverage based on the contained spatial coverages of aggregations (file type). Note: This action will overwrite any existing resource spatial coverage data.</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def update_spatial_coverage(self):\n    \"\"\"Updates resource spatial coverage based on the contained spatial coverages of\n    aggregations (file type). Note: This action will overwrite any existing resource spatial\n    coverage data.\n    \"\"\"\n\n    update_target_spatial_coverage(self)\n</code></pre>"},{"location":"hs_composite_resource/models/#hs_composite_resource.models.CompositeResource.update_temporal_coverage","title":"<code>update_temporal_coverage()</code>","text":"<p>Updates resource temporal coverage based on the contained temporal coverages of aggregations (file type). Note: This action will overwrite any existing resource temporal coverage data.</p> Source code in <code>hs_composite_resource/models.py</code> <pre><code>def update_temporal_coverage(self):\n    \"\"\"Updates resource temporal coverage based on the contained temporal coverages of\n    aggregations (file type). Note: This action will overwrite any existing resource temporal\n    coverage data.\n    \"\"\"\n\n    update_target_temporal_coverage(self)\n</code></pre>"},{"location":"hs_composite_resource/views/","title":"Views","text":""},{"location":"hs_composite_resource/views/#hs_composite_resource.views.check_aggregation_files_to_sync","title":"<code>check_aggregation_files_to_sync(request, resource_id, **kwargs)</code>","text":"<p>Checks if there are files in netcdf or timeseries aggregations that need to be updated due to metadata changes by the user :param  request: an instance of HttpRequest :param  resource_id: id of resource for which files in netcdf or timeseries aggregations need to be checked :return an instance of JsonResponse type with data containing the list of files (file paths) that need to be updated</p> Source code in <code>hs_composite_resource/views.py</code> <pre><code>def check_aggregation_files_to_sync(request, resource_id, **kwargs):\n    \"\"\"\n    Checks if there are files in netcdf or timeseries aggregations that need to be updated due to metadata changes by\n    the user\n    :param  request: an instance of HttpRequest\n    :param  resource_id: id of resource for which files in netcdf or timeseries aggregations need to be checked\n    :return an instance of JsonResponse type with data containing the list of files (file paths) that need to be updated\n    \"\"\"\n\n    resource, authorized, _ = authorize(request, resource_id,\n                                        needed_permission=ACTION_TO_AUTHORIZE.EDIT_RESOURCE,\n                                        raises_exception=False)\n    if not authorized:\n        response_data = {\"status\": \"ERROR\", \"error\": \"Permission denied\"}\n        return JsonResponse(response_data, status=status.HTTP_401_UNAUTHORIZED)\n\n    file_paths = {\"nc_files\": [], \"ts_files\": []}\n    if resource.resource_type != \"CompositeResource\":\n        response_data = {\"status\": \"ERROR\", \"error\": \"Resource is not a composite resource\"}\n        return JsonResponse(response_data, status=status.HTTP_400_BAD_REQUEST)\n\n    if resource.raccess.published or resource.raccess.review_pending:\n        # if resource is published or in review, no need to check for files to sync as the user cannot update the files\n        data = {\"status\": \"SUCCESS\", \"files_to_sync\": file_paths}\n        return JsonResponse(data)\n\n    # check netcdf aggregations\n    nc_file_paths = []\n    netcdf_logical_files = resource.get_logical_files('NetCDFLogicalFile')\n    for lf in netcdf_logical_files:\n        if lf.metadata.is_update_file:\n            nc_file_paths.append(lf.aggregation_name)\n    file_paths[\"nc_files\"] = nc_file_paths\n    # check time series aggregations\n    ts_file_paths = []\n    timeseries_logical_files = resource.get_logical_files('TimeSeriesLogicalFile')\n    for lf in timeseries_logical_files:\n        if lf.metadata.is_update_file:\n            ts_file_paths.append(lf.aggregation_name)\n    file_paths[\"ts_files\"] = ts_file_paths\n\n    data = {\"status\": \"SUCCESS\", \"files_to_sync\": file_paths}\n    return JsonResponse(data)\n</code></pre>"},{"location":"hs_composite_resource/views/#hs_composite_resource.views.delete_resource_coverage","title":"<code>delete_resource_coverage(request, resource_id, coverage_type, **kwargs)</code>","text":"<p>Deletes resource coverage :param  request: an instance of HttpRequest :param  resource_id: id of resource for which coverage needs to be deleted :param  coverage_type: a value of either temporal or spatial :return an instance of JsonResponse type</p> Source code in <code>hs_composite_resource/views.py</code> <pre><code>@login_required\ndef delete_resource_coverage(request, resource_id, coverage_type, **kwargs):\n    \"\"\"Deletes resource coverage\n    :param  request: an instance of HttpRequest\n    :param  resource_id: id of resource for which coverage needs to be deleted\n    :param  coverage_type: a value of either temporal or spatial\n    :return an instance of JsonResponse type\n    \"\"\"\n    return _process_resource_coverage_action(request, resource_id, coverage_type, action='delete')\n</code></pre>"},{"location":"hs_composite_resource/views/#hs_composite_resource.views.update_resource_coverage","title":"<code>update_resource_coverage(request, resource_id, coverage_type, **kwargs)</code>","text":"<p>Updates resource coverage based on the coverages of the contained aggregations (file types) :param  request: an instance of HttpRequest :param  resource_id: id of resource for which coverage needs to be updated :param  coverage_type: a value of either temporal or spatial :return an instance of JsonResponse type</p> Source code in <code>hs_composite_resource/views.py</code> <pre><code>@login_required\ndef update_resource_coverage(request, resource_id, coverage_type, **kwargs):\n    \"\"\"Updates resource coverage based on the coverages of the contained aggregations\n    (file types)\n    :param  request: an instance of HttpRequest\n    :param  resource_id: id of resource for which coverage needs to be updated\n    :param  coverage_type: a value of either temporal or spatial\n    :return an instance of JsonResponse type\n    \"\"\"\n\n    return _process_resource_coverage_action(request, resource_id, coverage_type, action='update')\n</code></pre>"},{"location":"hs_core/models/","title":"Models","text":"<p>Declare critical models for Hydroshare hs_core app.</p>"},{"location":"hs_core/models/#hs_core.models.AbstractMetaDataElement","title":"<code>AbstractMetaDataElement</code>","text":"<p>               Bases: <code>Model</code>, <code>RDF_Term_MixIn</code></p> <p>Define abstract class for all metadata elements.</p> Source code in <code>hs_core/models.py</code> <pre><code>class AbstractMetaDataElement(models.Model, RDF_Term_MixIn):\n    \"\"\"Define abstract class for all metadata elements.\"\"\"\n\n    object_id = models.PositiveIntegerField()\n    # see the following link the reason for having the related_name setting\n    # for the content_type attribute\n    # https://docs.djangoproject.com/en/1.6/topics/db/models/#abstract-related-name\n    content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE,\n                                     related_name=\"%(app_label)s_%(class)s_related\")\n    content_object = GenericForeignKey('content_type', 'object_id')\n\n    def __str__(self):\n        \"\"\"Return unicode for python 3 compatibility in templates\"\"\"\n        return self.__unicode__()\n\n    @property\n    def metadata(self):\n        \"\"\"Return content object that describes metadata.\"\"\"\n        return self.content_object\n\n    @property\n    def dict(self):\n        return {self.__class__.__name__: model_to_dict(self)}\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Pass through kwargs to object.create method.\"\"\"\n        return cls.objects.create(**kwargs)\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        \"\"\"Pass through kwargs to update specific metadata object.\"\"\"\n        element = cls.objects.get(id=element_id)\n        for key, value in list(kwargs.items()):\n            setattr(element, key, value)\n        element.save()\n        return element\n\n    @classmethod\n    def remove(cls, element_id):\n        \"\"\"Pass through element id to objects.get and then delete() method.\n\n        Could not name this method as 'delete' since the parent 'Model' class has such a method\n        \"\"\"\n        element = cls.objects.get(id=element_id)\n        element.delete()\n\n    class Meta:\n        \"\"\"Define meta properties for AbstractMetaDataElement class.\"\"\"\n\n        abstract = True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractMetaDataElement.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Return content object that describes metadata.</p>"},{"location":"hs_core/models/#hs_core.models.AbstractMetaDataElement.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for AbstractMetaDataElement class.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for AbstractMetaDataElement class.\"\"\"\n\n    abstract = True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractMetaDataElement.__str__","title":"<code>__str__()</code>","text":"<p>Return unicode for python 3 compatibility in templates</p> Source code in <code>hs_core/models.py</code> <pre><code>def __str__(self):\n    \"\"\"Return unicode for python 3 compatibility in templates\"\"\"\n    return self.__unicode__()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractMetaDataElement.create","title":"<code>create(**kwargs)</code>  <code>classmethod</code>","text":"<p>Pass through kwargs to object.create method.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"Pass through kwargs to object.create method.\"\"\"\n    return cls.objects.create(**kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractMetaDataElement.remove","title":"<code>remove(element_id)</code>  <code>classmethod</code>","text":"<p>Pass through element id to objects.get and then delete() method.</p> <p>Could not name this method as 'delete' since the parent 'Model' class has such a method</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id):\n    \"\"\"Pass through element id to objects.get and then delete() method.\n\n    Could not name this method as 'delete' since the parent 'Model' class has such a method\n    \"\"\"\n    element = cls.objects.get(id=element_id)\n    element.delete()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractMetaDataElement.update","title":"<code>update(element_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Pass through kwargs to update specific metadata object.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef update(cls, element_id, **kwargs):\n    \"\"\"Pass through kwargs to update specific metadata object.\"\"\"\n    element = cls.objects.get(id=element_id)\n    for key, value in list(kwargs.items()):\n        setattr(element, key, value)\n    element.save()\n    return element\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractRelation","title":"<code>AbstractRelation</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define Abstract Relation class.</p> Source code in <code>hs_core/models.py</code> <pre><code>class AbstractRelation(AbstractMetaDataElement):\n    \"\"\"Define Abstract Relation class.\"\"\"\n    SOURCE_TYPES = ()\n    term = 'Relation'\n    type = models.CharField(max_length=100, choices=SOURCE_TYPES)\n    value = models.TextField()\n\n    def __str__(self):\n        \"\"\"Return {type} {value} for string representation.\"\"\"\n        return \"{type} {value}\".format(type=self.type, value=self.value)\n\n    def __unicode__(self):\n        \"\"\"Return {type} {value} for unicode representation (deprecated).\"\"\"\n        return \"{type} {value}\".format(type=self.type, value=self.value)\n\n    @classmethod\n    def get_supported_types(cls):\n        return dict(cls.SOURCE_TYPES).keys()\n\n    def type_description(self):\n        return dict(self.SOURCE_TYPES)[self.type]\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Define custom create method for Relation class.\"\"\"\n        if 'type' not in kwargs:\n            ValidationError(\"Type of relation element is missing.\")\n        if 'value' not in kwargs:\n            ValidationError(\"Value of relation element is missing.\")\n\n        if not kwargs['type'] in list(dict(cls.SOURCE_TYPES).keys()):\n            raise ValidationError('Invalid relation type:%s' % kwargs['type'])\n\n        # ensure isHostedBy and isCopiedFrom are mutually exclusive\n        metadata_obj = kwargs['content_object']\n        metadata_type = ContentType.objects.get_for_model(metadata_obj)\n\n        # avoid creating duplicate element (same type and same value)\n        if cls.objects.filter(type=kwargs['type'],\n                              value=kwargs['value'],\n                              object_id=metadata_obj.id,\n                              content_type=metadata_type).exists():\n            raise ValidationError('Relation element of the same type '\n                                  'and value already exists.')\n\n        return super(AbstractRelation, cls).create(**kwargs)\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        \"\"\"Define custom update method for Relation class.\"\"\"\n        if 'type' not in kwargs:\n            ValidationError(\"Type of relation element is missing.\")\n        if 'value' not in kwargs:\n            ValidationError(\"Value of relation element is missing.\")\n\n        if not kwargs['type'] in list(dict(cls.SOURCE_TYPES).keys()):\n            raise ValidationError('Invalid relation type:%s' % kwargs['type'])\n\n        # avoid changing this relation to an existing relation of same type and same value\n        rel = cls.objects.get(id=element_id)\n        metadata_obj = kwargs['content_object']\n        metadata_type = ContentType.objects.get_for_model(metadata_obj)\n        qs = cls.objects.filter(type=kwargs['type'],\n                                value=kwargs['value'],\n                                object_id=metadata_obj.id,\n                                content_type=metadata_type)\n\n        if qs.exists() and qs.first() != rel:\n            # this update will create a duplicate relation element\n            raise ValidationError('A relation element of the same type and value already exists.')\n\n        super(AbstractRelation, cls).update(element_id, **kwargs)\n\n    class Meta:\n        abstract = True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractRelation.__str__","title":"<code>__str__()</code>","text":"<p>Return {type} {value} for string representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __str__(self):\n    \"\"\"Return {type} {value} for string representation.\"\"\"\n    return \"{type} {value}\".format(type=self.type, value=self.value)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractRelation.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return {type} {value} for unicode representation (deprecated).</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return {type} {value} for unicode representation (deprecated).\"\"\"\n    return \"{type} {value}\".format(type=self.type, value=self.value)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractRelation.create","title":"<code>create(**kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom create method for Relation class.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"Define custom create method for Relation class.\"\"\"\n    if 'type' not in kwargs:\n        ValidationError(\"Type of relation element is missing.\")\n    if 'value' not in kwargs:\n        ValidationError(\"Value of relation element is missing.\")\n\n    if not kwargs['type'] in list(dict(cls.SOURCE_TYPES).keys()):\n        raise ValidationError('Invalid relation type:%s' % kwargs['type'])\n\n    # ensure isHostedBy and isCopiedFrom are mutually exclusive\n    metadata_obj = kwargs['content_object']\n    metadata_type = ContentType.objects.get_for_model(metadata_obj)\n\n    # avoid creating duplicate element (same type and same value)\n    if cls.objects.filter(type=kwargs['type'],\n                          value=kwargs['value'],\n                          object_id=metadata_obj.id,\n                          content_type=metadata_type).exists():\n        raise ValidationError('Relation element of the same type '\n                              'and value already exists.')\n\n    return super(AbstractRelation, cls).create(**kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractRelation.update","title":"<code>update(element_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom update method for Relation class.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef update(cls, element_id, **kwargs):\n    \"\"\"Define custom update method for Relation class.\"\"\"\n    if 'type' not in kwargs:\n        ValidationError(\"Type of relation element is missing.\")\n    if 'value' not in kwargs:\n        ValidationError(\"Value of relation element is missing.\")\n\n    if not kwargs['type'] in list(dict(cls.SOURCE_TYPES).keys()):\n        raise ValidationError('Invalid relation type:%s' % kwargs['type'])\n\n    # avoid changing this relation to an existing relation of same type and same value\n    rel = cls.objects.get(id=element_id)\n    metadata_obj = kwargs['content_object']\n    metadata_type = ContentType.objects.get_for_model(metadata_obj)\n    qs = cls.objects.filter(type=kwargs['type'],\n                            value=kwargs['value'],\n                            object_id=metadata_obj.id,\n                            content_type=metadata_type)\n\n    if qs.exists() and qs.first() != rel:\n        # this update will create a duplicate relation element\n        raise ValidationError('A relation element of the same type and value already exists.')\n\n    super(AbstractRelation, cls).update(element_id, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource","title":"<code>AbstractResource</code>","text":"<p>               Bases: <code>ResourcePermissionsMixin</code>, <code>ResourceS3Mixin</code></p> <p>Create Abstract Class for all Resources.</p> <p>All hydroshare objects inherit from this mixin.  It defines things that must be present to be considered a hydroshare resource.  Additionally, all hydroshare resources should inherit from Page.  This gives them what they need to be represented in the Mezzanine CMS.</p> <p>In some cases, it is possible that the order of inheritence matters.  Best practice dictates that you list pages.Page first and then other classes:</p> <pre><code>class MyResourceContentType(pages.Page, hs_core.AbstractResource):\n    ...\n</code></pre> Source code in <code>hs_core/models.py</code> <pre><code>class AbstractResource(ResourcePermissionsMixin, ResourceS3Mixin):\n    \"\"\"\n    Create Abstract Class for all Resources.\n\n    All hydroshare objects inherit from this mixin.  It defines things that must\n    be present to be considered a hydroshare resource.  Additionally, all\n    hydroshare resources should inherit from Page.  This gives them what they\n    need to be represented in the Mezzanine CMS.\n\n    In some cases, it is possible that the order of inheritence matters.  Best\n    practice dictates that you list pages.Page first and then other classes:\n\n        class MyResourceContentType(pages.Page, hs_core.AbstractResource):\n            ...\n    \"\"\"\n\n    content = models.TextField()  # the field added for use by Django inplace editing\n    last_changed_by = models.ForeignKey(User, on_delete=models.CASCADE,\n                                        help_text='The person who last changed the resource',\n                                        related_name='last_changed_%(app_label)s_%(class)s',\n                                        null=False,\n                                        default=1\n                                        )\n\n    files = GenericRelation('hs_core.ResourceFile',\n                            help_text='The files associated with this resource',\n                            for_concrete_model=True)\n\n    file_unpack_status = models.CharField(max_length=7,\n                                          null=True, blank=True,\n                                          choices=(('Pending', 'Pending'), ('Running', 'Running'),\n                                                   ('Done', 'Done'), ('Error', 'Error'))\n                                          )\n    file_unpack_message = models.TextField(null=True, blank=True)\n\n    short_id = models.CharField(max_length=32, default=short_id, db_index=True)\n    doi = models.CharField(max_length=128, null=False, blank=True, db_index=True, default='',\n                           help_text='Permanent identifier. Never changes once it\\'s been set.')\n    comments = CommentsField()\n    rating = RatingField()\n\n    # this is to establish a relationship between a resource and\n    # any metadata container object (e.g., CoreMetaData object)\n    object_id = models.PositiveIntegerField(null=True, blank=True)\n    content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE, null=True, blank=True)\n    content_object = GenericForeignKey('content_type', 'object_id')\n\n    # key/value metadata (additional metadata)\n    extra_metadata = HStoreField(default=dict)\n\n    # this field is for resources to store extra key:value pairs as needed, e.g., bag checksum is stored as\n    # \"bag_checksum\":value pair for published resources in order to meet the DataONE data distribution needs\n    # for internal use only\n    # this field WILL NOT get recorded in bag and SHOULD NEVER be used for storing metadata\n    extra_data = HStoreField(default=dict)\n\n    # cache metadata for performance optimization of my-resources page\n    cached_metadata = models.JSONField(default=dict)\n\n    # for tracking number of times resource and its files have been downloaded\n    download_count = models.PositiveIntegerField(default=0)\n    bag_last_downloaded = models.DateTimeField(null=True, blank=True)\n    # for tracking number of times resource has been viewed\n    view_count = models.PositiveIntegerField(default=0)\n\n    # for tracking when resourceFiles were last compared with S3\n    files_checked = models.DateTimeField(null=True)\n    repaired = models.DateTimeField(null=True)\n\n    # allow resource that contains spam_patterns to be discoverable/public\n    spam_allowlisted = models.BooleanField(default=False)\n\n    quota_holder = models.ForeignKey(User, on_delete=models.SET_NULL, null=True, blank=True,\n                                     related_name='quota_holder')\n\n    def save(self, *args, **kwargs):\n        \"\"\"Refresh cached metadata before resource is saved to prevent stale cached metadata in memory getting\n        written to DB\"\"\"\n        # Only refresh cached_metadata from DB if object already exists\n        if not self._state.adding:\n            self.refresh_from_db(fields=['cached_metadata'])\n        super(AbstractResource, self).save(*args, **kwargs)\n\n    def update_view_count(self):\n        self.view_count += 1\n        # using update query api to update instead of self.save() to avoid triggering solr realtime indexing\n        type(self).objects.filter(id=self.id).update(view_count=self.view_count)\n\n    def update_download_count(self):\n        self.download_count += 1\n        # using update query api to update instead of self.save() to avoid triggering solr realtime indexing\n        type(self).objects.filter(id=self.id).update(download_count=self.download_count)\n\n    def update_cached_metadata_field(self, field_name):\n        \"\"\"\n        Update a specific field in the cached metadata or all fields if 'all' is specified\n\n        :param field_name: The field to update ('creator', 'subject', etc.). If 'all', update all fields.\n\n        NOTE: This method gets called from post_save and post_delete signal handler for\n        any core metadata elements. We need to update the 'modified' field in cached metadata\n        for any change to core metadata elements. The Date metadata element gets updated for the\n        modified date type by the system as needed. We are depending on that change to modified date\n        to update the 'modified' field in cached metadata.\n        \"\"\"\n        self.refresh_from_db()\n        metadata = self.metadata\n        copied_metadata = copy.deepcopy(self.cached_metadata)\n\n        # These are the fields that need to be updated in cached metadata\n        field_updaters = {\n            'creator': self._update_creators_field,\n            'contributor': self._update_contributors_field,\n            'coverage': self._update_coverage_field,\n            'title': self._update_title_field,\n            'subject': self._update_subjects_field,\n            'date': self._update_date_field,\n            'status': self._update_status_field,\n            'description': self._update_abstract_field,\n            'relation': self._update_relation_field,\n            'geospatialrelation': self._update_geospatialrelation_field,\n            'fundingagency': self._update_fundingagency_field,\n            'rights': self._update_rights_field,\n            'language': self._update_language_field,\n            'identifier': self._update_identifier_field,\n            'type': self._update_type_field,\n            'publisher': self._update_publisher_field\n        }\n\n        # Update all fields if 'all' is specified\n        if field_name == 'all':\n            for updater in field_updaters.values():\n                updater(copied_metadata, metadata)\n        else:\n            # Update the specified field\n            if field_name in field_updaters:\n                field_updaters[field_name](copied_metadata, metadata)\n\n        # Ensure all required fields are present\n        self._ensure_required_fields(copied_metadata, metadata)\n\n        # Update the modified date every time a metadata element is updated/deleted, or when 'all' is specified\n        modified_date = metadata.dates.filter(type='modified').first()\n        if field_name != 'all':\n            # this is the case of updating cached metadata as part of metadata save/delete signal handler\n            copied_metadata['modified'] = now().isoformat()\n            if modified_date:\n                # this update won't trigger the post_save signal for Date model since we are using update query api\n                type(modified_date).objects.filter(id=modified_date.id).update(start_date=copied_metadata['modified'])\n        else:\n            # this is the case of updating cached metadata as part of management command\n            if modified_date:\n                copied_metadata['modified'] = modified_date.start_date.isoformat()\n            else:\n                copied_metadata['modified'] = self.updated.isoformat()\n\n        type(self).objects.filter(id=self.id).update(cached_metadata=copied_metadata)\n\n    def _update_creators_field(self, copied_metadata, metadata):\n        \"\"\"Update creators field in cached metadata\"\"\"\n        creators = metadata.creators.all()\n        copied_metadata['creators'] = [\n            {\n                'id': c.id,\n                'name': c.name if c.name else None,\n                'email': c.email if c.email else None,\n                'address': c.address if c.address else None,\n                'phone': c.phone if c.phone else None,\n                'homepage': c.homepage if c.homepage else None,\n                'order': c.order,\n                'hs_user_id': c.hydroshare_user_id,\n                'is_active_user': c.is_active_user,\n                'relative_uri': c.relative_uri,\n                'organization': c.organization if c.organization else None,\n                'identifiers': c.identifiers\n            }\n            for c in creators\n        ]\n\n    def _update_contributors_field(self, copied_metadata, metadata):\n        \"\"\"Update contributors field in cached metadata\"\"\"\n        contributors = metadata.contributors.all()\n        copied_metadata['contributors'] = [\n            {\n                'id': c.id,\n                'name': c.name if c.name else None,\n                'email': c.email if c.email else None,\n                'address': c.address if c.address else None,\n                'phone': c.phone if c.phone else None,\n                'homepage': c.homepage if c.homepage else None,\n                'hs_user_id': c.hydroshare_user_id,\n                'is_active_user': c.is_active_user,\n                'relative_uri': c.relative_uri,\n                'organization': c.organization if c.organization else None,\n                'identifiers': c.identifiers\n            }\n            for c in contributors\n        ]\n\n    def _update_title_field(self, copied_metadata, metadata):\n        \"\"\"Update title field in cached metadata\"\"\"\n        title = metadata.title.value if hasattr(metadata, 'title') and metadata.title else \"\"\n        if title:\n            copied_metadata['title'] = {'value': title, 'id': metadata.title.id}\n        else:\n            copied_metadata['title'] = {}\n\n    def _update_subjects_field(self, copied_metadata, metadata):\n        \"\"\"Update subjects field in cached metadata\"\"\"\n        subjects = list(metadata.subjects.all())\n        copied_metadata['subjects'] = [s.value for s in subjects]\n\n    def _update_date_field(self, copied_metadata, metadata):\n        \"\"\"Update date field in cached metadata\"\"\"\n        if 'created' not in copied_metadata:\n            created_date = metadata.dates.filter(type='created').first()\n            if created_date:\n                copied_metadata['created'] = created_date.start_date.isoformat()\n            else:\n                copied_metadata['created'] = self.created.isoformat()\n\n        if 'published_date' not in copied_metadata:\n            published_date = metadata.dates.filter(type='published').first()\n            if published_date:\n                copied_metadata['published_date'] = published_date.start_date.isoformat()\n\n    def _update_status_field(self, copied_metadata, metadata):\n        \"\"\"Update status field in cached metadata\"\"\"\n        if hasattr(self, 'raccess'):\n            copied_metadata['status'] = {\n                \"public\": self.raccess.public,\n                \"discoverable\": self.raccess.discoverable,\n                \"published\": self.raccess.published,\n                \"shareable\": self.raccess.shareable\n            }\n\n    def _update_abstract_field(self, copied_metadata, metadata):\n        \"\"\"Update abstract field in cached metadata\"\"\"\n        abstract = metadata.description.abstract if hasattr(metadata, 'description') and metadata.description else \"\"\n        if abstract:\n            copied_metadata['abstract'] = {'value': abstract, 'id': metadata.description.id}\n        else:\n            copied_metadata['abstract'] = {}\n\n    def _update_coverage_field(self, copied_metadata, metadata):\n        \"\"\"Update coverage field in cached metadata\"\"\"\n        self._update_temporal_coverage_field(copied_metadata, metadata)\n        self._update_spatial_coverage_field(copied_metadata, metadata)\n\n    def _update_temporal_coverage_field(self, copied_metadata, metadata):\n        \"\"\"Update temporal coverage field in cached metadata\"\"\"\n        temporal_coverage = metadata.temporal_coverage\n        temp_coverage = {}\n        if temporal_coverage:\n            temp_coverage = {\n                'id': temporal_coverage.id,\n                'start_date': temporal_coverage.value['start'],\n                'end_date': temporal_coverage.value['end'],\n                'name': temporal_coverage.value.get('name', '')\n            }\n        copied_metadata['temporal_coverage'] = temp_coverage\n\n    def _update_spatial_coverage_field(self, copied_metadata, metadata):\n        \"\"\"Update spatial coverage field in cached metadata\"\"\"\n        spatial_coverage = metadata.spatial_coverage\n        spatial_coverage_dict = {}\n        if spatial_coverage:\n            spatial_coverage_dict['id'] = spatial_coverage.id\n            spatial_coverage_dict['exists'] = True\n            spatial_coverage_dict['name'] = spatial_coverage.value.get('name', None)\n            spatial_coverage_dict['type'] = spatial_coverage.type\n            spatial_coverage_dict['projection'] = spatial_coverage.value.get('projection', None)\n            spatial_coverage_dict['units'] = spatial_coverage.value['units']\n            spatial_coverage_dict['zunits'] = spatial_coverage.value.get('zunits', None)\n            if spatial_coverage.type == 'point':\n                spatial_coverage_dict['east'] = spatial_coverage.value['east']\n                spatial_coverage_dict['north'] = spatial_coverage.value['north']\n                spatial_coverage_dict['elevation'] = spatial_coverage.value.get('elevation', None)\n            else:\n                spatial_coverage_dict['northlimit'] = spatial_coverage.value['northlimit']\n                spatial_coverage_dict['eastlimit'] = spatial_coverage.value['eastlimit']\n                spatial_coverage_dict['southlimit'] = spatial_coverage.value['southlimit']\n                spatial_coverage_dict['westlimit'] = spatial_coverage.value['westlimit']\n        else:\n            spatial_coverage_dict['exists'] = False\n            spatial_coverage_dict['default_units'] = metadata.spatial_coverage_default_units\n            spatial_coverage_dict['default_projection'] = metadata.spatial_coverage_default_projection\n        copied_metadata['spatial_coverage'] = spatial_coverage_dict\n\n    def _update_relation_field(self, copied_metadata, metadata):\n        \"\"\"Update relation field in cached metadata\"\"\"\n        relations = metadata.relations.all()\n        copied_metadata['relations'] = [\n            {\n                'id': r.id,\n                'type': r.type,\n                'value': r.value,\n                'type_description': r.type_description(),\n                'is_user_editable': r.type not in Relation.NOT_USER_EDITABLE\n            }\n            for r in relations\n        ]\n\n    def _update_geospatialrelation_field(self, copied_metadata, metadata):\n        \"\"\"Update geospatialrelation field in cached metadata\"\"\"\n        geospatialrelations = metadata.geospatialrelations.all()\n        copied_metadata['geospatial_relations'] = [\n            {\n                'id': r.id,\n                'type': r.type,\n                'value': r.value,\n                'text': r.text,\n                'type_description': r.type_description()\n            }\n            for r in geospatialrelations\n        ]\n\n    def _update_fundingagency_field(self, copied_metadata, metadata):\n        \"\"\"Update fundingagency field in cached metadata\"\"\"\n        funding_agencies = metadata.funding_agencies.all()\n        copied_metadata['funding_agencies'] = [\n            {\n                'id': r.id,\n                'agency_name': r.agency_name,\n                'award_title': r.award_title if r.award_title else None,\n                'award_number': r.award_number if r.award_number else None,\n                'agency_url': r.agency_url if r.agency_url else None\n            }\n            for r in funding_agencies\n        ]\n\n    def _update_rights_field(self, copied_metadata, metadata):\n        \"\"\"Update rights field in cached metadata\"\"\"\n        rights = metadata.rights\n        if rights:\n            copied_metadata['rights'] = {\n                'id': rights.id,\n                'statement': rights.statement,\n                'url': rights.url if rights.url else None\n            }\n\n    def _update_identifier_field(self, copied_metadata, metadata):\n        \"\"\"Update identifier field in cached metadata\"\"\"\n        identifiers = metadata.identifiers.all()\n        copied_metadata['identifiers'] = [\n            {\n                'id': i.id,\n                'name': i.name,\n                'url': i.url\n            }\n            for i in identifiers\n        ]\n\n    def _update_language_field(self, copied_metadata, metadata):\n        \"\"\"Update language field in cached metadata\"\"\"\n        language = metadata.language\n        copied_metadata['language'] = language.code if language else None\n\n    def _update_type_field(self, copied_metadata, metadata):\n        \"\"\"Update type field in cached metadata\"\"\"\n        _type = metadata.type\n        copied_metadata['type'] = _type.url if _type else None\n\n    def _update_publisher_field(self, copied_metadata, metadata):\n        \"\"\"Update publisher field in cached metadata\"\"\"\n        publisher = metadata.publisher\n        if publisher:\n            copied_metadata['publisher'] = {\n                'id': publisher.id,\n                'name': publisher.name,\n                'url': publisher.url\n            }\n        else:\n            copied_metadata['publisher'] = {}\n\n    def _ensure_required_fields(self, copied_metadata, metadata):\n        \"\"\"Ensure all required fields are present in cached metadata\"\"\"\n\n        if 'title' not in copied_metadata or not copied_metadata['title']:\n            self._update_title_field(copied_metadata, metadata)\n\n        if 'creators' not in copied_metadata or len(copied_metadata['creators']) == 0:\n            self._update_creators_field(copied_metadata, metadata)\n\n        if 'contributors' not in copied_metadata or len(copied_metadata['contributors']) == 0:\n            self._update_contributors_field(copied_metadata, metadata)\n\n        if 'created' not in copied_metadata or copied_metadata['created'] == '':\n            self._update_date_field(copied_metadata, metadata)\n\n        if 'subjects' not in copied_metadata or len(copied_metadata['subjects']) == 0:\n            self._update_subjects_field(copied_metadata, metadata)\n\n        if 'status' not in copied_metadata:\n            self._update_status_field(copied_metadata, metadata)\n\n        if 'abstract' not in copied_metadata or not copied_metadata['abstract']:\n            self._update_abstract_field(copied_metadata, metadata)\n\n        if 'temporal_coverage' not in copied_metadata:\n            self._update_temporal_coverage_field(copied_metadata, metadata)\n\n        if 'spatial_coverage' not in copied_metadata:\n            self._update_spatial_coverage_field(copied_metadata, metadata)\n\n        if 'relations' not in copied_metadata or len(copied_metadata['relations']) == 0:\n            self._update_relation_field(copied_metadata, metadata)\n\n        if 'geospatial_relations' not in copied_metadata or len(copied_metadata['geospatial_relations']) == 0:\n            self._update_geospatialrelation_field(copied_metadata, metadata)\n\n        if 'funding_agencies' not in copied_metadata or len(copied_metadata['funding_agencies']) == 0:\n            self._update_fundingagency_field(copied_metadata, metadata)\n\n        if 'rights' not in copied_metadata:\n            self._update_rights_field(copied_metadata, metadata)\n\n        if 'language' not in copied_metadata:\n            self._update_language_field(copied_metadata, metadata)\n\n        if 'identifiers' not in copied_metadata or len(copied_metadata['identifiers']) == 0:\n            self._update_identifier_field(copied_metadata, metadata)\n\n        if 'type' not in copied_metadata:\n            self._update_type_field(copied_metadata, metadata)\n\n        if 'publisher' not in copied_metadata or not copied_metadata['publisher']:\n            self._update_publisher_field(copied_metadata, metadata)\n\n    def update_all_cached_metadata(self):\n        \"\"\"\n        Update all fields in the cached metadata\n        This method is primarily for use in management commands to refresh cached metadata\n        \"\"\"\n        self.update_cached_metadata_field(field_name='all')\n\n    # definition of resource logic\n    @property\n    def supports_folders(self):\n        \"\"\"Return whether folder operations are supported. Computed for polymorphic types.\"\"\"\n        return False\n\n    @property\n    def last_updated(self):\n        \"\"\"Return the last updated date stored in metadata\"\"\"\n        # get the modified date from the cached metadata\n        modified_date = self.cached_metadata.get('modified', None)\n        if modified_date:\n            return parser.parse(modified_date)\n        else:\n            # get the modified date from the Date metadata element\n            for dt in self.metadata.dates.all():\n                if dt.type == 'modified':\n                    return dt.start_date\n\n    @property\n    def has_required_metadata(self):\n        \"\"\"Return True only if all required metadata is present.\"\"\"\n        if self.metadata is None or not self.metadata.has_all_required_elements():\n            return False\n        return True\n\n    @property\n    def can_be_public_or_discoverable(self):\n        \"\"\"Return True if the resource can be set to public or discoverable.\n\n        This is True if\n\n        1. The resource has all metadata elements marked as required.\n        2. The resource has all files that are considered required.\n\n        and False otherwise\n        \"\"\"\n        has_files = self.has_required_content_files()\n        if not has_files:\n            return False\n\n        has_metadata = self.has_required_metadata\n        return has_metadata\n\n    def set_discoverable(self, value, user=None):\n        \"\"\"Set the discoverable flag for a resource.\n\n        :param value: True or False\n        :param user: user requesting the change, or None for changes that are not user requests.\n        :raises ValidationError: if the current configuration cannot be set to desired state\n\n        This sets the discoverable flag (self.raccess.discoverable) for a resource based\n        upon application logic. It is part of AbstractResource because its result depends\n        upon resource state, and not just access control.\n\n        * This flag can only be set to True if the resource passes basic validations\n          `has_required_metata` and `has_required_content_files`\n        * setting `discoverable` to `False` also sets `public` to `False`\n        * setting `discoverable` to `True` does not change `public`\n\n        Thus, the setting public=True, discoverable=False is disallowed.\n\n        If `user` is None, access control is not checked.  This happens when a resource has been\n        invalidated outside of the control of a specific user. In this case, user can be None\n        \"\"\"\n        # access control is separate from validation logic\n        if user is not None and not user.uaccess.can_change_resource_flags(self):\n            raise ValidationError(\"You don't have permission to change resource sharing status\")\n\n        # check that there is sufficient resource content\n        has_metadata = self.has_required_metadata\n        has_files = self.has_required_content_files()\n        if value and not (has_metadata and has_files):\n\n            if not has_metadata and not has_files:\n                msg = \"Resource does not have sufficient metadata and content files to be \" + \\\n                    \"discoverable\"\n                raise ValidationError(msg)\n            elif not has_metadata:\n                msg = \"Resource does not have sufficient metadata to be discoverable\"\n                raise ValidationError(msg)\n            elif not has_files:\n                msg = \"Resource does not have sufficient content files to be discoverable\"\n                raise ValidationError(msg)\n\n        else:  # state change is allowed\n            self.raccess.discoverable = value\n            self.raccess.save()\n            self.set_public(False)\n            self.update_index()\n\n    def set_public(self, value, user=None):\n        \"\"\"Set the public flag for a resource.\n\n        :param value: True or False\n        :param user: user requesting the change, or None for changes that are not user requests.\n        :raises ValidationError: if the current configuration cannot be set to desired state\n\n        This sets the public flag (self.raccess.public) for a resource based\n        upon application logic. It is part of AbstractResource because its result depends\n        upon resource state, and not just access control.\n\n        * This flag can only be set to True if the resource passes basic validations\n          `has_required_metata` and `has_required_content_files`\n        * setting `public` to `True` also sets `discoverable` to `True`\n        * setting `public` to `False` does not change `discoverable`\n        * setting `public` to either also modifies the AVU isPublic for the resource.\n\n        Thus, the setting public=True, discoverable=False is disallowed.\n\n        If `user` is None, access control is not checked.  This happens when a resource has been\n        invalidated outside of the control of a specific user. In this case, user can be None\n        \"\"\"\n        # avoid import loop\n        from hs_core.signals import post_raccess_change\n        from hs_access_control.models.shortcut import zone_of_publicity\n\n        # access control is separate from validation logic\n        if user is not None and not user.uaccess.can_change_resource_flags(self):\n            raise ValidationError(\"You don't have permission to change resource sharing status\")\n\n        old_value = self.raccess.public  # is this a change?\n\n        # check that there is sufficient resource content\n        has_metadata = self.has_required_metadata\n        has_files = self.has_required_content_files()\n        if value and not (has_metadata and has_files):\n\n            if not has_metadata and not has_files:\n                msg = \"Resource does not have sufficient metadata and content files to be public\"\n                raise ValidationError(msg)\n\n            elif not has_metadata:\n                msg = \"Resource does not have sufficient metadata to be public\"\n                raise ValidationError(msg)\n\n            elif not has_files:\n                msg = \"Resource does not have sufficient content files to be public\"\n                raise ValidationError(msg)\n\n        else:  # make valid state change\n            self.raccess.public = value\n            if value:  # can't be public without being discoverable\n                self.raccess.discoverable = value\n            self.raccess.save()\n            post_raccess_change.send(sender=self, resource=self)\n            self.update_index()\n            zone_of_publicity(resource=self)\n            # public changed state: set isPublic metadata AVU accordingly\n            if value != old_value:\n                self.setAVU(\"isPublic\", self.raccess.public)\n\n    def set_published(self, value):\n        \"\"\"Set the published flag for a resource.\n\n        :param value: True or False\n\n        This sets the published flag (self.raccess.published)\n        \"\"\"\n        from hs_core.signals import post_raccess_change\n\n        self.raccess.published = value\n        self.raccess.immutable = value\n        if value:  # can't be published without being public\n            self.raccess.public = value\n        self.raccess.save()\n        post_raccess_change.send(sender=self, resource=self)\n        self.update_index()\n\n    def update_index(self):\n        \"\"\"updates previous versions of a resource (self) in index\"\"\"\n        prev_version_resource_relation_meta = Relation.objects.filter(type='isReplacedBy',\n                                                                      value__contains=self.short_id).first()\n        if prev_version_resource_relation_meta:\n            prev_version_res = prev_version_resource_relation_meta.metadata.resource\n            if prev_version_res.raccess.discoverable or prev_version_res.raccess.public:\n                # saving to trigger index update for this previous version of resource\n                prev_version_res.save()\n            prev_version_res.update_index()\n\n    def set_require_download_agreement(self, user, value):\n        \"\"\"Set resource require_download_agreement flag to True or False.\n        If require_download_agreement is True then user will be prompted to agree to resource\n        rights statement before he/she can download resource files or bag.\n\n        :param user: user requesting the change\n        :param value: True or False\n        :raises PermissionDenied: if the user lacks permission to change resource flag\n        \"\"\"\n        if not user.uaccess.can_change_resource_flags(self):\n            raise PermissionDenied(\"You don't have permission to change resource download agreement\"\n                                   \" status\")\n        self.raccess.require_download_agreement = value\n        self.raccess.save()\n\n    def set_private_sharing_link(self, user, value):\n        \"\"\"Set resource 'allow_private_sharing' flag to True or False.\n        If allow_private_sharing is True then any user including anonymous user will be able to use the resource url\n        to view the resource (view mode).\n\n        :param user: user requesting the change\n        :param value: True or False\n        :raises PermissionDenied: if the user lacks permission to change resource flag\n        \"\"\"\n        if not user.uaccess.can_change_resource_flags(self):\n            raise PermissionDenied(\"You don't have permission to change resource private link sharing \"\n                                   \" status\")\n        self.raccess.allow_private_sharing = value\n        self.raccess.save()\n\n    def update_public_and_discoverable(self):\n        \"\"\"Update the settings of the public and discoverable flags for changes in metadata.\"\"\"\n        if self.raccess.discoverable and not self.can_be_public_or_discoverable:\n            self.set_discoverable(False)  # also sets Public\n\n    @property\n    def absolute_url(self):\n        return self.get_url_of_path('')\n\n    def get_url_of_path(self, path):\n        \"\"\"Return the URL of an arbitrary path in this resource.\n\n        A GET of this URL simply returns the contents of the path.\n        This URL is independent of federation.\n        PUT, POST, and DELETE are not supported.\n        path includes data/contents/\n\n        This choice for a URL is dependent mainly upon conformance to DataOne URL standards\n        that are also conformant to the format in resourcemap.xml. This url does not contain\n        the site URL, which is prefixed when needed.\n\n        This is based upon the resourcemap_urls.py entry:\n\n            url(r'^resource/(?P&lt;shortkey&gt;[0-9a-f-]+)/data/contents/(?.+)/$',\n                views.file_download_url_mapper,\n                name='get_resource_file')\n\n        \"\"\"\n        # must start with a / in order to concat with current_site_url.\n        url_encoded_path = urllib.parse.quote(path)\n        return '/' + os.path.join('resource', self.short_id, url_encoded_path)\n\n    def get_s3_path(self, path, prepend_short_id=True):\n        \"\"\"Return the S3 path by which the given path is accessed.\n           The input path includes data/contents/ as needed.\n        \"\"\"\n        if prepend_short_id and not path.startswith(self.short_id):\n            full_path = os.path.join(self.short_id, path)\n        else:\n            full_path = path\n\n        return full_path\n\n    def set_quota_holder(self, setter, new_holder):\n        \"\"\"Set quota holder of the resource to new_holder who must be an owner.\n\n        setter is the requesting user to transfer quota holder and setter must also be an owner\n        \"\"\"\n        from hs_core.hydroshare.utils import validate_user_quota\n\n        if __debug__:\n            assert (isinstance(setter, User))\n            assert (isinstance(new_holder, User))\n        if not setter.uaccess.owns_resource(self) or \\\n                not new_holder.uaccess.owns_resource(self):\n            raise PermissionDenied(\"Only owners can set or be set as quota holder for the resource\")\n\n        # ensure the new holder has a bucket, buckets only exist for users with resources\n        istorage = self.get_s3_storage()\n        istorage.create_bucket(new_holder.userprofile.bucket_name)\n        # QuotaException will be raised if new_holder does not have enough quota to hold this\n        # new resource, in which case, set_quota_holder to the new user fails\n        validate_user_quota(new_holder, self.size)\n        # if the resource is new, it does not have a quota holder yet\n        if self.quota_holder:\n            self.get_s3_storage().new_quota_holder(self.short_id, new_holder.username)\n\n        self.quota_holder = new_holder\n        self.save()\n\n    def setAVU(self, attribute, value):\n        \"\"\"Set an AVU at the resource level.\n\n        This avoids mistakes in setting AVUs by assuring that the appropriate root path\n        is alway used.\n        \"\"\"\n        if isinstance(value, bool):\n            value = str(value).lower()  # normalize boolean values to strings\n        istorage = self.get_s3_storage()\n        root_path = self.root_path\n        istorage.setAVU(root_path, attribute, value)\n\n    def getAVU(self, attribute):\n        \"\"\"Get an AVU for a resource.\n\n        This avoids mistakes in getting AVUs by assuring that the appropriate root path\n        is alway used.\n        \"\"\"\n        istorage = self.get_s3_storage()\n        root_path = self.root_path\n        value = istorage.getAVU(root_path, attribute)\n\n        # Convert selected boolean attribute values to bool; non-existence implies False\n        # \"Private\" is the appropriate response if \"isPublic\" is None\n        if attribute == 'isPublic':\n            if value is not None and value.lower() == 'true':\n                return True\n            else:\n                return False\n\n        # Convert selected boolean attribute values to bool; non-existence implies True\n        # If bag_modified or metadata_dirty does not exist, then we do not know the\n        # state of metadata files and/or bags. They may not exist. Thus we interpret\n        # None as \"true\", which will generate the appropriate files if they do not exist.\n        if attribute == 'bag_modified' or attribute == 'metadata_dirty':\n            if value is None or value.lower() == 'true':\n                return True\n            else:\n                return False\n\n        # return strings for all other attributes\n        else:\n            return value\n\n    @classmethod\n    def scimeta_url(cls, resource_id):\n        \"\"\" Get URL of the science metadata file resourcemetadata.xml \"\"\"\n        res = BaseResource.objects.get(short_id=resource_id)\n        scimeta_path = res.scimeta_path\n        scimeta_url = reverse('rest_download', kwargs={'path': scimeta_path})\n        return scimeta_url\n\n    # TODO: there are too many ways to get to the resourcemap.\n    # 1. {id}/data/resourcemap.xml\n    # 2. {id}/resmap\n    # Choose one!\n    @classmethod\n    def resmap_url(cls, resource_id):\n        \"\"\" Get URL of the resource map resourcemap.xml.\"\"\"\n        resmap_path = \"{resource_id}/data/resourcemap.xml\".format(resource_id=resource_id)\n        resmap_url = reverse('rest_download', kwargs={'path': resmap_path})\n        return resmap_url\n\n    # TODO: this is inaccurate; resourcemap.xml != systemmetadata.xml\n    @classmethod\n    def sysmeta_path(cls, resource_id):\n        \"\"\"Get URL of resource map xml.\"\"\"\n        return \"{resource_id}/data/resourcemap.xml\".format(resource_id=resource_id)\n\n    def delete(self, using=None, keep_parents=False):\n        \"\"\"Delete resource along with all of its metadata and data bag.\"\"\"\n        from .hydroshare import hs_bagit\n        for fl in self.files.all():\n            # COUCH: delete of file objects now cascades.\n            fl.delete(delete_logical_file=True)\n        self.metadata.delete()\n        hs_bagit.delete_files_and_bag(self)\n        super(AbstractResource, self).delete()\n\n    @property\n    def metadata(self):\n        \"\"\"Return the metadata object for this resource.\"\"\"\n        return self.content_object\n\n    @classmethod\n    def get_metadata_class(cls):\n        return CoreMetaData\n\n    @property\n    def first_creator(self):\n        \"\"\"Get first creator of resource from metadata.\"\"\"\n        first_creator = self.metadata.creators.filter(order=1).first()\n        return first_creator\n\n    def get_metadata_xml(self, pretty_print=True, include_format_elements=True):\n        \"\"\"Get metadata xml for Resource.\n\n        Resource types that support file types\n        must override this method. See Composite Resource\n        type as an example\n        \"\"\"\n        return self.metadata.get_xml(pretty_print=pretty_print,\n                                     include_format_elements=include_format_elements)\n\n    @classmethod\n    def is_schema_json_file(cls, file_path):\n        \"\"\"Determine whether a given file is a schema.json file.\n        Note: this will return true for any file that ends with the schema.json ending\n        We are taking the risk that user might create a file with the same filename ending\n        \"\"\"\n        from django_s3.utils import is_schema_json_file as _is_schema_json_file\n\n        return _is_schema_json_file(file_path)\n\n    @classmethod\n    def is_schema_json_values_file(cls, file_path):\n        \"\"\"Determine whether a given file is a schema_values.json file.\n        Note: this will return true for any file that ends with the _schema_values.json ending\n        We are taking the risk that user might create a file with the same filename ending\n        \"\"\"\n        from django_s3.utils import is_schema_json_values_file as _is_schema_json_values_file\n\n        return _is_schema_json_values_file(file_path)\n\n    def is_collection_list_csv(self, file_path):\n        \"\"\"Determine if a given file is an internally-generated collection list\n        \"\"\"\n        from hs_collection_resource.utils import CSV_FULL_NAME_TEMPLATE\n        collection_list_filename = CSV_FULL_NAME_TEMPLATE.format(self.short_id)\n        if collection_list_filename in file_path:\n            return True\n        return False\n\n    @classmethod\n    def is_metadata_xml_file(cls, file_path):\n        \"\"\"Determine whether a given file is metadata.\n        Note: this will return true for any file that ends with the metadata endings\n        We are taking the risk that user might create a file with the same filename ending\n        \"\"\"\n        from django_s3.utils import is_metadata_xml_file as _is_metadata_xml_file\n\n        return _is_metadata_xml_file(file_path)\n\n    @classmethod\n    def is_metadata_json_file(cls, file_path):\n        \"\"\"Determine whether a given file is a metadata json file.\n        Note: this will return true for any file that ends with the metadata endings or\n        has the same name as the metadata json file\n        \"\"\"\n        from django_s3.utils import is_metadata_json_file as _is_metadata_json_file\n\n        return _is_metadata_json_file(file_path)\n\n    def is_aggregation_xml_file(self, file_path):\n        \"\"\"Checks if the file path *file_path* is one of the aggregation related xml file paths\n\n        :param  file_path: full file path starting with resource short_id\n        :return True if file_path is one of the aggregation xml file paths else False\n\n        This function is overridden for Composite Resource.\n        \"\"\"\n        return False\n\n    def extra_capabilites(self):\n        \"\"\"Return None. No-op method.\n\n        This is not terribly well defined yet, but should return at least a JSON serializable\n        object of URL endpoints where extra self-describing services exist and can be queried by\n        the user in the form of { \"name\" : \"endpoint\" }\n        \"\"\"\n        return None\n\n    def parse_citation_name(self, name, first_author=False):\n        \"\"\"Return properly formatted citation name from metadata.\"\"\"\n        CREATOR_NAME_ERROR = \"Failed to generate citation - invalid creator name.\"\n        first_names = None\n        if \",\" in name:\n            name_parts = name.split(\",\")\n            if len(name_parts) == 0:\n                return CREATOR_NAME_ERROR\n            elif len(name_parts) == 1:\n                last_names = name_parts[0]\n            elif len(name_parts) == 2:\n                first_names = name_parts[1]\n                first_names = first_names.split()\n                last_names = name_parts[0]\n            else:\n                return CREATOR_NAME_ERROR\n        else:\n            name_parts = name.split()\n            if len(name_parts) == 0:\n                return CREATOR_NAME_ERROR\n            elif len(name_parts) &gt; 1:\n                first_names = name_parts[:-1]\n                last_names = name_parts[-1]\n            else:\n                last_names = name_parts[0]\n\n        if first_names:\n            initials_list = [i[0] for i in first_names]\n            initials = \". \".join(initials_list) + \".\"\n            if first_author:\n                author_name = \"{last_name}, {initials}\"\n            else:\n                author_name = \"{initials} {last_name}\"\n            author_name = author_name.format(last_name=last_names,\n                                             initials=initials\n                                             )\n        else:\n            author_name = \"{last_name}\".format(last_name=last_names)\n\n        return author_name + \", \"\n\n    def get_custom_citation(self):\n        \"\"\"Get custom citation.\"\"\"\n        if self.metadata.citation.first() is None:\n            return ''\n        return str(self.metadata.citation.first())\n\n    def get_citation(self, forceHydroshareURI=True):\n        \"\"\"Get citation or citations from resource metadata using cached_metadata.\"\"\"\n\n        logger = logging.getLogger(__name__)\n        citation_str_lst = []\n        CITATION_ERROR = \"Failed to generate citation.\"\n\n        # Get creators from cached_metadata\n        self.refresh_from_db(fields=['cached_metadata'])\n        cached_creators = self.cached_metadata.get('creators', [])\n        if not cached_creators:\n            logger.error(f\"{CITATION_ERROR} No creators found in cached_metadata for resource {self.short_id}\")\n            return CITATION_ERROR\n\n        # Find first creator (order == 1)\n        first_creator = next((c for c in cached_creators if c.get('order') == 1), None)\n        if not first_creator:\n            logger.error(f\"{CITATION_ERROR} No first creator found in cached_metadata for resource {self.short_id}\")\n            return CITATION_ERROR\n\n        # Format first creator name\n        creator_name = first_creator.get('name', '')\n        creator_name = creator_name.strip() if creator_name else ''  # No Nonetype\n        if first_creator.get('organization', '') and not creator_name:\n            citation_str_lst.append(first_creator['organization'] + \", \")\n        else:\n            citation_str_lst.append(self.parse_citation_name(creator_name, first_author=True))\n\n        # Add other creators\n        other_creators = [c for c in cached_creators if c.get('order', 0) &gt; 1]\n        for author in other_creators:\n            author_name = author.get('name', '')\n            author_name = author_name.strip() if author_name else ''  # No Nonetype\n            if author.get('organization', '') and not author_name:\n                citation_str_lst.append(author['organization'] + \", \")\n            elif author_name and len(author_name) != 0:\n                citation_str_lst.append(self.parse_citation_name(author_name))\n\n        # Remove the last added comma and space\n        if len(citation_str_lst[-1]) &gt; 2:\n            citation_str_lst[-1] = citation_str_lst[-1][:-2]\n        else:\n            err_msg = f\"No valid creator names found in cached_metadata for resource {self.short_id}\"\n            logger.error(f\"{CITATION_ERROR} {err_msg}\")\n            return CITATION_ERROR\n\n        # Get citation date from cached_metadata\n        citation_date = None\n        if self.cached_metadata.get('published', ''):\n            citation_date = self.cached_metadata['published']\n        elif self.cached_metadata.get('modified'):\n            citation_date = self.cached_metadata['modified']\n\n        if not citation_date:\n            err_msg = f\"No published or modified date found in cached_metadata for resource {self.short_id}\"\n            logger.error(f\"{CITATION_ERROR} {err_msg}\")\n            return CITATION_ERROR\n\n        # Parse the date and extract year\n        try:\n            citation_date_obj = parser.parse(citation_date)\n            citation_year = citation_date_obj.year\n        except (ValueError, TypeError):\n            logger.error(f\"{CITATION_ERROR} Invalid date found in cached_metadata for resource {self.short_id}\")\n            return CITATION_ERROR\n\n        citation_str_lst.append(\" ({year}). \".format(year=citation_year))\n        title = self.cached_metadata.get('title', {})\n        if not title:\n            logger.error(f\"{CITATION_ERROR} No title found in cached_metadata for resource {self.short_id}\")\n            return CITATION_ERROR\n        if not title.get('value', ''):\n            logger.error(f\"{CITATION_ERROR} No title value found in cached_metadata for resource {self.short_id}\")\n            return CITATION_ERROR\n        citation_str_lst.append(title.get('value'))\n\n        # Check for DOI identifier\n        isPendingActivation = False\n        identifiers = self.cached_metadata.get('identifiers', [])\n        doi = [idn for idn in identifiers if idn.get('name') == \"doi\"]\n\n        if doi and not forceHydroshareURI:\n            hs_identifier = doi[0]\n            if (self.doi.find(DataciteSubmissionStatus.PENDING.value) &gt;= 0\n                    or self.doi.find(DataciteSubmissionStatus.FAILURE.value) &gt;= 0):\n                isPendingActivation = True\n        else:\n            hs_identifier = [idn for idn in identifiers if idn.get('name') == \"hydroShareIdentifier\"]\n            if hs_identifier:\n                hs_identifier = hs_identifier[0]\n            else:\n                err_msg = f\"No hydroShareIdentifier found in cached_metadata for resource {self.short_id}\"\n                logger.error(f\"{CITATION_ERROR} {err_msg}\")\n                return CITATION_ERROR\n\n        citation_str_lst.append(\", HydroShare, {url}\".format(url=hs_identifier.get('url', '')))\n\n        if isPendingActivation and not forceHydroshareURI:\n            citation_str_lst.append(\", DOI for this published resource is pending activation.\")\n\n        return ''.join(citation_str_lst)\n\n    @classmethod\n    def get_supported_upload_file_types(cls):\n        \"\"\"Get a list of permissible upload types.\n\n        Subclasses override this function to allow only specific file types.\n        Any version should return a tuple of those file extensions\n        (ex: return (\".csv\", \".txt\",))\n\n        To disallow all file upload, return an empty tuple ( return ())\n\n        By default all file types are supported\n\n        This is called before creating a specific instance; hence it is a class method.\n        \"\"\"\n        return (\".*\",)\n\n    @classmethod\n    def allow_multiple_file_upload(cls):\n        \"\"\"\n        Return whether multiple files can be uploaded.\n\n        Subclasses of BaseResource override this function to tailor file upload.\n        To allow multiple files to be uploaded return True, otherwise return False\n\n        Resource by default allows multiple file upload.\n        \"\"\"\n        return True\n\n    @classmethod\n    def can_have_multiple_files(cls):\n        \"\"\"Return whether this kind of resource can contain multiple files.\n\n        Subclasses of BaseResource override this function to tailor file upload.\n\n        To allow resource to have only 1 file or no file, return False\n\n        A resource by default can contain multiple files\n        \"\"\"\n        return True\n\n    def has_required_content_files(self):\n        \"\"\"Check whether a resource has the required content files.\n\n        Any subclass of this class may need to override this function\n        to apply specific requirements as it relates to resource content files\n        \"\"\"\n        if len(self.get_supported_upload_file_types()) &gt; 0:\n            if self.files.all().count() &gt; 0:\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_contained_file_extensions(self):\n        # return a set of all file extensions for all files in the resource\n        return set([f.extension for f in self.files.all()])\n\n    @property\n    def readme_file(self):\n        \"\"\"Returns a resource file that is at the root with a file name of either\n        'readme.txt' or 'readme.md' (filename is case insensitive). If no such file then None\n        is returned. If both files exist then resource file for readme.md is returned\"\"\"\n\n        if self.files.filter(file_folder='').count() == 0:\n            # no files exist at the root of the resource path - no need to check for readme file\n            return None\n\n        file_path_md = os.path.join(self.file_path, 'readme.md')\n        file_path_txt = os.path.join(self.file_path, 'readme.txt')\n        readme_res_file = self.files.filter(resource_file__iexact=file_path_md).first()\n        if readme_res_file is None:\n            readme_res_file = self.files.filter(resource_file__iexact=file_path_txt).first()\n\n        return readme_res_file\n\n    def get_readme_file_content(self):\n        \"\"\"Gets the content of the readme file. If both a readme.md and a readme.txt file exist,\n        then the content of the readme.md file is returned, otherwise None\n\n        Note: The user uploaded readme file if originally not encoded as utf-8, then any non-ascii\n        characters in the file will be escaped when we return the file content.\n        \"\"\"\n        readme_file = self.readme_file\n        # check the file exists on S3\n\n        if readme_file is None:\n            return readme_file\n\n        # check the file exists on S3\n        if readme_file.exists:\n            readme_file_content = readme_file.read().decode('utf-8', 'ignore')\n            if readme_file.extension.lower() == '.md':\n                markdown_file_content = markdown(readme_file_content, extensions=['tables'])\n                return {'content': markdown_file_content,\n                        'file_name': readme_file.file_name, 'file_type': 'md'}\n            else:\n                return {'content': readme_file_content, 'file_name': readme_file.file_name}\n        else:\n            file_name = readme_file.file_name\n            readme_file.delete()\n            logger = logging.getLogger(__name__)\n            log_msg = f\"readme file ({file_name}) is missing on S3. Deleting the file from Django.\"\n            logger.warning(log_msg)\n            return None\n\n    @property\n    def logical_files(self):\n        \"\"\"Gets a generator to access each of the logical files of the resource.\n        Note: Any derived class that supports logical file must override this function\n        \"\"\"\n\n        # empty generator\n        yield from ()\n\n    @property\n    def aggregation_types(self):\n        \"\"\"Gets a list of all aggregation types that currently exist in this resource\n        Note: Any derived class that supports logical file must override this function\n        \"\"\"\n        return []\n\n    @property\n    def aggregation_type_names(self):\n        \"\"\"Gets a list of all aggregation type names that currently exist in this resource\n        Note: Any derived class that supports logical file must override this function\n        \"\"\"\n        return []\n\n    def get_logical_files(self, logical_file_class_name):\n        \"\"\"Get a list of logical files (aggregations) for a specified logical file class name.\n        Note: Any derived class that supports logical file must override this function\n        \"\"\"\n        return []\n\n    @property\n    def has_logical_spatial_coverage(self):\n        \"\"\"Checks if any of the logical files has spatial coverage\n        Note: Any derived class that supports logical file must override this function\n        \"\"\"\n\n        return False\n\n    @property\n    def has_logical_temporal_coverage(self):\n        \"\"\"Checks if any of the logical files has temporal coverage\n        Note: Any derived class that supports logical file must override this function\n        \"\"\"\n\n        return False\n\n    @property\n    def supports_logical_file(self):\n        \"\"\"Check if resource allows associating resource file objects with logical file.\"\"\"\n        return False\n\n    def supports_folder_creation(self, folder_full_path):\n        \"\"\"Check if resource supports creation of folder at the specified path.\"\"\"\n        return True\n\n    def supports_rename_path(self, src_full_path, tgt_full_path):\n        \"\"\"Check if file/folder rename/move is allowed by this resource.\"\"\"\n        return True\n\n    def supports_zip(self, folder_to_zip):\n        \"\"\"Check if resource supports the specified folder to be zipped.\"\"\"\n        return True\n\n    def supports_unzip(self, file_to_unzip):\n        \"\"\"Check if resource supports the unzipping of the specified file.\"\"\"\n        return True\n\n    def supports_delete_folder_on_zip(self, original_folder):\n        \"\"\"Check if resource allows the original folder to be deleted upon zip.\"\"\"\n        return True\n\n    @property\n    def storage_type(self):\n        return 'local'\n\n    def is_folder(self, folder_path):\n        \"\"\"Determine whether a given path (relative to resource root, including /data/contents/)\n           is a folder or not. Returns False if the path does not exist.\n        \"\"\"\n        path_split = folder_path.split('/')\n        while path_split[-1] == '':\n            path_split.pop()\n        dir_path = '/'.join(path_split[0:-1])\n\n        # handles federation\n        s3_path = os.path.join(self.root_path, dir_path)\n        istorage = self.get_s3_storage()\n        try:\n            listing = istorage.listdir(s3_path)\n        except SessionException:\n            return False\n        if path_split[-1] in listing[0]:  # folders\n            return True\n        else:\n            return False\n\n    class Meta:\n        \"\"\"Define meta properties for AbstractResource class.\"\"\"\n\n        abstract = True\n        unique_together = (\"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.aggregation_type_names","title":"<code>aggregation_type_names</code>  <code>property</code>","text":"<p>Gets a list of all aggregation type names that currently exist in this resource Note: Any derived class that supports logical file must override this function</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.aggregation_types","title":"<code>aggregation_types</code>  <code>property</code>","text":"<p>Gets a list of all aggregation types that currently exist in this resource Note: Any derived class that supports logical file must override this function</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.can_be_public_or_discoverable","title":"<code>can_be_public_or_discoverable</code>  <code>property</code>","text":"<p>Return True if the resource can be set to public or discoverable.</p> <p>This is True if</p> <ol> <li>The resource has all metadata elements marked as required.</li> <li>The resource has all files that are considered required.</li> </ol> <p>and False otherwise</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.first_creator","title":"<code>first_creator</code>  <code>property</code>","text":"<p>Get first creator of resource from metadata.</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.has_logical_spatial_coverage","title":"<code>has_logical_spatial_coverage</code>  <code>property</code>","text":"<p>Checks if any of the logical files has spatial coverage Note: Any derived class that supports logical file must override this function</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.has_logical_temporal_coverage","title":"<code>has_logical_temporal_coverage</code>  <code>property</code>","text":"<p>Checks if any of the logical files has temporal coverage Note: Any derived class that supports logical file must override this function</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.has_required_metadata","title":"<code>has_required_metadata</code>  <code>property</code>","text":"<p>Return True only if all required metadata is present.</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.last_updated","title":"<code>last_updated</code>  <code>property</code>","text":"<p>Return the last updated date stored in metadata</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.logical_files","title":"<code>logical_files</code>  <code>property</code>","text":"<p>Gets a generator to access each of the logical files of the resource. Note: Any derived class that supports logical file must override this function</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Return the metadata object for this resource.</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.readme_file","title":"<code>readme_file</code>  <code>property</code>","text":"<p>Returns a resource file that is at the root with a file name of either 'readme.txt' or 'readme.md' (filename is case insensitive). If no such file then None is returned. If both files exist then resource file for readme.md is returned</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.supports_folders","title":"<code>supports_folders</code>  <code>property</code>","text":"<p>Return whether folder operations are supported. Computed for polymorphic types.</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.supports_logical_file","title":"<code>supports_logical_file</code>  <code>property</code>","text":"<p>Check if resource allows associating resource file objects with logical file.</p>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for AbstractResource class.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for AbstractResource class.\"\"\"\n\n    abstract = True\n    unique_together = (\"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.allow_multiple_file_upload","title":"<code>allow_multiple_file_upload()</code>  <code>classmethod</code>","text":"<p>Return whether multiple files can be uploaded.</p> <p>Subclasses of BaseResource override this function to tailor file upload. To allow multiple files to be uploaded return True, otherwise return False</p> <p>Resource by default allows multiple file upload.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef allow_multiple_file_upload(cls):\n    \"\"\"\n    Return whether multiple files can be uploaded.\n\n    Subclasses of BaseResource override this function to tailor file upload.\n    To allow multiple files to be uploaded return True, otherwise return False\n\n    Resource by default allows multiple file upload.\n    \"\"\"\n    return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.can_have_multiple_files","title":"<code>can_have_multiple_files()</code>  <code>classmethod</code>","text":"<p>Return whether this kind of resource can contain multiple files.</p> <p>Subclasses of BaseResource override this function to tailor file upload.</p> <p>To allow resource to have only 1 file or no file, return False</p> <p>A resource by default can contain multiple files</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef can_have_multiple_files(cls):\n    \"\"\"Return whether this kind of resource can contain multiple files.\n\n    Subclasses of BaseResource override this function to tailor file upload.\n\n    To allow resource to have only 1 file or no file, return False\n\n    A resource by default can contain multiple files\n    \"\"\"\n    return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.delete","title":"<code>delete(using=None, keep_parents=False)</code>","text":"<p>Delete resource along with all of its metadata and data bag.</p> Source code in <code>hs_core/models.py</code> <pre><code>def delete(self, using=None, keep_parents=False):\n    \"\"\"Delete resource along with all of its metadata and data bag.\"\"\"\n    from .hydroshare import hs_bagit\n    for fl in self.files.all():\n        # COUCH: delete of file objects now cascades.\n        fl.delete(delete_logical_file=True)\n    self.metadata.delete()\n    hs_bagit.delete_files_and_bag(self)\n    super(AbstractResource, self).delete()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.extra_capabilites","title":"<code>extra_capabilites()</code>","text":"<p>Return None. No-op method.</p> <p>This is not terribly well defined yet, but should return at least a JSON serializable object of URL endpoints where extra self-describing services exist and can be queried by the user in the form of { \"name\" : \"endpoint\" }</p> Source code in <code>hs_core/models.py</code> <pre><code>def extra_capabilites(self):\n    \"\"\"Return None. No-op method.\n\n    This is not terribly well defined yet, but should return at least a JSON serializable\n    object of URL endpoints where extra self-describing services exist and can be queried by\n    the user in the form of { \"name\" : \"endpoint\" }\n    \"\"\"\n    return None\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.getAVU","title":"<code>getAVU(attribute)</code>","text":"<p>Get an AVU for a resource.</p> <p>This avoids mistakes in getting AVUs by assuring that the appropriate root path is alway used.</p> Source code in <code>hs_core/models.py</code> <pre><code>def getAVU(self, attribute):\n    \"\"\"Get an AVU for a resource.\n\n    This avoids mistakes in getting AVUs by assuring that the appropriate root path\n    is alway used.\n    \"\"\"\n    istorage = self.get_s3_storage()\n    root_path = self.root_path\n    value = istorage.getAVU(root_path, attribute)\n\n    # Convert selected boolean attribute values to bool; non-existence implies False\n    # \"Private\" is the appropriate response if \"isPublic\" is None\n    if attribute == 'isPublic':\n        if value is not None and value.lower() == 'true':\n            return True\n        else:\n            return False\n\n    # Convert selected boolean attribute values to bool; non-existence implies True\n    # If bag_modified or metadata_dirty does not exist, then we do not know the\n    # state of metadata files and/or bags. They may not exist. Thus we interpret\n    # None as \"true\", which will generate the appropriate files if they do not exist.\n    if attribute == 'bag_modified' or attribute == 'metadata_dirty':\n        if value is None or value.lower() == 'true':\n            return True\n        else:\n            return False\n\n    # return strings for all other attributes\n    else:\n        return value\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.get_citation","title":"<code>get_citation(forceHydroshareURI=True)</code>","text":"<p>Get citation or citations from resource metadata using cached_metadata.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_citation(self, forceHydroshareURI=True):\n    \"\"\"Get citation or citations from resource metadata using cached_metadata.\"\"\"\n\n    logger = logging.getLogger(__name__)\n    citation_str_lst = []\n    CITATION_ERROR = \"Failed to generate citation.\"\n\n    # Get creators from cached_metadata\n    self.refresh_from_db(fields=['cached_metadata'])\n    cached_creators = self.cached_metadata.get('creators', [])\n    if not cached_creators:\n        logger.error(f\"{CITATION_ERROR} No creators found in cached_metadata for resource {self.short_id}\")\n        return CITATION_ERROR\n\n    # Find first creator (order == 1)\n    first_creator = next((c for c in cached_creators if c.get('order') == 1), None)\n    if not first_creator:\n        logger.error(f\"{CITATION_ERROR} No first creator found in cached_metadata for resource {self.short_id}\")\n        return CITATION_ERROR\n\n    # Format first creator name\n    creator_name = first_creator.get('name', '')\n    creator_name = creator_name.strip() if creator_name else ''  # No Nonetype\n    if first_creator.get('organization', '') and not creator_name:\n        citation_str_lst.append(first_creator['organization'] + \", \")\n    else:\n        citation_str_lst.append(self.parse_citation_name(creator_name, first_author=True))\n\n    # Add other creators\n    other_creators = [c for c in cached_creators if c.get('order', 0) &gt; 1]\n    for author in other_creators:\n        author_name = author.get('name', '')\n        author_name = author_name.strip() if author_name else ''  # No Nonetype\n        if author.get('organization', '') and not author_name:\n            citation_str_lst.append(author['organization'] + \", \")\n        elif author_name and len(author_name) != 0:\n            citation_str_lst.append(self.parse_citation_name(author_name))\n\n    # Remove the last added comma and space\n    if len(citation_str_lst[-1]) &gt; 2:\n        citation_str_lst[-1] = citation_str_lst[-1][:-2]\n    else:\n        err_msg = f\"No valid creator names found in cached_metadata for resource {self.short_id}\"\n        logger.error(f\"{CITATION_ERROR} {err_msg}\")\n        return CITATION_ERROR\n\n    # Get citation date from cached_metadata\n    citation_date = None\n    if self.cached_metadata.get('published', ''):\n        citation_date = self.cached_metadata['published']\n    elif self.cached_metadata.get('modified'):\n        citation_date = self.cached_metadata['modified']\n\n    if not citation_date:\n        err_msg = f\"No published or modified date found in cached_metadata for resource {self.short_id}\"\n        logger.error(f\"{CITATION_ERROR} {err_msg}\")\n        return CITATION_ERROR\n\n    # Parse the date and extract year\n    try:\n        citation_date_obj = parser.parse(citation_date)\n        citation_year = citation_date_obj.year\n    except (ValueError, TypeError):\n        logger.error(f\"{CITATION_ERROR} Invalid date found in cached_metadata for resource {self.short_id}\")\n        return CITATION_ERROR\n\n    citation_str_lst.append(\" ({year}). \".format(year=citation_year))\n    title = self.cached_metadata.get('title', {})\n    if not title:\n        logger.error(f\"{CITATION_ERROR} No title found in cached_metadata for resource {self.short_id}\")\n        return CITATION_ERROR\n    if not title.get('value', ''):\n        logger.error(f\"{CITATION_ERROR} No title value found in cached_metadata for resource {self.short_id}\")\n        return CITATION_ERROR\n    citation_str_lst.append(title.get('value'))\n\n    # Check for DOI identifier\n    isPendingActivation = False\n    identifiers = self.cached_metadata.get('identifiers', [])\n    doi = [idn for idn in identifiers if idn.get('name') == \"doi\"]\n\n    if doi and not forceHydroshareURI:\n        hs_identifier = doi[0]\n        if (self.doi.find(DataciteSubmissionStatus.PENDING.value) &gt;= 0\n                or self.doi.find(DataciteSubmissionStatus.FAILURE.value) &gt;= 0):\n            isPendingActivation = True\n    else:\n        hs_identifier = [idn for idn in identifiers if idn.get('name') == \"hydroShareIdentifier\"]\n        if hs_identifier:\n            hs_identifier = hs_identifier[0]\n        else:\n            err_msg = f\"No hydroShareIdentifier found in cached_metadata for resource {self.short_id}\"\n            logger.error(f\"{CITATION_ERROR} {err_msg}\")\n            return CITATION_ERROR\n\n    citation_str_lst.append(\", HydroShare, {url}\".format(url=hs_identifier.get('url', '')))\n\n    if isPendingActivation and not forceHydroshareURI:\n        citation_str_lst.append(\", DOI for this published resource is pending activation.\")\n\n    return ''.join(citation_str_lst)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.get_custom_citation","title":"<code>get_custom_citation()</code>","text":"<p>Get custom citation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_custom_citation(self):\n    \"\"\"Get custom citation.\"\"\"\n    if self.metadata.citation.first() is None:\n        return ''\n    return str(self.metadata.citation.first())\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.get_logical_files","title":"<code>get_logical_files(logical_file_class_name)</code>","text":"<p>Get a list of logical files (aggregations) for a specified logical file class name. Note: Any derived class that supports logical file must override this function</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_logical_files(self, logical_file_class_name):\n    \"\"\"Get a list of logical files (aggregations) for a specified logical file class name.\n    Note: Any derived class that supports logical file must override this function\n    \"\"\"\n    return []\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.get_metadata_xml","title":"<code>get_metadata_xml(pretty_print=True, include_format_elements=True)</code>","text":"<p>Get metadata xml for Resource.</p> <p>Resource types that support file types must override this method. See Composite Resource type as an example</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_metadata_xml(self, pretty_print=True, include_format_elements=True):\n    \"\"\"Get metadata xml for Resource.\n\n    Resource types that support file types\n    must override this method. See Composite Resource\n    type as an example\n    \"\"\"\n    return self.metadata.get_xml(pretty_print=pretty_print,\n                                 include_format_elements=include_format_elements)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.get_readme_file_content","title":"<code>get_readme_file_content()</code>","text":"<p>Gets the content of the readme file. If both a readme.md and a readme.txt file exist, then the content of the readme.md file is returned, otherwise None</p> <p>Note: The user uploaded readme file if originally not encoded as utf-8, then any non-ascii characters in the file will be escaped when we return the file content.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_readme_file_content(self):\n    \"\"\"Gets the content of the readme file. If both a readme.md and a readme.txt file exist,\n    then the content of the readme.md file is returned, otherwise None\n\n    Note: The user uploaded readme file if originally not encoded as utf-8, then any non-ascii\n    characters in the file will be escaped when we return the file content.\n    \"\"\"\n    readme_file = self.readme_file\n    # check the file exists on S3\n\n    if readme_file is None:\n        return readme_file\n\n    # check the file exists on S3\n    if readme_file.exists:\n        readme_file_content = readme_file.read().decode('utf-8', 'ignore')\n        if readme_file.extension.lower() == '.md':\n            markdown_file_content = markdown(readme_file_content, extensions=['tables'])\n            return {'content': markdown_file_content,\n                    'file_name': readme_file.file_name, 'file_type': 'md'}\n        else:\n            return {'content': readme_file_content, 'file_name': readme_file.file_name}\n    else:\n        file_name = readme_file.file_name\n        readme_file.delete()\n        logger = logging.getLogger(__name__)\n        log_msg = f\"readme file ({file_name}) is missing on S3. Deleting the file from Django.\"\n        logger.warning(log_msg)\n        return None\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.get_s3_path","title":"<code>get_s3_path(path, prepend_short_id=True)</code>","text":"<p>Return the S3 path by which the given path is accessed. The input path includes data/contents/ as needed.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_s3_path(self, path, prepend_short_id=True):\n    \"\"\"Return the S3 path by which the given path is accessed.\n       The input path includes data/contents/ as needed.\n    \"\"\"\n    if prepend_short_id and not path.startswith(self.short_id):\n        full_path = os.path.join(self.short_id, path)\n    else:\n        full_path = path\n\n    return full_path\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.get_supported_upload_file_types","title":"<code>get_supported_upload_file_types()</code>  <code>classmethod</code>","text":"<p>Get a list of permissible upload types.</p> <p>Subclasses override this function to allow only specific file types. Any version should return a tuple of those file extensions (ex: return (\".csv\", \".txt\",))</p> <p>To disallow all file upload, return an empty tuple ( return ())</p> <p>By default all file types are supported</p> <p>This is called before creating a specific instance; hence it is a class method.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef get_supported_upload_file_types(cls):\n    \"\"\"Get a list of permissible upload types.\n\n    Subclasses override this function to allow only specific file types.\n    Any version should return a tuple of those file extensions\n    (ex: return (\".csv\", \".txt\",))\n\n    To disallow all file upload, return an empty tuple ( return ())\n\n    By default all file types are supported\n\n    This is called before creating a specific instance; hence it is a class method.\n    \"\"\"\n    return (\".*\",)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.get_url_of_path","title":"<code>get_url_of_path(path)</code>","text":"<p>Return the URL of an arbitrary path in this resource.</p> <p>A GET of this URL simply returns the contents of the path. This URL is independent of federation. PUT, POST, and DELETE are not supported. path includes data/contents/</p> <p>This choice for a URL is dependent mainly upon conformance to DataOne URL standards that are also conformant to the format in resourcemap.xml. This url does not contain the site URL, which is prefixed when needed.</p> <p>This is based upon the resourcemap_urls.py entry:</p> <pre><code>url(r'^resource/(?P&lt;shortkey&gt;[0-9a-f-]+)/data/contents/(?.+)/$',\n    views.file_download_url_mapper,\n    name='get_resource_file')\n</code></pre> Source code in <code>hs_core/models.py</code> <pre><code>def get_url_of_path(self, path):\n    \"\"\"Return the URL of an arbitrary path in this resource.\n\n    A GET of this URL simply returns the contents of the path.\n    This URL is independent of federation.\n    PUT, POST, and DELETE are not supported.\n    path includes data/contents/\n\n    This choice for a URL is dependent mainly upon conformance to DataOne URL standards\n    that are also conformant to the format in resourcemap.xml. This url does not contain\n    the site URL, which is prefixed when needed.\n\n    This is based upon the resourcemap_urls.py entry:\n\n        url(r'^resource/(?P&lt;shortkey&gt;[0-9a-f-]+)/data/contents/(?.+)/$',\n            views.file_download_url_mapper,\n            name='get_resource_file')\n\n    \"\"\"\n    # must start with a / in order to concat with current_site_url.\n    url_encoded_path = urllib.parse.quote(path)\n    return '/' + os.path.join('resource', self.short_id, url_encoded_path)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.has_required_content_files","title":"<code>has_required_content_files()</code>","text":"<p>Check whether a resource has the required content files.</p> <p>Any subclass of this class may need to override this function to apply specific requirements as it relates to resource content files</p> Source code in <code>hs_core/models.py</code> <pre><code>def has_required_content_files(self):\n    \"\"\"Check whether a resource has the required content files.\n\n    Any subclass of this class may need to override this function\n    to apply specific requirements as it relates to resource content files\n    \"\"\"\n    if len(self.get_supported_upload_file_types()) &gt; 0:\n        if self.files.all().count() &gt; 0:\n            return True\n        else:\n            return False\n    else:\n        return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.is_aggregation_xml_file","title":"<code>is_aggregation_xml_file(file_path)</code>","text":"<p>Checks if the file path file_path is one of the aggregation related xml file paths</p> <p>:param  file_path: full file path starting with resource short_id :return True if file_path is one of the aggregation xml file paths else False</p> <p>This function is overridden for Composite Resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>def is_aggregation_xml_file(self, file_path):\n    \"\"\"Checks if the file path *file_path* is one of the aggregation related xml file paths\n\n    :param  file_path: full file path starting with resource short_id\n    :return True if file_path is one of the aggregation xml file paths else False\n\n    This function is overridden for Composite Resource.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.is_collection_list_csv","title":"<code>is_collection_list_csv(file_path)</code>","text":"<p>Determine if a given file is an internally-generated collection list</p> Source code in <code>hs_core/models.py</code> <pre><code>def is_collection_list_csv(self, file_path):\n    \"\"\"Determine if a given file is an internally-generated collection list\n    \"\"\"\n    from hs_collection_resource.utils import CSV_FULL_NAME_TEMPLATE\n    collection_list_filename = CSV_FULL_NAME_TEMPLATE.format(self.short_id)\n    if collection_list_filename in file_path:\n        return True\n    return False\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.is_folder","title":"<code>is_folder(folder_path)</code>","text":"<p>Determine whether a given path (relative to resource root, including /data/contents/) is a folder or not. Returns False if the path does not exist.</p> Source code in <code>hs_core/models.py</code> <pre><code>def is_folder(self, folder_path):\n    \"\"\"Determine whether a given path (relative to resource root, including /data/contents/)\n       is a folder or not. Returns False if the path does not exist.\n    \"\"\"\n    path_split = folder_path.split('/')\n    while path_split[-1] == '':\n        path_split.pop()\n    dir_path = '/'.join(path_split[0:-1])\n\n    # handles federation\n    s3_path = os.path.join(self.root_path, dir_path)\n    istorage = self.get_s3_storage()\n    try:\n        listing = istorage.listdir(s3_path)\n    except SessionException:\n        return False\n    if path_split[-1] in listing[0]:  # folders\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.is_metadata_json_file","title":"<code>is_metadata_json_file(file_path)</code>  <code>classmethod</code>","text":"<p>Determine whether a given file is a metadata json file. Note: this will return true for any file that ends with the metadata endings or has the same name as the metadata json file</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef is_metadata_json_file(cls, file_path):\n    \"\"\"Determine whether a given file is a metadata json file.\n    Note: this will return true for any file that ends with the metadata endings or\n    has the same name as the metadata json file\n    \"\"\"\n    from django_s3.utils import is_metadata_json_file as _is_metadata_json_file\n\n    return _is_metadata_json_file(file_path)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.is_metadata_xml_file","title":"<code>is_metadata_xml_file(file_path)</code>  <code>classmethod</code>","text":"<p>Determine whether a given file is metadata. Note: this will return true for any file that ends with the metadata endings We are taking the risk that user might create a file with the same filename ending</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef is_metadata_xml_file(cls, file_path):\n    \"\"\"Determine whether a given file is metadata.\n    Note: this will return true for any file that ends with the metadata endings\n    We are taking the risk that user might create a file with the same filename ending\n    \"\"\"\n    from django_s3.utils import is_metadata_xml_file as _is_metadata_xml_file\n\n    return _is_metadata_xml_file(file_path)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.is_schema_json_file","title":"<code>is_schema_json_file(file_path)</code>  <code>classmethod</code>","text":"<p>Determine whether a given file is a schema.json file. Note: this will return true for any file that ends with the schema.json ending We are taking the risk that user might create a file with the same filename ending</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef is_schema_json_file(cls, file_path):\n    \"\"\"Determine whether a given file is a schema.json file.\n    Note: this will return true for any file that ends with the schema.json ending\n    We are taking the risk that user might create a file with the same filename ending\n    \"\"\"\n    from django_s3.utils import is_schema_json_file as _is_schema_json_file\n\n    return _is_schema_json_file(file_path)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.is_schema_json_values_file","title":"<code>is_schema_json_values_file(file_path)</code>  <code>classmethod</code>","text":"<p>Determine whether a given file is a schema_values.json file. Note: this will return true for any file that ends with the _schema_values.json ending We are taking the risk that user might create a file with the same filename ending</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef is_schema_json_values_file(cls, file_path):\n    \"\"\"Determine whether a given file is a schema_values.json file.\n    Note: this will return true for any file that ends with the _schema_values.json ending\n    We are taking the risk that user might create a file with the same filename ending\n    \"\"\"\n    from django_s3.utils import is_schema_json_values_file as _is_schema_json_values_file\n\n    return _is_schema_json_values_file(file_path)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.parse_citation_name","title":"<code>parse_citation_name(name, first_author=False)</code>","text":"<p>Return properly formatted citation name from metadata.</p> Source code in <code>hs_core/models.py</code> <pre><code>def parse_citation_name(self, name, first_author=False):\n    \"\"\"Return properly formatted citation name from metadata.\"\"\"\n    CREATOR_NAME_ERROR = \"Failed to generate citation - invalid creator name.\"\n    first_names = None\n    if \",\" in name:\n        name_parts = name.split(\",\")\n        if len(name_parts) == 0:\n            return CREATOR_NAME_ERROR\n        elif len(name_parts) == 1:\n            last_names = name_parts[0]\n        elif len(name_parts) == 2:\n            first_names = name_parts[1]\n            first_names = first_names.split()\n            last_names = name_parts[0]\n        else:\n            return CREATOR_NAME_ERROR\n    else:\n        name_parts = name.split()\n        if len(name_parts) == 0:\n            return CREATOR_NAME_ERROR\n        elif len(name_parts) &gt; 1:\n            first_names = name_parts[:-1]\n            last_names = name_parts[-1]\n        else:\n            last_names = name_parts[0]\n\n    if first_names:\n        initials_list = [i[0] for i in first_names]\n        initials = \". \".join(initials_list) + \".\"\n        if first_author:\n            author_name = \"{last_name}, {initials}\"\n        else:\n            author_name = \"{initials} {last_name}\"\n        author_name = author_name.format(last_name=last_names,\n                                         initials=initials\n                                         )\n    else:\n        author_name = \"{last_name}\".format(last_name=last_names)\n\n    return author_name + \", \"\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.resmap_url","title":"<code>resmap_url(resource_id)</code>  <code>classmethod</code>","text":"<p>Get URL of the resource map resourcemap.xml.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef resmap_url(cls, resource_id):\n    \"\"\" Get URL of the resource map resourcemap.xml.\"\"\"\n    resmap_path = \"{resource_id}/data/resourcemap.xml\".format(resource_id=resource_id)\n    resmap_url = reverse('rest_download', kwargs={'path': resmap_path})\n    return resmap_url\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.save","title":"<code>save(*args, **kwargs)</code>","text":"<p>Refresh cached metadata before resource is saved to prevent stale cached metadata in memory getting written to DB</p> Source code in <code>hs_core/models.py</code> <pre><code>def save(self, *args, **kwargs):\n    \"\"\"Refresh cached metadata before resource is saved to prevent stale cached metadata in memory getting\n    written to DB\"\"\"\n    # Only refresh cached_metadata from DB if object already exists\n    if not self._state.adding:\n        self.refresh_from_db(fields=['cached_metadata'])\n    super(AbstractResource, self).save(*args, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.scimeta_url","title":"<code>scimeta_url(resource_id)</code>  <code>classmethod</code>","text":"<p>Get URL of the science metadata file resourcemetadata.xml</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef scimeta_url(cls, resource_id):\n    \"\"\" Get URL of the science metadata file resourcemetadata.xml \"\"\"\n    res = BaseResource.objects.get(short_id=resource_id)\n    scimeta_path = res.scimeta_path\n    scimeta_url = reverse('rest_download', kwargs={'path': scimeta_path})\n    return scimeta_url\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.setAVU","title":"<code>setAVU(attribute, value)</code>","text":"<p>Set an AVU at the resource level.</p> <p>This avoids mistakes in setting AVUs by assuring that the appropriate root path is alway used.</p> Source code in <code>hs_core/models.py</code> <pre><code>def setAVU(self, attribute, value):\n    \"\"\"Set an AVU at the resource level.\n\n    This avoids mistakes in setting AVUs by assuring that the appropriate root path\n    is alway used.\n    \"\"\"\n    if isinstance(value, bool):\n        value = str(value).lower()  # normalize boolean values to strings\n    istorage = self.get_s3_storage()\n    root_path = self.root_path\n    istorage.setAVU(root_path, attribute, value)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.set_discoverable","title":"<code>set_discoverable(value, user=None)</code>","text":"<p>Set the discoverable flag for a resource.</p> <p>:param value: True or False :param user: user requesting the change, or None for changes that are not user requests. :raises ValidationError: if the current configuration cannot be set to desired state</p> <p>This sets the discoverable flag (self.raccess.discoverable) for a resource based upon application logic. It is part of AbstractResource because its result depends upon resource state, and not just access control.</p> <ul> <li>This flag can only be set to True if the resource passes basic validations   <code>has_required_metata</code> and <code>has_required_content_files</code></li> <li>setting <code>discoverable</code> to <code>False</code> also sets <code>public</code> to <code>False</code></li> <li>setting <code>discoverable</code> to <code>True</code> does not change <code>public</code></li> </ul> <p>Thus, the setting public=True, discoverable=False is disallowed.</p> <p>If <code>user</code> is None, access control is not checked.  This happens when a resource has been invalidated outside of the control of a specific user. In this case, user can be None</p> Source code in <code>hs_core/models.py</code> <pre><code>def set_discoverable(self, value, user=None):\n    \"\"\"Set the discoverable flag for a resource.\n\n    :param value: True or False\n    :param user: user requesting the change, or None for changes that are not user requests.\n    :raises ValidationError: if the current configuration cannot be set to desired state\n\n    This sets the discoverable flag (self.raccess.discoverable) for a resource based\n    upon application logic. It is part of AbstractResource because its result depends\n    upon resource state, and not just access control.\n\n    * This flag can only be set to True if the resource passes basic validations\n      `has_required_metata` and `has_required_content_files`\n    * setting `discoverable` to `False` also sets `public` to `False`\n    * setting `discoverable` to `True` does not change `public`\n\n    Thus, the setting public=True, discoverable=False is disallowed.\n\n    If `user` is None, access control is not checked.  This happens when a resource has been\n    invalidated outside of the control of a specific user. In this case, user can be None\n    \"\"\"\n    # access control is separate from validation logic\n    if user is not None and not user.uaccess.can_change_resource_flags(self):\n        raise ValidationError(\"You don't have permission to change resource sharing status\")\n\n    # check that there is sufficient resource content\n    has_metadata = self.has_required_metadata\n    has_files = self.has_required_content_files()\n    if value and not (has_metadata and has_files):\n\n        if not has_metadata and not has_files:\n            msg = \"Resource does not have sufficient metadata and content files to be \" + \\\n                \"discoverable\"\n            raise ValidationError(msg)\n        elif not has_metadata:\n            msg = \"Resource does not have sufficient metadata to be discoverable\"\n            raise ValidationError(msg)\n        elif not has_files:\n            msg = \"Resource does not have sufficient content files to be discoverable\"\n            raise ValidationError(msg)\n\n    else:  # state change is allowed\n        self.raccess.discoverable = value\n        self.raccess.save()\n        self.set_public(False)\n        self.update_index()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.set_private_sharing_link","title":"<code>set_private_sharing_link(user, value)</code>","text":"<p>Set resource 'allow_private_sharing' flag to True or False. If allow_private_sharing is True then any user including anonymous user will be able to use the resource url to view the resource (view mode).</p> <p>:param user: user requesting the change :param value: True or False :raises PermissionDenied: if the user lacks permission to change resource flag</p> Source code in <code>hs_core/models.py</code> <pre><code>def set_private_sharing_link(self, user, value):\n    \"\"\"Set resource 'allow_private_sharing' flag to True or False.\n    If allow_private_sharing is True then any user including anonymous user will be able to use the resource url\n    to view the resource (view mode).\n\n    :param user: user requesting the change\n    :param value: True or False\n    :raises PermissionDenied: if the user lacks permission to change resource flag\n    \"\"\"\n    if not user.uaccess.can_change_resource_flags(self):\n        raise PermissionDenied(\"You don't have permission to change resource private link sharing \"\n                               \" status\")\n    self.raccess.allow_private_sharing = value\n    self.raccess.save()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.set_public","title":"<code>set_public(value, user=None)</code>","text":"<p>Set the public flag for a resource.</p> <p>:param value: True or False :param user: user requesting the change, or None for changes that are not user requests. :raises ValidationError: if the current configuration cannot be set to desired state</p> <p>This sets the public flag (self.raccess.public) for a resource based upon application logic. It is part of AbstractResource because its result depends upon resource state, and not just access control.</p> <ul> <li>This flag can only be set to True if the resource passes basic validations   <code>has_required_metata</code> and <code>has_required_content_files</code></li> <li>setting <code>public</code> to <code>True</code> also sets <code>discoverable</code> to <code>True</code></li> <li>setting <code>public</code> to <code>False</code> does not change <code>discoverable</code></li> <li>setting <code>public</code> to either also modifies the AVU isPublic for the resource.</li> </ul> <p>Thus, the setting public=True, discoverable=False is disallowed.</p> <p>If <code>user</code> is None, access control is not checked.  This happens when a resource has been invalidated outside of the control of a specific user. In this case, user can be None</p> Source code in <code>hs_core/models.py</code> <pre><code>def set_public(self, value, user=None):\n    \"\"\"Set the public flag for a resource.\n\n    :param value: True or False\n    :param user: user requesting the change, or None for changes that are not user requests.\n    :raises ValidationError: if the current configuration cannot be set to desired state\n\n    This sets the public flag (self.raccess.public) for a resource based\n    upon application logic. It is part of AbstractResource because its result depends\n    upon resource state, and not just access control.\n\n    * This flag can only be set to True if the resource passes basic validations\n      `has_required_metata` and `has_required_content_files`\n    * setting `public` to `True` also sets `discoverable` to `True`\n    * setting `public` to `False` does not change `discoverable`\n    * setting `public` to either also modifies the AVU isPublic for the resource.\n\n    Thus, the setting public=True, discoverable=False is disallowed.\n\n    If `user` is None, access control is not checked.  This happens when a resource has been\n    invalidated outside of the control of a specific user. In this case, user can be None\n    \"\"\"\n    # avoid import loop\n    from hs_core.signals import post_raccess_change\n    from hs_access_control.models.shortcut import zone_of_publicity\n\n    # access control is separate from validation logic\n    if user is not None and not user.uaccess.can_change_resource_flags(self):\n        raise ValidationError(\"You don't have permission to change resource sharing status\")\n\n    old_value = self.raccess.public  # is this a change?\n\n    # check that there is sufficient resource content\n    has_metadata = self.has_required_metadata\n    has_files = self.has_required_content_files()\n    if value and not (has_metadata and has_files):\n\n        if not has_metadata and not has_files:\n            msg = \"Resource does not have sufficient metadata and content files to be public\"\n            raise ValidationError(msg)\n\n        elif not has_metadata:\n            msg = \"Resource does not have sufficient metadata to be public\"\n            raise ValidationError(msg)\n\n        elif not has_files:\n            msg = \"Resource does not have sufficient content files to be public\"\n            raise ValidationError(msg)\n\n    else:  # make valid state change\n        self.raccess.public = value\n        if value:  # can't be public without being discoverable\n            self.raccess.discoverable = value\n        self.raccess.save()\n        post_raccess_change.send(sender=self, resource=self)\n        self.update_index()\n        zone_of_publicity(resource=self)\n        # public changed state: set isPublic metadata AVU accordingly\n        if value != old_value:\n            self.setAVU(\"isPublic\", self.raccess.public)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.set_published","title":"<code>set_published(value)</code>","text":"<p>Set the published flag for a resource.</p> <p>:param value: True or False</p> <p>This sets the published flag (self.raccess.published)</p> Source code in <code>hs_core/models.py</code> <pre><code>def set_published(self, value):\n    \"\"\"Set the published flag for a resource.\n\n    :param value: True or False\n\n    This sets the published flag (self.raccess.published)\n    \"\"\"\n    from hs_core.signals import post_raccess_change\n\n    self.raccess.published = value\n    self.raccess.immutable = value\n    if value:  # can't be published without being public\n        self.raccess.public = value\n    self.raccess.save()\n    post_raccess_change.send(sender=self, resource=self)\n    self.update_index()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.set_quota_holder","title":"<code>set_quota_holder(setter, new_holder)</code>","text":"<p>Set quota holder of the resource to new_holder who must be an owner.</p> <p>setter is the requesting user to transfer quota holder and setter must also be an owner</p> Source code in <code>hs_core/models.py</code> <pre><code>def set_quota_holder(self, setter, new_holder):\n    \"\"\"Set quota holder of the resource to new_holder who must be an owner.\n\n    setter is the requesting user to transfer quota holder and setter must also be an owner\n    \"\"\"\n    from hs_core.hydroshare.utils import validate_user_quota\n\n    if __debug__:\n        assert (isinstance(setter, User))\n        assert (isinstance(new_holder, User))\n    if not setter.uaccess.owns_resource(self) or \\\n            not new_holder.uaccess.owns_resource(self):\n        raise PermissionDenied(\"Only owners can set or be set as quota holder for the resource\")\n\n    # ensure the new holder has a bucket, buckets only exist for users with resources\n    istorage = self.get_s3_storage()\n    istorage.create_bucket(new_holder.userprofile.bucket_name)\n    # QuotaException will be raised if new_holder does not have enough quota to hold this\n    # new resource, in which case, set_quota_holder to the new user fails\n    validate_user_quota(new_holder, self.size)\n    # if the resource is new, it does not have a quota holder yet\n    if self.quota_holder:\n        self.get_s3_storage().new_quota_holder(self.short_id, new_holder.username)\n\n    self.quota_holder = new_holder\n    self.save()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.set_require_download_agreement","title":"<code>set_require_download_agreement(user, value)</code>","text":"<p>Set resource require_download_agreement flag to True or False. If require_download_agreement is True then user will be prompted to agree to resource rights statement before he/she can download resource files or bag.</p> <p>:param user: user requesting the change :param value: True or False :raises PermissionDenied: if the user lacks permission to change resource flag</p> Source code in <code>hs_core/models.py</code> <pre><code>def set_require_download_agreement(self, user, value):\n    \"\"\"Set resource require_download_agreement flag to True or False.\n    If require_download_agreement is True then user will be prompted to agree to resource\n    rights statement before he/she can download resource files or bag.\n\n    :param user: user requesting the change\n    :param value: True or False\n    :raises PermissionDenied: if the user lacks permission to change resource flag\n    \"\"\"\n    if not user.uaccess.can_change_resource_flags(self):\n        raise PermissionDenied(\"You don't have permission to change resource download agreement\"\n                               \" status\")\n    self.raccess.require_download_agreement = value\n    self.raccess.save()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.supports_delete_folder_on_zip","title":"<code>supports_delete_folder_on_zip(original_folder)</code>","text":"<p>Check if resource allows the original folder to be deleted upon zip.</p> Source code in <code>hs_core/models.py</code> <pre><code>def supports_delete_folder_on_zip(self, original_folder):\n    \"\"\"Check if resource allows the original folder to be deleted upon zip.\"\"\"\n    return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.supports_folder_creation","title":"<code>supports_folder_creation(folder_full_path)</code>","text":"<p>Check if resource supports creation of folder at the specified path.</p> Source code in <code>hs_core/models.py</code> <pre><code>def supports_folder_creation(self, folder_full_path):\n    \"\"\"Check if resource supports creation of folder at the specified path.\"\"\"\n    return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.supports_rename_path","title":"<code>supports_rename_path(src_full_path, tgt_full_path)</code>","text":"<p>Check if file/folder rename/move is allowed by this resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>def supports_rename_path(self, src_full_path, tgt_full_path):\n    \"\"\"Check if file/folder rename/move is allowed by this resource.\"\"\"\n    return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.supports_unzip","title":"<code>supports_unzip(file_to_unzip)</code>","text":"<p>Check if resource supports the unzipping of the specified file.</p> Source code in <code>hs_core/models.py</code> <pre><code>def supports_unzip(self, file_to_unzip):\n    \"\"\"Check if resource supports the unzipping of the specified file.\"\"\"\n    return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.supports_zip","title":"<code>supports_zip(folder_to_zip)</code>","text":"<p>Check if resource supports the specified folder to be zipped.</p> Source code in <code>hs_core/models.py</code> <pre><code>def supports_zip(self, folder_to_zip):\n    \"\"\"Check if resource supports the specified folder to be zipped.\"\"\"\n    return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.sysmeta_path","title":"<code>sysmeta_path(resource_id)</code>  <code>classmethod</code>","text":"<p>Get URL of resource map xml.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef sysmeta_path(cls, resource_id):\n    \"\"\"Get URL of resource map xml.\"\"\"\n    return \"{resource_id}/data/resourcemap.xml\".format(resource_id=resource_id)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.update_all_cached_metadata","title":"<code>update_all_cached_metadata()</code>","text":"<p>Update all fields in the cached metadata This method is primarily for use in management commands to refresh cached metadata</p> Source code in <code>hs_core/models.py</code> <pre><code>def update_all_cached_metadata(self):\n    \"\"\"\n    Update all fields in the cached metadata\n    This method is primarily for use in management commands to refresh cached metadata\n    \"\"\"\n    self.update_cached_metadata_field(field_name='all')\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.update_cached_metadata_field","title":"<code>update_cached_metadata_field(field_name)</code>","text":"<p>Update a specific field in the cached metadata or all fields if 'all' is specified</p> <p>:param field_name: The field to update ('creator', 'subject', etc.). If 'all', update all fields.</p> <p>NOTE: This method gets called from post_save and post_delete signal handler for any core metadata elements. We need to update the 'modified' field in cached metadata for any change to core metadata elements. The Date metadata element gets updated for the modified date type by the system as needed. We are depending on that change to modified date to update the 'modified' field in cached metadata.</p> Source code in <code>hs_core/models.py</code> <pre><code>def update_cached_metadata_field(self, field_name):\n    \"\"\"\n    Update a specific field in the cached metadata or all fields if 'all' is specified\n\n    :param field_name: The field to update ('creator', 'subject', etc.). If 'all', update all fields.\n\n    NOTE: This method gets called from post_save and post_delete signal handler for\n    any core metadata elements. We need to update the 'modified' field in cached metadata\n    for any change to core metadata elements. The Date metadata element gets updated for the\n    modified date type by the system as needed. We are depending on that change to modified date\n    to update the 'modified' field in cached metadata.\n    \"\"\"\n    self.refresh_from_db()\n    metadata = self.metadata\n    copied_metadata = copy.deepcopy(self.cached_metadata)\n\n    # These are the fields that need to be updated in cached metadata\n    field_updaters = {\n        'creator': self._update_creators_field,\n        'contributor': self._update_contributors_field,\n        'coverage': self._update_coverage_field,\n        'title': self._update_title_field,\n        'subject': self._update_subjects_field,\n        'date': self._update_date_field,\n        'status': self._update_status_field,\n        'description': self._update_abstract_field,\n        'relation': self._update_relation_field,\n        'geospatialrelation': self._update_geospatialrelation_field,\n        'fundingagency': self._update_fundingagency_field,\n        'rights': self._update_rights_field,\n        'language': self._update_language_field,\n        'identifier': self._update_identifier_field,\n        'type': self._update_type_field,\n        'publisher': self._update_publisher_field\n    }\n\n    # Update all fields if 'all' is specified\n    if field_name == 'all':\n        for updater in field_updaters.values():\n            updater(copied_metadata, metadata)\n    else:\n        # Update the specified field\n        if field_name in field_updaters:\n            field_updaters[field_name](copied_metadata, metadata)\n\n    # Ensure all required fields are present\n    self._ensure_required_fields(copied_metadata, metadata)\n\n    # Update the modified date every time a metadata element is updated/deleted, or when 'all' is specified\n    modified_date = metadata.dates.filter(type='modified').first()\n    if field_name != 'all':\n        # this is the case of updating cached metadata as part of metadata save/delete signal handler\n        copied_metadata['modified'] = now().isoformat()\n        if modified_date:\n            # this update won't trigger the post_save signal for Date model since we are using update query api\n            type(modified_date).objects.filter(id=modified_date.id).update(start_date=copied_metadata['modified'])\n    else:\n        # this is the case of updating cached metadata as part of management command\n        if modified_date:\n            copied_metadata['modified'] = modified_date.start_date.isoformat()\n        else:\n            copied_metadata['modified'] = self.updated.isoformat()\n\n    type(self).objects.filter(id=self.id).update(cached_metadata=copied_metadata)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.update_index","title":"<code>update_index()</code>","text":"<p>updates previous versions of a resource (self) in index</p> Source code in <code>hs_core/models.py</code> <pre><code>def update_index(self):\n    \"\"\"updates previous versions of a resource (self) in index\"\"\"\n    prev_version_resource_relation_meta = Relation.objects.filter(type='isReplacedBy',\n                                                                  value__contains=self.short_id).first()\n    if prev_version_resource_relation_meta:\n        prev_version_res = prev_version_resource_relation_meta.metadata.resource\n        if prev_version_res.raccess.discoverable or prev_version_res.raccess.public:\n            # saving to trigger index update for this previous version of resource\n            prev_version_res.save()\n        prev_version_res.update_index()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.AbstractResource.update_public_and_discoverable","title":"<code>update_public_and_discoverable()</code>","text":"<p>Update the settings of the public and discoverable flags for changes in metadata.</p> Source code in <code>hs_core/models.py</code> <pre><code>def update_public_and_discoverable(self):\n    \"\"\"Update the settings of the public and discoverable flags for changes in metadata.\"\"\"\n    if self.raccess.discoverable and not self.can_be_public_or_discoverable:\n        self.set_discoverable(False)  # also sets Public\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource","title":"<code>BaseResource</code>","text":"<p>               Bases: <code>Page</code>, <code>AbstractResource</code></p> <p>Combine mezzanine Page model and AbstractResource model to establish base resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>class BaseResource(Page, AbstractResource):\n    \"\"\"Combine mezzanine Page model and AbstractResource model to establish base resource.\"\"\"\n\n    resource_type = models.CharField(max_length=50, default=\"GenericResource\")\n    # this locked_time field is added for resource versioning locking representing\n    # the time when the resource is locked for a new version action. A value of null\n    # means the resource is not locked\n    locked_time = models.DateTimeField(null=True, blank=True)\n\n    objects = PublishedManager()\n    public_resources = PublicResourceManager()\n    discoverable_resources = DiscoverableResourceManager()\n\n    collections = models.ManyToManyField('BaseResource', related_name='resources')\n\n    # used during discovery as well as in all other places in UI where resource type is displayed\n    display_name = 'Generic'\n\n    class Meta:\n        \"\"\"Define meta properties for BaseResource model.\"\"\"\n\n        verbose_name = 'Generic'\n        db_table = 'hs_core_genericresource'\n\n    def can_add(self, request):\n        \"\"\"Pass through to abstract resource can_add function.\"\"\"\n        return AbstractResource.can_add(self, request)\n\n    def can_change(self, request):\n        \"\"\"Pass through to abstract resource can_add function.\"\"\"\n        return AbstractResource.can_change(self, request)\n\n    def can_delete(self, request):\n        \"\"\"Pass through to abstract resource can_delete function.\"\"\"\n        return AbstractResource.can_delete(self, request)\n\n    def can_view(self, request):\n        \"\"\"Pass through to abstract resource can_view function.\"\"\"\n        return AbstractResource.can_view(self, request)\n\n    def get_s3_storage(self):\n        \"\"\"Return S3Storage.\"\"\"\n        return S3Storage()\n\n    # Paths relative to the resource\n    @property\n    def root_path(self):\n        \"\"\"Return the root folder of the S3 structure containing resource files.\n\n        Note that this folder doesn't directly contain the resource files;\n        They are contained in ./data/contents/* instead.\n        \"\"\"\n        return self.short_id\n\n    @property\n    def file_path(self):\n        \"\"\"Return the file path of the resource.\n\n        This is the root path plus \"data/contents\".\n        This is the root of the folder structure for resource files.\n        \"\"\"\n        return os.path.join(self.root_path, \"data\", \"contents\")\n\n    @property\n    def scimeta_path(self):\n        \"\"\" path to science metadata file (in S3) \"\"\"\n        return os.path.join(self.root_path, \"data\", \"resourcemetadata.xml\")\n\n    @property\n    def resmap_path(self):\n        \"\"\" path to resource map file (in S3) \"\"\"\n        return os.path.join(self.root_path, \"data\", \"resourcemap.xml\")\n\n    @property\n    def bag_path(self):\n        \"\"\"Return the unique S3 path to the bag for the resource.\n\n        Since this is a cache, it is stored in a different place than the resource files.\n        \"\"\"\n        bagit_path = getattr(settings, 'HS_BAGIT_PATH', 'bags')\n        bagit_postfix = getattr(settings, 'HS_BAGIT_POSTFIX', 'zip')\n        return os.path.join(bagit_path, self.short_id + '.' + bagit_postfix)\n\n    @property\n    def bag_url(self):\n        \"\"\"Get bag url of resource data bag.\"\"\"\n        bagit_path = getattr(settings, 'HS_BAGIT_PATH', 'bags')\n        bagit_postfix = getattr(settings, 'HS_BAGIT_POSTFIX', 'zip')\n        bag_path = \"{path}/{resource_id}.{postfix}\".format(path=bagit_path,\n                                                           resource_id=self.short_id,\n                                                           postfix=bagit_postfix)\n        reverse_url = reverse(\"django_s3_download\", kwargs={\"path\": bag_path})\n        return reverse_url\n\n    @property\n    def bag_checksum(self):\n        \"\"\"\n        get checksum of resource bag. Currently only published resources have bag checksums computed and saved\n        :return: checksum if bag checksum exists; empty string '' otherwise\n        \"\"\"\n        extra_data = self.extra_data\n        if 'bag_checksum' in extra_data and extra_data['bag_checksum']:\n            return extra_data['bag_checksum'].strip('\\n')\n        else:\n            return ''\n\n    @bag_checksum.setter\n    def bag_checksum(self, checksum):\n        \"\"\"\n        Set bag checksum implemented as a property setter.\n        :param checksum: checksum value to be set\n        \"\"\"\n        if checksum:\n            extra_data = self.extra_data\n            extra_data['bag_checksum'] = checksum\n            self.extra_data = extra_data\n            self.save()\n        else:\n            return ValidationError(\"checksum to set on the bag of the resource {} is empty\".format(self.short_id))\n\n    # URIs relative to resource\n    # these are independent of federation strategy\n    # TODO: utilize \"reverse\" abstraction to tie this to urls.py for robustness\n\n    # add these one by one to avoid errors.\n\n    # @property\n    # def root_uri(self):\n    #     pass\n\n    # @property\n    # def scimeta_uri(self):\n    #     return os.path.join(self.root_uri, 'scimeta')\n\n    # @property\n    # def sysmeta_uri(self):\n    #     return os.path.join(self.root_uri, 'sysmeta')\n\n    # @property\n    # def file_uri(self):\n    #     return os.path.join(self.root_uri, 'files')\n\n    @staticmethod\n    def parse_creator_name(creator):\n        creator_name = creator.name.strip()\n        name = HumanName(creator_name)\n        if not name.first or not name.last:\n            name_parts = creator_name.split()\n            if not name.first and name_parts:\n                name.first = name_parts[0]\n            if not name.last and name_parts:\n                name.last = name_parts[-1]\n        return name.first, name.last\n\n    @staticmethod\n    def get_contributor_data(self):\n        contributor_list = []\n\n        for contributor in self.metadata.contributors.all():\n            if contributor.name:\n                first_name, last_name = self.parse_creator_name(contributor)\n                contributor_data = {\n                    \"name\": contributor.name,\n                    \"nameType\": \"Personal\",\n                    \"givenName\": first_name,\n                    \"familyName\": last_name,\n                    \"contributorType\": \"Other\",\n                    \"affiliation\": [{\"name\": contributor.organization or \"CUAHSI\"}],\n                    \"nameIdentifiers\": []\n                }\n                if contributor.hydroshare_user_id:\n                    contributor_data[\"nameIdentifiers\"].append({\n                        \"nameIdentifier\": f\"https://hydroshare.org{contributor.relative_uri}\",\n                        \"nameIdentifierScheme\": \"Other\",\n                    })\n                if contributor.identifiers.get('ORCID'):\n                    contributor_data[\"nameIdentifiers\"].append({\n                        \"nameIdentifier\": contributor.identifiers['ORCID'],\n                        \"nameIdentifierScheme\": \"ORCID\",\n                        \"schemeUri\": \"https://orcid.org\"\n                    })\n            else:\n                contributor_data = {\n                    \"name\": contributor.organization or \"CUAHSI\",\n                    \"nameType\": \"Organizational\",\n                    \"contributorType\": \"Other\",\n                    \"nameIdentifiers\": []\n                }\n\n            contributor_list.append(contributor_data)\n\n        return contributor_list\n\n    @staticmethod\n    def get_funding_references(self):\n        \"\"\"\n        Build fundingReferences list for the DataCite JSON payload.\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        funding_references = []\n\n        def get_funder_id(funder_name):\n            funder_name_lower = funder_name.lower()\n            encoded_funder_name = urllib.parse.quote(funder_name_lower)\n            url = f\"https://api.ror.org/v2/organizations?filter=types:funder&amp;query={encoded_funder_name}\"\n\n            try:\n                response = requests.get(url, verify=False, timeout=10)\n                if response.status_code == 200:\n                    items = response.json().get('items', [])\n                    for item in items:\n                        for name in item.get('names', []):\n                            if name.get('value', '').lower() == funder_name_lower:\n                                return item.get('id')\n                else:\n                    logger.error(f\"Failed to get funder ID for '{funder_name}'. Status: {response.status_code}\")\n            except requests.RequestException as e:\n                logger.error(f\"Error fetching funder ID for '{funder_name}': {str(e)}\")\n\n            return ''\n\n        for funder in self.metadata.funding_agencies.all():\n            funder_data = {\n                \"funderName\": funder.agency_name\n            }\n\n            funder_id = get_funder_id(funder.agency_name)\n            if funder_id:\n                funder_data[\"funderIdentifier\"] = funder_id\n                funder_data[\"funderIdentifierType\"] = \"ROR\"\n            elif funder.agency_url:\n                funder_data[\"funderIdentifier\"] = funder.agency_url\n                funder_data[\"funderIdentifierType\"] = \"Other\"\n\n            if funder.award_number:\n                funder_data[\"awardNumber\"] = funder.award_number\n            if funder.award_title:\n                funder_data[\"awardTitle\"] = funder.award_title\n\n            funding_references.append(funder_data)\n\n        return funding_references\n\n    def get_related_items(self):\n        \"\"\"\n        Returns two lists:\n        - relatedIdentifiers: list of dictionaries for relatedIdentifiers\n        - relatedItems: list of dictionaries for relatedItems\n        \"\"\"\n        VALID_RELATION_TYPE_MAP = {\n            RelationTypes.isPartOf: \"IsPartOf\",\n            RelationTypes.hasPart: \"HasPart\",\n            RelationTypes.isExecutedBy: \"IsSupplementedBy\",\n            RelationTypes.isCreatedBy: \"IsDocumentedBy\",\n            RelationTypes.isVersionOf: \"IsVersionOf\",\n            RelationTypes.isReplacedBy: \"IsNewVersionOf\",\n            RelationTypes.isDescribedBy: \"IsDescribedBy\",\n            RelationTypes.conformsTo: \"References\",\n            RelationTypes.hasFormat: \"HasPart\",\n            RelationTypes.isFormatOf: \"IsPartOf\",\n            RelationTypes.isRequiredBy: \"IsRequiredBy\",\n            RelationTypes.requires: \"Requires\",\n            RelationTypes.isReferencedBy: \"IsReferencedBy\",\n            RelationTypes.references: \"References\",\n            RelationTypes.replaces: \"Replaces\",\n            RelationTypes.source: \"IsDerivedFrom\",\n            RelationTypes.isSimilarTo: \"IsIdenticalTo\",\n            RelationTypes.relation: \"References\"\n        }\n\n        RESOURCE_TYPE_MAP = {\n            \"CompositeResource\": \"Dataset\",\n            \"CollectionResource\": \"Collection\",\n            \"ToolResource\": \"Software\",\n            \"ModelProgramResource\": \"Software\",\n            \"ModelInstanceResource\": \"Model\",\n            \"GenericResource\": \"Dataset\",\n            \"TimeSeriesResource\": \"Dataset\",\n            \"GeographicFeatureResource\": \"Dataset\",\n            \"GeographicRasterResource\": \"Dataset\",\n            \"MultidimensionalResource\": \"Dataset\",\n            \"ScriptResource\": \"Software\",\n            \"WebAppResource\": \"Service\"\n        }\n\n        related_identifiers = []\n        related_items = []\n\n        for relation in self.metadata.relations.all():\n            match = re.search(r'(https?://\\S+|10\\.\\d{4,9}/\\S+)', relation.value)\n            identifier_value = match.group(0) if match else relation.value\n            identifier_type = \"URL\" if identifier_value.startswith(\"http\") else \"DOI\"\n\n            relation_type = VALID_RELATION_TYPE_MAP.get(relation.type)\n\n            related_identifiers.append({\n                \"relatedIdentifier\": identifier_value,\n                \"relatedIdentifierType\": identifier_type,\n                \"relationType\": relation_type\n            })\n\n            related_items.append({\n                \"relatedItemType\": RESOURCE_TYPE_MAP.get(self.resource_type, \"Other\"),\n                \"relationType\": relation_type,\n                \"relatedItemIdentifier\": {\n                    \"relatedItemIdentifierType\": identifier_type,\n                    \"relatedItemIdentifier\": identifier_value\n                }\n            })\n\n        return related_identifiers, related_items\n\n    def get_dates_for_doi(self):\n        \"\"\"\n        Returns a list of date dictionaries formatted for the DOI creation payload,\n        handling both temporal coverage and metadata dates.\n        \"\"\"\n        date_mapping = {\n            'created': 'Created',\n            'modified': 'Updated',\n            'published': 'Accepted',\n        }\n        dates = []\n        # Handling temporal coverage date\n        temporal_dates = self.metadata.coverages.filter(type='period').first()\n        if temporal_dates and temporal_dates.value.get('start') and temporal_dates.value.get('end'):\n            # Format temporal coverage date as a range\n            coverage_date = {\n                \"date\": f\"{temporal_dates.value['start']}/{temporal_dates.value['end']}\",\n                \"dateType\": \"Coverage\"\n            }\n            dates.append(coverage_date)\n\n        # Handling individual metadata dates\n        metadata_dates = self.metadata.dates.all()\n        for date in metadata_dates:\n            if date.type in date_mapping:\n                # Mapping the date types (created, modified, published)\n                mapped_date = {\n                    \"date\": date.start_date.strftime(\"%Y-%m-%d\"),\n                    \"dateType\": date_mapping[date.type]\n                }\n                dates.append(mapped_date)\n\n        return dates\n\n    def get_datacite_deposit_json(self, test_mode=False):\n        \"\"\"\n        Return JSON payload for creating a DOI with DataCite API.\n        Conforms to DataCite REST API for DOI creation: https://support.datacite.org/reference/post_dois\n        \"\"\"\n        logger = logging.getLogger(__name__)\n\n        if not self.metadata.title:\n            raise ValidationError(f\"No title found for resource {self.short_id}\")\n        if not self.metadata.creators.count():\n            raise ValidationError(f\"No creators found for resource {self.short_id}\")\n        if not self.metadata.description or not self.metadata.description.abstract:\n            logger.warning(f\"No abstract found for resource {self.short_id}. Using empty string.\")\n            self.metadata.description = type('obj', (), {'abstract': ''})()\n\n        # Add \"test\" to the suffix if in test mode\n        suffix = f\"hs.{self.short_id}-test\" if test_mode else f\"hs.{self.short_id}\"\n        doi = f\"{settings.DATACITE_PREFIX}/{suffix}\"\n\n        payload = {\n            \"data\": {\n                \"type\": \"dois\",\n                \"attributes\": {\n                    \"doi\": doi,\n                    \"prefix\": f\"{settings.DATACITE_PREFIX}\",\n                    \"suffix\": suffix,  # Use modified suffix\n                    \"event\": \"publish\",\n                    \"url\": None,\n                    \"creators\": [],\n                    \"titles\": [{\"title\": self.metadata.title.value, \"lang\": \"en\"}],\n                    \"publisher\": {\n                        \"name\": \"Consortium of Universities for the Advancement of Hydrologic Science, Inc\",\n                        \"lang\": \"en\",\n                        \"publisherIdentifier\": \"https://ror.org/04s2bx355\",\n                        \"publisherIdentifierScheme\": \"ROR\",\n                        \"schemeUri\": \"https://ror.org\"\n                    },\n                    \"publicationYear\": None,\n                    \"subjects\": [],\n                    \"contributors\": [],\n                    \"identifiers\": [],\n                    \"dates\": [],\n                    \"language\": \"en\",\n                    \"types\": {\n                        \"resourceTypeGeneral\": \"Dataset\",\n                        \"resourceType\": self.resource_type,\n                        \"schemaOrg\": \"Dataset\",\n                        \"bibtex\": \"misc\",\n                        \"citeproc\": \"dataset\"\n                    },\n                    # \"relatedIdentifiers\": [],\n                    \"relatedItems\": [],\n                    \"sizes\": [],\n                    \"formats\": [],\n                    \"version\": None,\n                    \"rightsList\": [],\n                    \"descriptions\": [],\n                    \"geoLocations\": [],\n                    \"fundingReferences\": [],\n                    \"container\": {\n                        \"type\": \"DataRepository\",\n                        \"identifier\": \"https://www.hydroshare.org\",\n                        \"identifierType\": \"URL\",\n                        \"title\": \"HydroShare Resources\"\n                    },\n                    \"contentUrl\": [self.bag_url] if hasattr(self, 'bag_url') and self.bag_url else [],\n                    \"schemaVersion\": \"http://datacite.org/schema/kernel-4\",\n                    \"state\": \"findable\"\n                },\n                \"relationships\": {\n                    \"client\": {\n                        \"data\": {\n                            \"type\": \"repositories\",\n                            \"id\": f\"{settings.DATACITE_USERNAME}\"\n                        }\n                    }\n                }\n            }\n        }\n\n        pub_date = self.metadata.dates.filter(type='published').first()\n        payload[\"data\"][\"attributes\"][\"publicationYear\"] = str(\n            pub_date.start_date.year if pub_date else self.updated.year)\n\n        hs_identifier = self.metadata.identifiers.filter(name='hydroShareIdentifier').first()\n        if hs_identifier:\n            payload[\"data\"][\"attributes\"][\"url\"] = hs_identifier.url\n        else:\n            raise ValidationError(f\"No hydroShareIdentifier found for resource {self.short_id}\")\n\n        creators = self.metadata.creators.all()\n        first_creator = [cr for cr in creators if cr.order == 1]\n        other_creators = [cr for cr in creators if cr.order &gt; 1]\n        for creator in [first_creator[0]] + other_creators if first_creator else creators:\n            if creator.name:\n                first_name, last_name = self.parse_creator_name(creator)\n                creator_data = {\n                    \"name\": creator.name,\n                    \"nameType\": \"Personal\",\n                    \"givenName\": first_name,\n                    \"familyName\": last_name,\n                    \"affiliation\": [{\"name\": creator.organization or \"CUAHSI\"}],\n                    \"nameIdentifiers\": []\n                }\n                if creator.hydroshare_user_id:\n                    creator_data[\"nameIdentifiers\"].append({\n                        \"nameIdentifier\": f\"https://hydroshare.org{creator.relative_uri}\",\n                        \"nameIdentifierScheme\": \"Other\",\n                    })\n                if creator.identifiers.get('ORCID'):\n                    creator_data[\"nameIdentifiers\"].append({\n                        \"nameIdentifier\": creator.identifiers['ORCID'],\n                        \"nameIdentifierScheme\": \"ORCID\",\n                        \"schemeUri\": \"https://orcid.org\"\n                    })\n                payload[\"data\"][\"attributes\"][\"creators\"].append(creator_data)\n            else:\n                creator_data = {\n                    \"name\": creator.organization or \"CUAHSI\",\n                    \"nameType\": \"Organizational\",\n                    \"nameIdentifiers\": []\n                }\n                payload[\"data\"][\"attributes\"][\"creators\"].append(creator_data)\n\n        payload[\"data\"][\"attributes\"][\"contributors\"] = self.get_contributor_data(self)\n\n        for subject in self.metadata.subjects.all():\n            payload[\"data\"][\"attributes\"][\"subjects\"].append({\n                \"subject\": subject.value,\n            })\n\n        payload[\"data\"][\"attributes\"][\"dates\"] = self.get_dates_for_doi()\n\n        if self.metadata.description and self.metadata.description.abstract:\n            payload[\"data\"][\"attributes\"][\"descriptions\"] = [\n                {\n                    \"description\": self.metadata.description.abstract,\n                    \"descriptionType\": \"Abstract\",\n                    \"lang\": \"en\"\n                }\n            ]\n\n        if self.metadata.rights and self.metadata.rights.url:\n            rights_data = {\n                \"rights\": self.metadata.rights.statement[:2000],\n                \"rightsUri\": self.metadata.rights.url\n            }\n            if pub_date:\n                rights_data[\"rightsUriStartDate\"] = pub_date.start_date.strftime(\"%Y-%m-%d\")\n            payload[\"data\"][\"attributes\"][\"rightsList\"] = [rights_data]\n\n        related_identifiers, related_items = self.get_related_items()\n        # payload[\"data\"][\"attributes\"][\"relatedIdentifiers\"] = related_identifiers\n        payload[\"data\"][\"attributes\"][\"relatedItems\"] = related_items\n\n        for coverage in self.metadata.coverages.all():\n            if coverage.type == 'box':\n                box_values = coverage.value\n                if all(k in box_values for k in ('westlimit', 'eastlimit', 'southlimit', 'northlimit')):\n                    payload[\"data\"][\"attributes\"].setdefault(\"geoLocations\", []).append({\n                        \"geoLocationBox\": {\n                            \"westBoundLongitude\": str(box_values['westlimit']),\n                            \"eastBoundLongitude\": str(box_values['eastlimit']),\n                            \"southBoundLatitude\": str(box_values['southlimit']),\n                            \"northBoundLatitude\": str(box_values['northlimit'])\n                        }\n                    })\n            elif coverage.type == 'point':\n                point_values = coverage.value\n                longitude = point_values.get('longitude', point_values.get('east'))\n                latitude = point_values.get('latitude', point_values.get('north'))\n\n                if longitude is not None and latitude is not None:\n                    geo_location = {\n                        \"geoLocationPoint\": {\n                            \"pointLongitude\": str(longitude),\n                            \"pointLatitude\": str(latitude)\n                        }\n                    }\n                    if point_values.get('name'):\n                        geo_location[\"geoLocationPlace\"] = point_values['name']\n\n                    payload[\"data\"][\"attributes\"].setdefault(\"geoLocations\", []).append(geo_location)\n\n        payload[\"data\"][\"attributes\"][\"fundingReferences\"] = self.get_funding_references(self)\n\n        for format_obj in self.metadata.formats.all():\n            payload[\"data\"][\"attributes\"][\"formats\"].append(format_obj.value)\n\n        all_identifiers = self.metadata.identifiers.all()\n\n        for identifier in all_identifiers:\n            if identifier.name == 'doi':\n                payload[\"data\"][\"attributes\"].setdefault(\"identifiers\", []).append({\n                    \"identifier\": identifier.url,\n                    \"identifierType\": identifier.name\n                })\n            else:\n                payload[\"data\"][\"attributes\"].setdefault(\"alternateIdentifiers\", []).append({\n                    \"alternateIdentifier\": identifier.url,\n                    \"alternateIdentifierType\": identifier.name\n                })\n\n        # if self.metadata.citation and hasattr(self.metadata.citation, 'value'):\n        #     payload[\"data\"][\"attributes\"][\"identifiers\"].append({\n        #         \"alternateIdentifier\": self.metadata.citation.value,\n        #         \"alternateIdentifierType\": \"Citation\"\n        #     })\n\n        if hasattr(self, 'size') and self.size:\n            payload[\"data\"][\"attributes\"][\"sizes\"].append(f\"{self.size} bytes\")\n\n        if hasattr(self, 'version') and self.version:\n            payload[\"data\"][\"attributes\"][\"version\"] = str(self.version)\n\n        return json.dumps(payload, indent=2)\n\n    @property\n    def size(self):\n        \"\"\"Return the total size of all data files in S3.\n\n        This size does not include metadata. Just files. Specifically,\n        resourcemetadata.xml, systemmetadata.xml are not included in this\n        size estimate.\n\n        Raises SessionException if S3 fails.\n        \"\"\"\n        # trigger file size read for files that haven't been set yet\n        res_size = 0\n        if self.files.count() &gt; 0:\n            for f in self.files.filter(_size__lt=0):\n                f.calculate_size()\n            # compute the total file size for the resource\n            res_size_dict = self.files.aggregate(Sum('_size'))\n            res_size = res_size_dict['_size__sum']\n\n        return res_size\n\n    @property\n    def verbose_name(self):\n        \"\"\"Return verbose name of content_model.\"\"\"\n        return self.get_content_model()._meta.verbose_name\n\n    @property\n    def discovery_content_type(self):\n        \"\"\"Return name used for the content type in discovery/solr search.\"\"\"\n        return self.get_content_model().display_name\n\n    @property\n    def can_be_submitted_for_metadata_review(self):\n        \"\"\"Determine when data and metadata are complete enough for the resource to be published.\n\n        The property can be overriden by specific resource type which is not appropriate for\n        publication such as the Web App resource\n        :return:\n        \"\"\"\n        if self.raccess.published:\n            return False\n\n        return self.can_be_public_or_discoverable\n\n    @classmethod\n    def get_supported_upload_file_types(cls):\n        \"\"\"Get supported upload types for a resource.\n\n        This can be overridden to choose which types of file can be uploaded by a subclass.\n\n        By default, all file types are supported\n        \"\"\"\n        # TODO: this should be replaced by an instance method.\n        return ('.*')\n\n    @classmethod\n    def can_have_multiple_files(cls):\n        \"\"\"Return True if multiple files can be uploaded.\n\n        This can be overridden to choose how many files can be uploaded by a subclass.\n\n        By default, uploads are not limited.\n        \"\"\"\n        # TODO: this should be replaced by an instance method.\n        return True\n\n    @classmethod\n    def can_have_files(cls):\n        \"\"\"Return whether the resource supports files at all.\n\n        This can be overridden to choose whether files can be uploaded by a subclass.\n\n        By default, uploads are allowed.\n        \"\"\"\n        # TODO: this should be replaced by an instance method.\n        return True\n\n    def get_hs_term_dict(self):\n        \"\"\"Return a dict of HS Terms and their values.\n\n        Will be used to parse webapp url templates\n\n        NOTES FOR ANY SUBCLASS OF THIS CLASS TO OVERRIDE THIS FUNCTION:\n        resource types that inherit this class should add/merge their resource-specific HS Terms\n        into this dict\n        \"\"\"\n        hs_term_dict = {}\n\n        hs_term_dict[\"HS_RES_ID\"] = self.short_id\n        hs_term_dict[\"HS_RES_TYPE\"] = self.resource_type\n        hs_term_dict.update(self.extra_metadata.items())\n\n        return hs_term_dict\n\n    def replaced_by(self):\n        \"\"\" return a list or resources that replaced this one \"\"\"\n        from hs_core.hydroshare import get_resource_by_shortkey\n\n        replaced_by_resources = []\n\n        def get_replaced_by(resource):\n            replace_relation_meta = resource.metadata.relations.all().filter(type=RelationTypes.isReplacedBy).first()\n            if replace_relation_meta is not None:\n                version_citation = replace_relation_meta.value\n                if '/resource/' in version_citation:\n                    version_res_id = version_citation.split('/resource/')[-1]\n                    try:\n                        new_version_res = get_resource_by_shortkey(version_res_id, or_404=False)\n                        replaced_by_resources.append(new_version_res)\n                        get_replaced_by(new_version_res)\n                    except BaseResource.DoesNotExist:\n                        pass\n\n        get_replaced_by(self)\n        return replaced_by_resources\n\n    def get_relation_version_res_url(self, rel_type):\n        \"\"\"Extracts the resource url from resource citation stored in relation metadata for resource\n        versioning\n        :param rel_type: type of relation (allowed types are: 'isVersionOf' and 'isReplacedBy')\n        \"\"\"\n        relation_meta_obj = self.metadata.relations.filter(type=rel_type).first()\n        if relation_meta_obj is not None:\n            # get the resource url from resource citation\n            version_res_url = relation_meta_obj.value.split(',')[-1]\n            return version_res_url\n        else:\n            return ''\n\n    @property\n    def spam_patterns(self):\n        # Compile a single regular expression that will match any individual\n        # pattern from a given list of patterns, case-insensitive.\n        # ( '|' is a special character in regular expressions. An expression\n        # 'A|B' will match either 'A' or 'B' ).\n        full_pattern = re.compile(\"|\".join(patterns), re.IGNORECASE)\n\n        if self.metadata:\n            try:\n                match = re.search(full_pattern, self.metadata.title.value)\n                if match is not None:\n                    return match\n            except AttributeError:\n                # no title\n                pass\n\n            try:\n                for sub in self.metadata.subjects.all():\n                    match = re.search(full_pattern, sub.value)\n                    if match is not None:\n                        return match\n            except AttributeError:\n                # no keywords\n                pass\n\n            try:\n                match = re.search(full_pattern, self.metadata.description.abstract)\n                if match is not None:\n                    return match\n            except AttributeError:\n                # no abstract\n                pass\n\n        return None\n\n    @property\n    def show_in_discover(self):\n        \"\"\"\n        return True if a resource should be exhibited\n        A resource should be exhibited if it is at least discoverable\n        and not replaced by anything that exists and is at least discoverable.\n        \"\"\"\n        if not self.raccess.discoverable:\n            return False  # not exhibitable\n\n        replaced_by_resources = self.replaced_by()\n        if any([res.raccess.discoverable for res in replaced_by_resources]):\n            # there is a newer discoverable resource - so this resource should not be shown in discover\n            return False\n\n        if not self.spam_allowlisted and not self.raccess.published:\n            if self.spam_patterns:\n                return False\n\n        return True\n\n    def update_relation_meta(self):\n        \"\"\"Updates the citation stored in relation metadata for relation type\n        'isReplacedBy', 'isPartOf' and 'hasPart' if needed\"\"\"\n\n        from hs_core.hydroshare import get_resource_by_shortkey\n\n        def _update_relation_meta(relation_meta_obj):\n            relation_updated = False\n            if relation_meta_obj.value and '/resource/' in relation_meta_obj.value:\n                version_citation = relation_meta_obj.value\n                version_res_id = version_citation.split('/resource/')[-1]\n                try:\n                    version_res = get_resource_by_shortkey(version_res_id, or_404=False)\n                except BaseResource.DoesNotExist:\n                    relation_meta_obj.delete()\n                    relation_updated = True\n                    return relation_updated\n                current_version_citation = version_res.get_citation()\n                if current_version_citation != version_citation:\n                    relation_meta_obj.value = current_version_citation\n                    relation_meta_obj.save()\n                    relation_updated = True\n            return relation_updated\n\n        relations = self.metadata.relations.all()\n        replace_relation = [rel for rel in relations if rel.type == RelationTypes.isReplacedBy]\n        replace_relation_updated = False\n        if replace_relation:\n            replace_relation = replace_relation[0]\n            replace_relation_updated = _update_relation_meta(replace_relation)\n\n        part_of_relation_updated = False\n        for part_of_relation in [rel for rel in relations if rel.type == RelationTypes.isPartOf]:\n            if _update_relation_meta(part_of_relation):\n                part_of_relation_updated = True\n\n        has_part_relation_updated = False\n        for has_part_relation in [rel for rel in relations if rel.type == RelationTypes.hasPart]:\n            if _update_relation_meta(has_part_relation):\n                has_part_relation_updated = True\n\n        if any([replace_relation_updated, part_of_relation_updated, has_part_relation_updated]):\n            self.setAVU(\"bag_modified\", True)\n            self.setAVU(\"metadata_dirty\", True)\n            self.refresh_from_db(fields=['cached_metadata'])\n\n    def get_non_preferred_path_names(self):\n        \"\"\"Returns a list of file/folder paths that do not meet hydroshare file/folder preferred naming convention\"\"\"\n\n        def find_non_preferred_folder_paths(dir_path):\n            if not dir_path.startswith(self.file_path):\n                dir_path = os.path.join(self.file_path, dir_path)\n\n            folders, _, _ = istorage.listdir(dir_path)\n            for folder in folders:\n                if folder not in not_preferred_paths and not ResourceFile.check_for_preferred_name(folder):\n                    folder_path = os.path.join(dir_path, folder)\n                    folder_path = folder_path[len(self.file_path) + 1:]\n                    not_preferred_paths.append(folder_path)\n                subdir_path = os.path.join(dir_path, folder)\n                find_non_preferred_folder_paths(subdir_path)\n\n        not_preferred_paths = []\n        istorage = self.get_s3_storage()\n        # check for non-conforming file names\n        for res_file in self.files.all():\n            short_path = res_file.short_path\n            _, file_name = os.path.split(short_path)\n            if not ResourceFile.check_for_preferred_name(file_name):\n                not_preferred_paths.append(short_path)\n\n        # check for non-conforming folder names\n        find_non_preferred_folder_paths(self.file_path)\n        return not_preferred_paths\n\n    def get_relative_path(self, dir_path):\n        if dir_path.startswith(self.file_path):\n            dir_path = dir_path[len(self.file_path) + 1:]\n        return dir_path\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.bag_checksum","title":"<code>bag_checksum</code>  <code>property</code> <code>writable</code>","text":"<p>get checksum of resource bag. Currently only published resources have bag checksums computed and saved :return: checksum if bag checksum exists; empty string '' otherwise</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.bag_path","title":"<code>bag_path</code>  <code>property</code>","text":"<p>Return the unique S3 path to the bag for the resource.</p> <p>Since this is a cache, it is stored in a different place than the resource files.</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.bag_url","title":"<code>bag_url</code>  <code>property</code>","text":"<p>Get bag url of resource data bag.</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.can_be_submitted_for_metadata_review","title":"<code>can_be_submitted_for_metadata_review</code>  <code>property</code>","text":"<p>Determine when data and metadata are complete enough for the resource to be published.</p> <p>The property can be overriden by specific resource type which is not appropriate for publication such as the Web App resource :return:</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.discovery_content_type","title":"<code>discovery_content_type</code>  <code>property</code>","text":"<p>Return name used for the content type in discovery/solr search.</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.file_path","title":"<code>file_path</code>  <code>property</code>","text":"<p>Return the file path of the resource.</p> <p>This is the root path plus \"data/contents\". This is the root of the folder structure for resource files.</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.resmap_path","title":"<code>resmap_path</code>  <code>property</code>","text":"<p>path to resource map file (in S3)</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.root_path","title":"<code>root_path</code>  <code>property</code>","text":"<p>Return the root folder of the S3 structure containing resource files.</p> <p>Note that this folder doesn't directly contain the resource files; They are contained in ./data/contents/* instead.</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.scimeta_path","title":"<code>scimeta_path</code>  <code>property</code>","text":"<p>path to science metadata file (in S3)</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.show_in_discover","title":"<code>show_in_discover</code>  <code>property</code>","text":"<p>return True if a resource should be exhibited A resource should be exhibited if it is at least discoverable and not replaced by anything that exists and is at least discoverable.</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.size","title":"<code>size</code>  <code>property</code>","text":"<p>Return the total size of all data files in S3.</p> <p>This size does not include metadata. Just files. Specifically, resourcemetadata.xml, systemmetadata.xml are not included in this size estimate.</p> <p>Raises SessionException if S3 fails.</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.verbose_name","title":"<code>verbose_name</code>  <code>property</code>","text":"<p>Return verbose name of content_model.</p>"},{"location":"hs_core/models/#hs_core.models.BaseResource.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for BaseResource model.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for BaseResource model.\"\"\"\n\n    verbose_name = 'Generic'\n    db_table = 'hs_core_genericresource'\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.can_add","title":"<code>can_add(request)</code>","text":"<p>Pass through to abstract resource can_add function.</p> Source code in <code>hs_core/models.py</code> <pre><code>def can_add(self, request):\n    \"\"\"Pass through to abstract resource can_add function.\"\"\"\n    return AbstractResource.can_add(self, request)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.can_change","title":"<code>can_change(request)</code>","text":"<p>Pass through to abstract resource can_add function.</p> Source code in <code>hs_core/models.py</code> <pre><code>def can_change(self, request):\n    \"\"\"Pass through to abstract resource can_add function.\"\"\"\n    return AbstractResource.can_change(self, request)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.can_delete","title":"<code>can_delete(request)</code>","text":"<p>Pass through to abstract resource can_delete function.</p> Source code in <code>hs_core/models.py</code> <pre><code>def can_delete(self, request):\n    \"\"\"Pass through to abstract resource can_delete function.\"\"\"\n    return AbstractResource.can_delete(self, request)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.can_have_files","title":"<code>can_have_files()</code>  <code>classmethod</code>","text":"<p>Return whether the resource supports files at all.</p> <p>This can be overridden to choose whether files can be uploaded by a subclass.</p> <p>By default, uploads are allowed.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef can_have_files(cls):\n    \"\"\"Return whether the resource supports files at all.\n\n    This can be overridden to choose whether files can be uploaded by a subclass.\n\n    By default, uploads are allowed.\n    \"\"\"\n    # TODO: this should be replaced by an instance method.\n    return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.can_have_multiple_files","title":"<code>can_have_multiple_files()</code>  <code>classmethod</code>","text":"<p>Return True if multiple files can be uploaded.</p> <p>This can be overridden to choose how many files can be uploaded by a subclass.</p> <p>By default, uploads are not limited.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef can_have_multiple_files(cls):\n    \"\"\"Return True if multiple files can be uploaded.\n\n    This can be overridden to choose how many files can be uploaded by a subclass.\n\n    By default, uploads are not limited.\n    \"\"\"\n    # TODO: this should be replaced by an instance method.\n    return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.can_view","title":"<code>can_view(request)</code>","text":"<p>Pass through to abstract resource can_view function.</p> Source code in <code>hs_core/models.py</code> <pre><code>def can_view(self, request):\n    \"\"\"Pass through to abstract resource can_view function.\"\"\"\n    return AbstractResource.can_view(self, request)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.get_datacite_deposit_json","title":"<code>get_datacite_deposit_json(test_mode=False)</code>","text":"<p>Return JSON payload for creating a DOI with DataCite API. Conforms to DataCite REST API for DOI creation: https://support.datacite.org/reference/post_dois</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_datacite_deposit_json(self, test_mode=False):\n    \"\"\"\n    Return JSON payload for creating a DOI with DataCite API.\n    Conforms to DataCite REST API for DOI creation: https://support.datacite.org/reference/post_dois\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    if not self.metadata.title:\n        raise ValidationError(f\"No title found for resource {self.short_id}\")\n    if not self.metadata.creators.count():\n        raise ValidationError(f\"No creators found for resource {self.short_id}\")\n    if not self.metadata.description or not self.metadata.description.abstract:\n        logger.warning(f\"No abstract found for resource {self.short_id}. Using empty string.\")\n        self.metadata.description = type('obj', (), {'abstract': ''})()\n\n    # Add \"test\" to the suffix if in test mode\n    suffix = f\"hs.{self.short_id}-test\" if test_mode else f\"hs.{self.short_id}\"\n    doi = f\"{settings.DATACITE_PREFIX}/{suffix}\"\n\n    payload = {\n        \"data\": {\n            \"type\": \"dois\",\n            \"attributes\": {\n                \"doi\": doi,\n                \"prefix\": f\"{settings.DATACITE_PREFIX}\",\n                \"suffix\": suffix,  # Use modified suffix\n                \"event\": \"publish\",\n                \"url\": None,\n                \"creators\": [],\n                \"titles\": [{\"title\": self.metadata.title.value, \"lang\": \"en\"}],\n                \"publisher\": {\n                    \"name\": \"Consortium of Universities for the Advancement of Hydrologic Science, Inc\",\n                    \"lang\": \"en\",\n                    \"publisherIdentifier\": \"https://ror.org/04s2bx355\",\n                    \"publisherIdentifierScheme\": \"ROR\",\n                    \"schemeUri\": \"https://ror.org\"\n                },\n                \"publicationYear\": None,\n                \"subjects\": [],\n                \"contributors\": [],\n                \"identifiers\": [],\n                \"dates\": [],\n                \"language\": \"en\",\n                \"types\": {\n                    \"resourceTypeGeneral\": \"Dataset\",\n                    \"resourceType\": self.resource_type,\n                    \"schemaOrg\": \"Dataset\",\n                    \"bibtex\": \"misc\",\n                    \"citeproc\": \"dataset\"\n                },\n                # \"relatedIdentifiers\": [],\n                \"relatedItems\": [],\n                \"sizes\": [],\n                \"formats\": [],\n                \"version\": None,\n                \"rightsList\": [],\n                \"descriptions\": [],\n                \"geoLocations\": [],\n                \"fundingReferences\": [],\n                \"container\": {\n                    \"type\": \"DataRepository\",\n                    \"identifier\": \"https://www.hydroshare.org\",\n                    \"identifierType\": \"URL\",\n                    \"title\": \"HydroShare Resources\"\n                },\n                \"contentUrl\": [self.bag_url] if hasattr(self, 'bag_url') and self.bag_url else [],\n                \"schemaVersion\": \"http://datacite.org/schema/kernel-4\",\n                \"state\": \"findable\"\n            },\n            \"relationships\": {\n                \"client\": {\n                    \"data\": {\n                        \"type\": \"repositories\",\n                        \"id\": f\"{settings.DATACITE_USERNAME}\"\n                    }\n                }\n            }\n        }\n    }\n\n    pub_date = self.metadata.dates.filter(type='published').first()\n    payload[\"data\"][\"attributes\"][\"publicationYear\"] = str(\n        pub_date.start_date.year if pub_date else self.updated.year)\n\n    hs_identifier = self.metadata.identifiers.filter(name='hydroShareIdentifier').first()\n    if hs_identifier:\n        payload[\"data\"][\"attributes\"][\"url\"] = hs_identifier.url\n    else:\n        raise ValidationError(f\"No hydroShareIdentifier found for resource {self.short_id}\")\n\n    creators = self.metadata.creators.all()\n    first_creator = [cr for cr in creators if cr.order == 1]\n    other_creators = [cr for cr in creators if cr.order &gt; 1]\n    for creator in [first_creator[0]] + other_creators if first_creator else creators:\n        if creator.name:\n            first_name, last_name = self.parse_creator_name(creator)\n            creator_data = {\n                \"name\": creator.name,\n                \"nameType\": \"Personal\",\n                \"givenName\": first_name,\n                \"familyName\": last_name,\n                \"affiliation\": [{\"name\": creator.organization or \"CUAHSI\"}],\n                \"nameIdentifiers\": []\n            }\n            if creator.hydroshare_user_id:\n                creator_data[\"nameIdentifiers\"].append({\n                    \"nameIdentifier\": f\"https://hydroshare.org{creator.relative_uri}\",\n                    \"nameIdentifierScheme\": \"Other\",\n                })\n            if creator.identifiers.get('ORCID'):\n                creator_data[\"nameIdentifiers\"].append({\n                    \"nameIdentifier\": creator.identifiers['ORCID'],\n                    \"nameIdentifierScheme\": \"ORCID\",\n                    \"schemeUri\": \"https://orcid.org\"\n                })\n            payload[\"data\"][\"attributes\"][\"creators\"].append(creator_data)\n        else:\n            creator_data = {\n                \"name\": creator.organization or \"CUAHSI\",\n                \"nameType\": \"Organizational\",\n                \"nameIdentifiers\": []\n            }\n            payload[\"data\"][\"attributes\"][\"creators\"].append(creator_data)\n\n    payload[\"data\"][\"attributes\"][\"contributors\"] = self.get_contributor_data(self)\n\n    for subject in self.metadata.subjects.all():\n        payload[\"data\"][\"attributes\"][\"subjects\"].append({\n            \"subject\": subject.value,\n        })\n\n    payload[\"data\"][\"attributes\"][\"dates\"] = self.get_dates_for_doi()\n\n    if self.metadata.description and self.metadata.description.abstract:\n        payload[\"data\"][\"attributes\"][\"descriptions\"] = [\n            {\n                \"description\": self.metadata.description.abstract,\n                \"descriptionType\": \"Abstract\",\n                \"lang\": \"en\"\n            }\n        ]\n\n    if self.metadata.rights and self.metadata.rights.url:\n        rights_data = {\n            \"rights\": self.metadata.rights.statement[:2000],\n            \"rightsUri\": self.metadata.rights.url\n        }\n        if pub_date:\n            rights_data[\"rightsUriStartDate\"] = pub_date.start_date.strftime(\"%Y-%m-%d\")\n        payload[\"data\"][\"attributes\"][\"rightsList\"] = [rights_data]\n\n    related_identifiers, related_items = self.get_related_items()\n    # payload[\"data\"][\"attributes\"][\"relatedIdentifiers\"] = related_identifiers\n    payload[\"data\"][\"attributes\"][\"relatedItems\"] = related_items\n\n    for coverage in self.metadata.coverages.all():\n        if coverage.type == 'box':\n            box_values = coverage.value\n            if all(k in box_values for k in ('westlimit', 'eastlimit', 'southlimit', 'northlimit')):\n                payload[\"data\"][\"attributes\"].setdefault(\"geoLocations\", []).append({\n                    \"geoLocationBox\": {\n                        \"westBoundLongitude\": str(box_values['westlimit']),\n                        \"eastBoundLongitude\": str(box_values['eastlimit']),\n                        \"southBoundLatitude\": str(box_values['southlimit']),\n                        \"northBoundLatitude\": str(box_values['northlimit'])\n                    }\n                })\n        elif coverage.type == 'point':\n            point_values = coverage.value\n            longitude = point_values.get('longitude', point_values.get('east'))\n            latitude = point_values.get('latitude', point_values.get('north'))\n\n            if longitude is not None and latitude is not None:\n                geo_location = {\n                    \"geoLocationPoint\": {\n                        \"pointLongitude\": str(longitude),\n                        \"pointLatitude\": str(latitude)\n                    }\n                }\n                if point_values.get('name'):\n                    geo_location[\"geoLocationPlace\"] = point_values['name']\n\n                payload[\"data\"][\"attributes\"].setdefault(\"geoLocations\", []).append(geo_location)\n\n    payload[\"data\"][\"attributes\"][\"fundingReferences\"] = self.get_funding_references(self)\n\n    for format_obj in self.metadata.formats.all():\n        payload[\"data\"][\"attributes\"][\"formats\"].append(format_obj.value)\n\n    all_identifiers = self.metadata.identifiers.all()\n\n    for identifier in all_identifiers:\n        if identifier.name == 'doi':\n            payload[\"data\"][\"attributes\"].setdefault(\"identifiers\", []).append({\n                \"identifier\": identifier.url,\n                \"identifierType\": identifier.name\n            })\n        else:\n            payload[\"data\"][\"attributes\"].setdefault(\"alternateIdentifiers\", []).append({\n                \"alternateIdentifier\": identifier.url,\n                \"alternateIdentifierType\": identifier.name\n            })\n\n    # if self.metadata.citation and hasattr(self.metadata.citation, 'value'):\n    #     payload[\"data\"][\"attributes\"][\"identifiers\"].append({\n    #         \"alternateIdentifier\": self.metadata.citation.value,\n    #         \"alternateIdentifierType\": \"Citation\"\n    #     })\n\n    if hasattr(self, 'size') and self.size:\n        payload[\"data\"][\"attributes\"][\"sizes\"].append(f\"{self.size} bytes\")\n\n    if hasattr(self, 'version') and self.version:\n        payload[\"data\"][\"attributes\"][\"version\"] = str(self.version)\n\n    return json.dumps(payload, indent=2)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.get_dates_for_doi","title":"<code>get_dates_for_doi()</code>","text":"<p>Returns a list of date dictionaries formatted for the DOI creation payload, handling both temporal coverage and metadata dates.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_dates_for_doi(self):\n    \"\"\"\n    Returns a list of date dictionaries formatted for the DOI creation payload,\n    handling both temporal coverage and metadata dates.\n    \"\"\"\n    date_mapping = {\n        'created': 'Created',\n        'modified': 'Updated',\n        'published': 'Accepted',\n    }\n    dates = []\n    # Handling temporal coverage date\n    temporal_dates = self.metadata.coverages.filter(type='period').first()\n    if temporal_dates and temporal_dates.value.get('start') and temporal_dates.value.get('end'):\n        # Format temporal coverage date as a range\n        coverage_date = {\n            \"date\": f\"{temporal_dates.value['start']}/{temporal_dates.value['end']}\",\n            \"dateType\": \"Coverage\"\n        }\n        dates.append(coverage_date)\n\n    # Handling individual metadata dates\n    metadata_dates = self.metadata.dates.all()\n    for date in metadata_dates:\n        if date.type in date_mapping:\n            # Mapping the date types (created, modified, published)\n            mapped_date = {\n                \"date\": date.start_date.strftime(\"%Y-%m-%d\"),\n                \"dateType\": date_mapping[date.type]\n            }\n            dates.append(mapped_date)\n\n    return dates\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.get_funding_references","title":"<code>get_funding_references()</code>  <code>staticmethod</code>","text":"<p>Build fundingReferences list for the DataCite JSON payload.</p> Source code in <code>hs_core/models.py</code> <pre><code>@staticmethod\ndef get_funding_references(self):\n    \"\"\"\n    Build fundingReferences list for the DataCite JSON payload.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    funding_references = []\n\n    def get_funder_id(funder_name):\n        funder_name_lower = funder_name.lower()\n        encoded_funder_name = urllib.parse.quote(funder_name_lower)\n        url = f\"https://api.ror.org/v2/organizations?filter=types:funder&amp;query={encoded_funder_name}\"\n\n        try:\n            response = requests.get(url, verify=False, timeout=10)\n            if response.status_code == 200:\n                items = response.json().get('items', [])\n                for item in items:\n                    for name in item.get('names', []):\n                        if name.get('value', '').lower() == funder_name_lower:\n                            return item.get('id')\n            else:\n                logger.error(f\"Failed to get funder ID for '{funder_name}'. Status: {response.status_code}\")\n        except requests.RequestException as e:\n            logger.error(f\"Error fetching funder ID for '{funder_name}': {str(e)}\")\n\n        return ''\n\n    for funder in self.metadata.funding_agencies.all():\n        funder_data = {\n            \"funderName\": funder.agency_name\n        }\n\n        funder_id = get_funder_id(funder.agency_name)\n        if funder_id:\n            funder_data[\"funderIdentifier\"] = funder_id\n            funder_data[\"funderIdentifierType\"] = \"ROR\"\n        elif funder.agency_url:\n            funder_data[\"funderIdentifier\"] = funder.agency_url\n            funder_data[\"funderIdentifierType\"] = \"Other\"\n\n        if funder.award_number:\n            funder_data[\"awardNumber\"] = funder.award_number\n        if funder.award_title:\n            funder_data[\"awardTitle\"] = funder.award_title\n\n        funding_references.append(funder_data)\n\n    return funding_references\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.get_hs_term_dict","title":"<code>get_hs_term_dict()</code>","text":"<p>Return a dict of HS Terms and their values.</p> <p>Will be used to parse webapp url templates</p> <p>NOTES FOR ANY SUBCLASS OF THIS CLASS TO OVERRIDE THIS FUNCTION: resource types that inherit this class should add/merge their resource-specific HS Terms into this dict</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_hs_term_dict(self):\n    \"\"\"Return a dict of HS Terms and their values.\n\n    Will be used to parse webapp url templates\n\n    NOTES FOR ANY SUBCLASS OF THIS CLASS TO OVERRIDE THIS FUNCTION:\n    resource types that inherit this class should add/merge their resource-specific HS Terms\n    into this dict\n    \"\"\"\n    hs_term_dict = {}\n\n    hs_term_dict[\"HS_RES_ID\"] = self.short_id\n    hs_term_dict[\"HS_RES_TYPE\"] = self.resource_type\n    hs_term_dict.update(self.extra_metadata.items())\n\n    return hs_term_dict\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.get_non_preferred_path_names","title":"<code>get_non_preferred_path_names()</code>","text":"<p>Returns a list of file/folder paths that do not meet hydroshare file/folder preferred naming convention</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_non_preferred_path_names(self):\n    \"\"\"Returns a list of file/folder paths that do not meet hydroshare file/folder preferred naming convention\"\"\"\n\n    def find_non_preferred_folder_paths(dir_path):\n        if not dir_path.startswith(self.file_path):\n            dir_path = os.path.join(self.file_path, dir_path)\n\n        folders, _, _ = istorage.listdir(dir_path)\n        for folder in folders:\n            if folder not in not_preferred_paths and not ResourceFile.check_for_preferred_name(folder):\n                folder_path = os.path.join(dir_path, folder)\n                folder_path = folder_path[len(self.file_path) + 1:]\n                not_preferred_paths.append(folder_path)\n            subdir_path = os.path.join(dir_path, folder)\n            find_non_preferred_folder_paths(subdir_path)\n\n    not_preferred_paths = []\n    istorage = self.get_s3_storage()\n    # check for non-conforming file names\n    for res_file in self.files.all():\n        short_path = res_file.short_path\n        _, file_name = os.path.split(short_path)\n        if not ResourceFile.check_for_preferred_name(file_name):\n            not_preferred_paths.append(short_path)\n\n    # check for non-conforming folder names\n    find_non_preferred_folder_paths(self.file_path)\n    return not_preferred_paths\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.get_related_items","title":"<code>get_related_items()</code>","text":"<p>Returns two lists: - relatedIdentifiers: list of dictionaries for relatedIdentifiers - relatedItems: list of dictionaries for relatedItems</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_related_items(self):\n    \"\"\"\n    Returns two lists:\n    - relatedIdentifiers: list of dictionaries for relatedIdentifiers\n    - relatedItems: list of dictionaries for relatedItems\n    \"\"\"\n    VALID_RELATION_TYPE_MAP = {\n        RelationTypes.isPartOf: \"IsPartOf\",\n        RelationTypes.hasPart: \"HasPart\",\n        RelationTypes.isExecutedBy: \"IsSupplementedBy\",\n        RelationTypes.isCreatedBy: \"IsDocumentedBy\",\n        RelationTypes.isVersionOf: \"IsVersionOf\",\n        RelationTypes.isReplacedBy: \"IsNewVersionOf\",\n        RelationTypes.isDescribedBy: \"IsDescribedBy\",\n        RelationTypes.conformsTo: \"References\",\n        RelationTypes.hasFormat: \"HasPart\",\n        RelationTypes.isFormatOf: \"IsPartOf\",\n        RelationTypes.isRequiredBy: \"IsRequiredBy\",\n        RelationTypes.requires: \"Requires\",\n        RelationTypes.isReferencedBy: \"IsReferencedBy\",\n        RelationTypes.references: \"References\",\n        RelationTypes.replaces: \"Replaces\",\n        RelationTypes.source: \"IsDerivedFrom\",\n        RelationTypes.isSimilarTo: \"IsIdenticalTo\",\n        RelationTypes.relation: \"References\"\n    }\n\n    RESOURCE_TYPE_MAP = {\n        \"CompositeResource\": \"Dataset\",\n        \"CollectionResource\": \"Collection\",\n        \"ToolResource\": \"Software\",\n        \"ModelProgramResource\": \"Software\",\n        \"ModelInstanceResource\": \"Model\",\n        \"GenericResource\": \"Dataset\",\n        \"TimeSeriesResource\": \"Dataset\",\n        \"GeographicFeatureResource\": \"Dataset\",\n        \"GeographicRasterResource\": \"Dataset\",\n        \"MultidimensionalResource\": \"Dataset\",\n        \"ScriptResource\": \"Software\",\n        \"WebAppResource\": \"Service\"\n    }\n\n    related_identifiers = []\n    related_items = []\n\n    for relation in self.metadata.relations.all():\n        match = re.search(r'(https?://\\S+|10\\.\\d{4,9}/\\S+)', relation.value)\n        identifier_value = match.group(0) if match else relation.value\n        identifier_type = \"URL\" if identifier_value.startswith(\"http\") else \"DOI\"\n\n        relation_type = VALID_RELATION_TYPE_MAP.get(relation.type)\n\n        related_identifiers.append({\n            \"relatedIdentifier\": identifier_value,\n            \"relatedIdentifierType\": identifier_type,\n            \"relationType\": relation_type\n        })\n\n        related_items.append({\n            \"relatedItemType\": RESOURCE_TYPE_MAP.get(self.resource_type, \"Other\"),\n            \"relationType\": relation_type,\n            \"relatedItemIdentifier\": {\n                \"relatedItemIdentifierType\": identifier_type,\n                \"relatedItemIdentifier\": identifier_value\n            }\n        })\n\n    return related_identifiers, related_items\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.get_relation_version_res_url","title":"<code>get_relation_version_res_url(rel_type)</code>","text":"<p>Extracts the resource url from resource citation stored in relation metadata for resource versioning :param rel_type: type of relation (allowed types are: 'isVersionOf' and 'isReplacedBy')</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_relation_version_res_url(self, rel_type):\n    \"\"\"Extracts the resource url from resource citation stored in relation metadata for resource\n    versioning\n    :param rel_type: type of relation (allowed types are: 'isVersionOf' and 'isReplacedBy')\n    \"\"\"\n    relation_meta_obj = self.metadata.relations.filter(type=rel_type).first()\n    if relation_meta_obj is not None:\n        # get the resource url from resource citation\n        version_res_url = relation_meta_obj.value.split(',')[-1]\n        return version_res_url\n    else:\n        return ''\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.get_s3_storage","title":"<code>get_s3_storage()</code>","text":"<p>Return S3Storage.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_s3_storage(self):\n    \"\"\"Return S3Storage.\"\"\"\n    return S3Storage()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.get_supported_upload_file_types","title":"<code>get_supported_upload_file_types()</code>  <code>classmethod</code>","text":"<p>Get supported upload types for a resource.</p> <p>This can be overridden to choose which types of file can be uploaded by a subclass.</p> <p>By default, all file types are supported</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef get_supported_upload_file_types(cls):\n    \"\"\"Get supported upload types for a resource.\n\n    This can be overridden to choose which types of file can be uploaded by a subclass.\n\n    By default, all file types are supported\n    \"\"\"\n    # TODO: this should be replaced by an instance method.\n    return ('.*')\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.replaced_by","title":"<code>replaced_by()</code>","text":"<p>return a list or resources that replaced this one</p> Source code in <code>hs_core/models.py</code> <pre><code>def replaced_by(self):\n    \"\"\" return a list or resources that replaced this one \"\"\"\n    from hs_core.hydroshare import get_resource_by_shortkey\n\n    replaced_by_resources = []\n\n    def get_replaced_by(resource):\n        replace_relation_meta = resource.metadata.relations.all().filter(type=RelationTypes.isReplacedBy).first()\n        if replace_relation_meta is not None:\n            version_citation = replace_relation_meta.value\n            if '/resource/' in version_citation:\n                version_res_id = version_citation.split('/resource/')[-1]\n                try:\n                    new_version_res = get_resource_by_shortkey(version_res_id, or_404=False)\n                    replaced_by_resources.append(new_version_res)\n                    get_replaced_by(new_version_res)\n                except BaseResource.DoesNotExist:\n                    pass\n\n    get_replaced_by(self)\n    return replaced_by_resources\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.BaseResource.update_relation_meta","title":"<code>update_relation_meta()</code>","text":"<p>Updates the citation stored in relation metadata for relation type 'isReplacedBy', 'isPartOf' and 'hasPart' if needed</p> Source code in <code>hs_core/models.py</code> <pre><code>def update_relation_meta(self):\n    \"\"\"Updates the citation stored in relation metadata for relation type\n    'isReplacedBy', 'isPartOf' and 'hasPart' if needed\"\"\"\n\n    from hs_core.hydroshare import get_resource_by_shortkey\n\n    def _update_relation_meta(relation_meta_obj):\n        relation_updated = False\n        if relation_meta_obj.value and '/resource/' in relation_meta_obj.value:\n            version_citation = relation_meta_obj.value\n            version_res_id = version_citation.split('/resource/')[-1]\n            try:\n                version_res = get_resource_by_shortkey(version_res_id, or_404=False)\n            except BaseResource.DoesNotExist:\n                relation_meta_obj.delete()\n                relation_updated = True\n                return relation_updated\n            current_version_citation = version_res.get_citation()\n            if current_version_citation != version_citation:\n                relation_meta_obj.value = current_version_citation\n                relation_meta_obj.save()\n                relation_updated = True\n        return relation_updated\n\n    relations = self.metadata.relations.all()\n    replace_relation = [rel for rel in relations if rel.type == RelationTypes.isReplacedBy]\n    replace_relation_updated = False\n    if replace_relation:\n        replace_relation = replace_relation[0]\n        replace_relation_updated = _update_relation_meta(replace_relation)\n\n    part_of_relation_updated = False\n    for part_of_relation in [rel for rel in relations if rel.type == RelationTypes.isPartOf]:\n        if _update_relation_meta(part_of_relation):\n            part_of_relation_updated = True\n\n    has_part_relation_updated = False\n    for has_part_relation in [rel for rel in relations if rel.type == RelationTypes.hasPart]:\n        if _update_relation_meta(has_part_relation):\n            has_part_relation_updated = True\n\n    if any([replace_relation_updated, part_of_relation_updated, has_part_relation_updated]):\n        self.setAVU(\"bag_modified\", True)\n        self.setAVU(\"metadata_dirty\", True)\n        self.refresh_from_db(fields=['cached_metadata'])\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Citation","title":"<code>Citation</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define Citation metadata element model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DCTERMS.bibliographicCitation)\nclass Citation(AbstractMetaDataElement):\n    \"\"\"Define Citation metadata element model.\"\"\"\n\n    term = 'Citation'\n    value = models.TextField()\n\n    def __unicode__(self):\n        \"\"\"Return value field for unicode representation.\"\"\"\n        return self.value\n\n    class Meta:\n        \"\"\"Define meta properties for Citation class.\"\"\"\n\n        unique_together = (\"content_type\", \"object_id\")\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        \"\"\"Call parent update function for Citation class.\"\"\"\n        super(Citation, cls).update(element_id, **kwargs)\n\n    @classmethod\n    def remove(cls, element_id):\n        \"\"\"Call delete function for Citation class.\"\"\"\n        element = cls.objects.get(id=element_id)\n        element.delete()\n\n    def rdf_triples(self, subject, graph):\n        graph.add((subject, self.get_class_term(), Literal(self.value)))\n\n    @classmethod\n    def ingest_rdf(cls, graph, subject, content_object):\n        citation = graph.value(subject=subject, predicate=cls.get_class_term())\n        if citation:\n            Citation.create(value=citation.value, content_object=content_object)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Citation.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Citation class.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Citation class.\"\"\"\n\n    unique_together = (\"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Citation.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return value field for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return value field for unicode representation.\"\"\"\n    return self.value\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Citation.remove","title":"<code>remove(element_id)</code>  <code>classmethod</code>","text":"<p>Call delete function for Citation class.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id):\n    \"\"\"Call delete function for Citation class.\"\"\"\n    element = cls.objects.get(id=element_id)\n    element.delete()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Citation.update","title":"<code>update(element_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Call parent update function for Citation class.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef update(cls, element_id, **kwargs):\n    \"\"\"Call parent update function for Citation class.\"\"\"\n    super(Citation, cls).update(element_id, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Contributor","title":"<code>Contributor</code>","text":"<p>               Bases: <code>Party</code></p> <p>Extend Party model with the term of 'Contributor'.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.contributor)\nclass Contributor(Party):\n    \"\"\"Extend Party model with the term of 'Contributor'.\"\"\"\n\n    term = 'Contributor'\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData","title":"<code>CoreMetaData</code>","text":"<p>               Bases: <code>Model</code>, <code>RDF_MetaData_Mixin</code></p> <p>Define CoreMetaData model.</p> Source code in <code>hs_core/models.py</code> <pre><code>class CoreMetaData(models.Model, RDF_MetaData_Mixin):\n    \"\"\"Define CoreMetaData model.\"\"\"\n\n    XML_HEADER = '''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;'''\n\n    NAMESPACES = {'rdf': \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n                  'rdfs1': \"http://www.w3.org/2000/01/rdf-schema#\",\n                  'dc': \"http://purl.org/dc/elements/1.1/\",\n                  'dcterms': \"http://purl.org/dc/terms/\",\n                  'hsterms': \"https://www.hydroshare.org/terms/\"}\n\n    id = models.AutoField(primary_key=True)\n\n    _description = GenericRelation(Description)    # resource abstract\n    _title = GenericRelation(Title)\n    creators = GenericRelation(Creator)\n    contributors = GenericRelation(Contributor)\n    citation = GenericRelation(Citation)\n    dates = GenericRelation(Date)\n    coverages = GenericRelation(Coverage)\n    formats = GenericRelation(Format)\n    identifiers = GenericRelation(Identifier)\n    _language = GenericRelation(Language)\n    subjects = GenericRelation(Subject)\n    relations = GenericRelation(Relation)\n    geospatialrelations = GenericRelation(GeospatialRelation)\n    _rights = GenericRelation(Rights)\n    _type = GenericRelation(Type)\n    _publisher = GenericRelation(Publisher)\n    funding_agencies = GenericRelation(FundingAgency)\n\n    @property\n    def resource(self):\n        \"\"\"Return base resource object that the metadata defines.\"\"\"\n        return BaseResource.objects.filter(object_id=self.id).first()\n\n    @property\n    def title(self):\n        \"\"\"Return the first title object from metadata.\"\"\"\n        return self._title.all().first()\n\n    @property\n    def description(self):\n        \"\"\"Return the first description object from metadata.\"\"\"\n        return self._description.all().first()\n\n    @property\n    def language(self):\n        \"\"\"Return the first _language object from metadata.\"\"\"\n        return self._language.all().first()\n\n    @property\n    def rights(self):\n        \"\"\"Return the first rights object from metadata.\"\"\"\n        return self._rights.all().first()\n\n    @property\n    def type(self):\n        \"\"\"Return the first _type object from metadata.\"\"\"\n        return self._type.all().first()\n\n    @property\n    def publisher(self):\n        \"\"\"Return the first _publisher object from metadata.\"\"\"\n        return self._publisher.all().first()\n\n    @property\n    def spatial_coverage(self):\n        return self.coverages.exclude(type='period').first()\n\n    @property\n    def temporal_coverage(self):\n        return self.coverages.filter(type='period').first()\n\n    @property\n    def spatial_coverage_default_projection(self):\n        return 'WGS 84 EPSG:4326'\n\n    @property\n    def spatial_coverage_default_units(self):\n        return 'Decimal degrees'\n\n    @property\n    def serializer(self):\n        \"\"\"Return an instance of rest_framework Serializer for self\n        Note: Subclass must override this property\n        \"\"\"\n        from .views.resource_metadata_rest_api import CoreMetaDataSerializer\n        return CoreMetaDataSerializer(self)\n\n    def rdf_subject(self):\n        from .hydroshare import current_site_url\n        return URIRef(\"{}/resource/{}\".format(current_site_url(), self.resource.short_id))\n\n    def rdf_metadata_subject(self):\n        from .hydroshare import current_site_url\n        return URIRef(\"{}/resource/{}/data/resourcemetadata.xml\".format(current_site_url(), self.resource.short_id))\n\n    def rdf_type(self):\n        return getattr(HSTERMS, self.resource.resource_type)\n\n    def ignored_generic_relations(self):\n        \"\"\"Override to exclude generic relations from the rdf/xml.  This is built specifically for Format, which is the\n        only AbstractMetadataElement that is on a metadata model and not included in the rdf/xml.  Returns a list\n        of classes to be ignored\"\"\"\n        return [Format]\n\n    def ingest_metadata(self, graph):\n        super(CoreMetaData, self).ingest_metadata(graph)\n        subject = self.rdf_subject_from_graph(graph)\n        extra_metadata = {}\n        for o in graph.objects(subject=subject, predicate=HSTERMS.extendedMetadata):\n            key = graph.value(subject=o, predicate=HSTERMS.key).value\n            value = graph.value(subject=o, predicate=HSTERMS.value).value\n            extra_metadata[key] = value\n        res = self.resource\n        res.extra_metadata = copy.deepcopy(extra_metadata)\n\n        # delete ingested default citation\n        citation_regex = re.compile(\"(.*) \\(\\d{4}\\)\\. (.*), http:\\/\\/(.*)\\/[A-z0-9]{32}\")  # noqa\n        ingested_citation = self.citation.first()\n        if ingested_citation and citation_regex.match(ingested_citation.value):\n            self.citation.first().delete()\n\n        res.save()\n\n    def get_rdf_graph(self):\n        graph = super(CoreMetaData, self).get_rdf_graph()\n\n        subject = self.rdf_subject()\n\n        # add any key/value metadata items\n        if len(self.resource.extra_metadata) &gt; 0:\n            for key, value in self.resource.extra_metadata.items():\n                extendedMetadata = BNode()\n                graph.add((subject, HSTERMS.extendedMetadata, extendedMetadata))\n                graph.add((extendedMetadata, HSTERMS.key, Literal(key)))\n                graph.add((extendedMetadata, HSTERMS.value, Literal(value)))\n\n        # if custom citation does not exist, use the default citation\n        if not self.citation.first():\n            graph.add((subject, DCTERMS.bibliographicCitation, Literal(\n                self.resource.get_citation(forceHydroshareURI=False))))\n\n        from .hydroshare import current_site_url\n        TYPE_SUBJECT = URIRef(\"{}/terms/{}\".format(current_site_url(), self.resource.resource_type))\n        graph.add((TYPE_SUBJECT, RDFS1.label, Literal(self.resource.verbose_name)))\n        graph.add((TYPE_SUBJECT, RDFS1.isDefinedBy, URIRef(HSTERMS)))\n        return graph\n\n    @classmethod\n    def parse_for_bulk_update(cls, metadata, parsed_metadata):\n        \"\"\"Parse the input *metadata* dict to needed format and store it in\n        *parsed_metadata* list\n        :param  metadata: a dict of metadata that needs to be parsed to get the metadata in the\n        format needed for updating the metadata elements supported by resource type\n        :param  parsed_metadata: a list of dicts that will be appended with parsed data\n        \"\"\"\n\n        keys_to_update = list(metadata.keys())\n        if 'title' in keys_to_update:\n            parsed_metadata.append({\"title\": {\"value\": metadata.pop('title')}})\n\n        if 'creators' in keys_to_update:\n            if not isinstance(metadata['creators'], list):\n                metadata['creators'] = json.loads(metadata['creators'])\n            for creator in metadata.pop('creators'):\n                parsed_metadata.append({\"creator\": creator})\n\n        if 'contributors' in keys_to_update:\n            if not isinstance(metadata['contributors'], list):\n                metadata['contributors'] = json.loads(metadata['contributors'])\n            for contributor in metadata.pop('contributors'):\n                parsed_metadata.append({\"contributor\": contributor})\n\n        if 'coverages' in keys_to_update:\n            for coverage in metadata.pop('coverages'):\n                parsed_metadata.append({\"coverage\": coverage})\n\n        if 'dates' in keys_to_update:\n            for date in metadata.pop('dates'):\n                parsed_metadata.append({\"date\": date})\n\n        if 'description' in keys_to_update:\n            parsed_metadata.append({\"description\": {\"abstract\": metadata.pop('description')}})\n\n        if 'language' in keys_to_update:\n            parsed_metadata.append({\"language\": {\"code\": metadata.pop('language')}})\n\n        if 'rights' in keys_to_update:\n            parsed_metadata.append({\"rights\": metadata.pop('rights')})\n\n        if 'sources' in keys_to_update:\n            for source in metadata.pop('sources'):\n                parsed_metadata.append({\"source\": source})\n\n        if 'subjects' in keys_to_update:\n            for subject in metadata.pop('subjects'):\n                parsed_metadata.append({\"subject\": {\"value\": subject['value']}})\n\n        if 'funding_agencies' in keys_to_update:\n            for agency in metadata.pop(\"funding_agencies\"):\n                # using fundingagency instead of funding_agency to be consistent with UI\n                # add-metadata logic as well as the term for the metadata element.\n                parsed_metadata.append({\"fundingagency\": agency})\n\n        if 'relations' in keys_to_update:\n            for relation in metadata.pop('relations'):\n                parsed_metadata.append({\"relation\": relation})\n\n        if 'geospatialrelations' in keys_to_update:\n            for relation in metadata.pop('geospatialrelations'):\n                parsed_metadata.append({\"geospatialrelation\": relation})\n\n    @classmethod\n    def get_supported_element_names(cls):\n        \"\"\"Return a list of supported metadata element names.\"\"\"\n        return ['Description',\n                'Citation',\n                'Creator',\n                'Contributor',\n                'Coverage',\n                'Format',\n                'Rights',\n                'Title',\n                'Type',\n                'Date',\n                'Identifier',\n                'Language',\n                'Subject',\n                'Relation',\n                'GeospatialRelation',\n                'Publisher',\n                'FundingAgency']\n\n    @classmethod\n    def get_form_errors_as_string(cls, form):\n        \"\"\"Helper method to generate a string from form.errors\n        :param  form: an instance of Django Form class\n        \"\"\"\n        error_string = \", \".join(key + \":\" + form.errors[key][0]\n                                 for key in list(form.errors.keys()))\n        return error_string\n\n    def set_dirty(self, flag):\n        \"\"\"Track whethrer metadata object is dirty.\n\n        Subclasses that have the attribute to track whether metadata object is dirty\n        should override this method to allow setting that attribute\n\n        :param flag: a boolean value\n        :return:\n        \"\"\"\n        pass\n\n    def has_all_required_elements(self):\n        \"\"\"Determine whether metadata has all required elements.\n\n        This method needs to be overriden by any subclass of this class\n        if they implement additional metadata elements that are required\n        \"\"\"\n        resource = self.resource\n        if not resource.cached_metadata.get('title', {}):\n            return False\n        elif resource.cached_metadata['title']['value'].lower() == 'untitled resource':\n            return False\n\n        if not resource.cached_metadata.get('abstract', {}):\n            return False\n        elif len(resource.cached_metadata['abstract']['value'].strip()) == 0:\n            return False\n\n        if not resource.cached_metadata.get('creators', []):\n            return False\n\n        if not resource.cached_metadata.get('rights', {}):\n            return False\n        elif len(resource.cached_metadata['rights']['statement'].strip()) == 0:\n            return False\n\n        if not resource.cached_metadata.get('subjects', []):\n            return False\n\n        return True\n\n    def get_required_missing_elements(self, desired_state='discoverable'):\n        \"\"\"Return a list of required missing metadata elements.\n\n        This method needs to be overriden by any subclass of this class\n        if they implement additional metadata elements that are required\n        \"\"\"\n\n        resource_states = ('discoverable', 'public', 'published')\n        if desired_state not in resource_states:\n            raise ValidationError(f\"Desired resource state is not in: {','.join(resource_states)}\")\n\n        resource = self.resource\n        missing_required_elements = []\n        if desired_state != 'published':\n            # check for title, abstract, rights, and keywords using cached_metadata\n            if not resource.cached_metadata.get('title', {}):\n                missing_required_elements.append('Title (at least 30 characters)')\n            elif resource.cached_metadata['title']['value'].lower() == 'untitled resource':\n                missing_required_elements.append('Title (at least 30 characters)')\n            if not resource.cached_metadata.get('abstract', {}):\n                missing_required_elements.append('Abstract (at least 150 characters)')\n            if not resource.cached_metadata.get('rights', {}):\n                missing_required_elements.append('Rights')\n            if not resource.cached_metadata.get('subjects', []):\n                missing_required_elements.append('Keywords (at least 3)')\n        else:\n            # check for title, abstract, and keywords using cached_metadata\n            cached_metadata = resource.cached_metadata\n            if not cached_metadata.get('title', {}) or len(cached_metadata['title']['value']) &lt; 30:\n                missing_required_elements.append('The title must be at least 30 characters.')\n            if not cached_metadata.get('abstract', {}) or len(cached_metadata['abstract']['value']) &lt; 150:\n                missing_required_elements.append('The abstract must be at least 150 characters.')\n            if not cached_metadata.get('subjects', []) or len(cached_metadata['subjects']) &lt; 3:\n                missing_required_elements.append('You must include at least 3 keywords.')\n\n        return missing_required_elements\n\n    def get_recommended_missing_elements(self):\n        \"\"\"Return a list of recommended missing metadata elements.\n\n        This method needs to be overriden by any subclass of this class\n        if they implement additional metadata elements that are required\n        \"\"\"\n\n        missing_recommended_elements = []\n        resource = self.resource\n        if not resource.cached_metadata.get('funding_agencies', []):\n            missing_recommended_elements.append('Funding Agency')\n\n        if not resource.readme_file and resource.resource_type == \"CompositeResource\":\n            missing_recommended_elements.append('Readme file containing variables, '\n                                                'abbreviations/acronyms, and non-standard file formats')\n        if not resource.cached_metadata.get('spatial_coverage', {}) and \\\n                not resource.cached_metadata.get('temporal_coverage', {}):\n            missing_recommended_elements.append('Coverage that describes locations that are related to the dataset')\n\n        return missing_recommended_elements\n\n    def delete_all_elements(self):\n        \"\"\"Delete all metadata elements.\n\n        This method needs to be overriden by any subclass of this class if that class\n        has additional metadata elements\n        \"\"\"\n        if self.title:\n            self.title.delete()\n        if self.description:\n            self.description.delete()\n        if self.language:\n            self.language.delete()\n        if self.rights:\n            self.rights.delete()\n        if self.publisher:\n            self.publisher.delete()\n        if self.type:\n            self.type.delete()\n\n        self.creators.all().delete()\n        self.contributors.all().delete()\n        self.dates.all().delete()\n        self.identifiers.all().delete()\n        self.coverages.all().delete()\n        self.formats.all().delete()\n        self.subjects.all().delete()\n        self.relations.all().delete()\n        self.funding_agencies.all().delete()\n\n    def copy_all_elements_from(self, src_md, exclude_elements=None):\n        \"\"\"Copy all metadata elements from another resource.\"\"\"\n        logger = logging.getLogger(__name__)\n        md_type = ContentType.objects.get_for_model(src_md)\n        supported_element_names = src_md.get_supported_element_names()\n        for element_name in supported_element_names:\n            element_model_type = src_md._get_metadata_element_model_type(element_name)\n            elements_to_copy = element_model_type.model_class().objects.filter(\n                object_id=src_md.id, content_type=md_type).all()\n            for element in elements_to_copy:\n                element_args = model_to_dict(element)\n                element_args.pop('content_type')\n                element_args.pop('id')\n                element_args.pop('object_id')\n                try:\n                    if exclude_elements:\n                        if not element_name.lower() in exclude_elements:\n                            self.create_element(element_name, **element_args)\n                    else:\n                        self.create_element(element_name, **element_args)\n                except UserValidationError as uve:\n                    logger.error(f\"Error copying {element}: {str(uve)}\")\n                    element_args[\"hydroshare_user_id\"] = None\n                    del element_args[\"is_active_user\"]\n                    self.create_element(element_name, **element_args)\n\n    # this method needs to be overriden by any subclass of this class\n    # to allow updating of extended (resource specific) metadata\n    def update(self, metadata, user):\n        \"\"\"Define custom update method for CoreMetaData model.\n\n        :param metadata: a list of dicts - each dict in the format of {element_name: **kwargs}\n        element_name must be in lowercase.\n        example of a dict in metadata list:\n            {'creator': {'name': 'John Howard', 'email: 'jh@gmail.com'}}\n        :param  user: user who is updating metadata\n        :return:\n        \"\"\"\n        from .forms import (AbstractValidationForm, ContributorValidationForm,\n                            CreatorValidationForm, FundingAgencyValidationForm,\n                            GeospatialRelationValidationForm,\n                            LanguageValidationForm, RelationValidationForm,\n                            RightsValidationForm, TitleValidationForm)\n\n        validation_forms_mapping = {'title': TitleValidationForm,\n                                    'description': AbstractValidationForm,\n                                    'language': LanguageValidationForm,\n                                    'rights': RightsValidationForm,\n                                    'creator': CreatorValidationForm,\n                                    'contributor': ContributorValidationForm,\n                                    'relation': RelationValidationForm,\n                                    'geospatialrelation': GeospatialRelationValidationForm,\n                                    'fundingagency': FundingAgencyValidationForm\n                                    }\n        # updating non-repeatable elements\n        with transaction.atomic():\n            for element_name in ('title', 'description', 'language', 'rights'):\n                for dict_item in metadata:\n                    if element_name in dict_item:\n                        validation_form = validation_forms_mapping[element_name](\n                            dict_item[element_name])\n                        if not validation_form.is_valid():\n                            err_string = self.get_form_errors_as_string(validation_form)\n                            raise ValidationError(err_string)\n                self.update_non_repeatable_element(element_name, metadata)\n            for element_name in ('creator', 'contributor', 'coverage', 'source', 'relation',\n                                 'geospatialrelation', 'subject'):\n                subjects = []\n                for dict_item in metadata:\n                    if element_name in dict_item:\n                        if element_name == 'subject':\n                            subject_data = dict_item['subject']\n                            if 'value' not in subject_data:\n                                raise ValidationError(\"Subject value is missing\")\n                            subjects.append(dict_item['subject']['value'])\n                            continue\n                        if element_name == 'coverage':\n                            coverage_data = dict_item[element_name]\n                            if 'type' not in coverage_data:\n                                raise ValidationError(\"Coverage type data is missing\")\n                            if 'value' not in coverage_data:\n                                raise ValidationError(\"Coverage value data is missing\")\n                            coverage_value_dict = coverage_data['value']\n                            coverage_type = coverage_data['type']\n                            Coverage.validate_coverage_type_value_attributes(coverage_type,\n                                                                             coverage_value_dict)\n                            continue\n                        if element_name in ['creator', 'contributor']:\n                            try:\n                                party_data = dict_item[element_name]\n                                if 'identifiers' in party_data:\n                                    if isinstance(party_data['identifiers'], dict):\n                                        # convert dict to json for form validation\n                                        party_data['identifiers'] = json.dumps(\n                                            party_data['identifiers'])\n                            except Exception:\n                                raise ValidationError(\"Invalid identifier data for \"\n                                                      \"creator/contributor\")\n                            validation_form = validation_forms_mapping[element_name](\n                                party_data)\n                        else:\n                            validation_form = validation_forms_mapping[element_name](\n                                dict_item[element_name])\n\n                        if not validation_form.is_valid():\n                            err_string = self.get_form_errors_as_string(validation_form)\n                            err_string += \" element name:{}\".format(element_name)\n                            raise ValidationError(err_string)\n                if subjects:\n                    subjects_set = set([s.lower() for s in subjects])\n                    if len(subjects_set) &lt; len(subjects):\n                        raise ValidationError(\"Duplicate subject values found\")\n                self.update_repeatable_element(element_name=element_name, metadata=metadata)\n\n            # allow only updating or creating date element of type valid\n            element_name = 'date'\n            date_list = [date_dict for date_dict in metadata if element_name in date_dict]\n            if len(date_list) &gt; 0:\n                for date_item in date_list:\n                    if 'type' in date_item[element_name]:\n                        if date_item[element_name]['type'] == 'valid':\n                            self.dates.filter(type='valid').delete()\n                            self.create_element(element_model_name=element_name,\n                                                **date_item[element_name])\n                            break\n\n            # allow only updating or creating identifiers which does not have name value\n            # 'hydroShareIdentifier'\n            element_name = 'identifier'\n            identifier_list = [id_dict for id_dict in metadata if element_name in id_dict]\n            if len(identifier_list) &gt; 0:\n                for id_item in identifier_list:\n                    if 'name' in id_item[element_name]:\n                        if id_item[element_name]['name'].lower() != 'hydroshareidentifier':\n                            self.identifiers.filter(name=id_item[element_name]['name']).delete()\n                            self.create_element(element_model_name=element_name,\n                                                **id_item[element_name])\n\n            element_name = 'fundingagency'\n            identifier_list = [id_dict for id_dict in metadata if element_name in id_dict]\n            if len(identifier_list) &gt; 0:\n                for id_item in identifier_list:\n                    validation_form = validation_forms_mapping[element_name](\n                        id_item[element_name])\n                    if not validation_form.is_valid():\n                        err_string = self.get_form_errors_as_string(validation_form)\n                        raise ValidationError(err_string)\n                # update_repeatable_elements will append an 's' to element_name before getattr,\n                # unless property_name is provided.  I'd like to remove English grammar rules from\n                # our codebase, but in the interest of time, I'll just add a special case for\n                # handling funding_agencies\n                self.update_repeatable_element(element_name=element_name, metadata=metadata,\n                                               property_name=\"funding_agencies\")\n\n    @property\n    def resource_uri(self):\n        return self.identifiers.all().filter(name='hydroShareIdentifier')[0].url\n\n    def create_element(self, element_model_name, **kwargs):\n        \"\"\"Create any supported metadata element.\"\"\"\n        model_type = self._get_metadata_element_model_type(element_model_name)\n        kwargs['content_object'] = self\n        element_model_name = element_model_name.lower()\n        resource = self.resource\n        if resource.raccess.published:\n            if element_model_name == 'creator':\n                raise ValidationError(\"{} can't be created for a published resource\".format(element_model_name))\n            elif element_model_name == 'identifier':\n                name_value = kwargs.get('name', '')\n                if name_value != 'doi':\n                    # for published resource the 'name' attribute of the identifier must be set to 'doi'\n                    raise ValidationError(\"For a published resource only a doi identifier can be created\")\n            elif element_model_name == 'date':\n                date_type = kwargs.get('type', '')\n                if date_type and date_type not in ('modified', 'published'):\n                    raise ValidationError(\"{} date can't be created for a published resource\".format(date_type))\n        element = model_type.model_class().create(**kwargs)\n\n        return element\n\n    def update_element(self, element_model_name, element_id, **kwargs):\n        \"\"\"Update metadata element.\"\"\"\n        model_type = self._get_metadata_element_model_type(element_model_name)\n        kwargs['content_object'] = self\n        element_model_name = element_model_name.lower()\n        resource = self.resource\n        if resource.raccess.published:\n            if element_model_name in ('title', 'creator', 'rights', 'identifier', 'format', 'publisher'):\n                raise ValidationError(\"{} can't be updated for a published resource\".format(element_model_name))\n            elif element_model_name == 'date':\n                date_type = kwargs.get('type', '')\n                if date_type and date_type != 'modified':\n                    raise ValidationError(\"{} date can't be updated for a published resource\".format(date_type))\n        model_type.model_class().update(element_id, **kwargs)\n\n    def delete_element(self, element_model_name, element_id):\n        \"\"\"Delete Metadata element.\"\"\"\n        model_type = self._get_metadata_element_model_type(element_model_name)\n        element_model_name = element_model_name.lower()\n        resource = self.resource\n        if resource.raccess.published:\n            if element_model_name not in ('subject', 'contributor', 'source', 'relation', 'fundingagency', 'format'):\n                raise ValidationError(\"{} can't be deleted for a published resource\".format(element_model_name))\n        model_type.model_class().remove(element_id)\n\n    def _get_metadata_element_model_type(self, element_model_name):\n        \"\"\"Get type of metadata element based on model type.\"\"\"\n        element_model_name = element_model_name.lower()\n        if not self._is_valid_element(element_model_name):\n            raise ValidationError(\"Metadata element type:%s is not one of the \"\n                                  \"supported in core metadata elements.\"\n                                  % element_model_name)\n\n        unsupported_element_error = \"Metadata element type:%s is not supported.\" \\\n                                    % element_model_name\n        try:\n            model_type = ContentType.objects.get(app_label=self._meta.app_label,\n                                                 model=element_model_name)\n        except ObjectDoesNotExist:\n            try:\n                model_type = ContentType.objects.get(app_label='hs_core',\n                                                     model=element_model_name)\n            except ObjectDoesNotExist:\n                raise ValidationError(unsupported_element_error)\n\n        if not issubclass(model_type.model_class(), AbstractMetaDataElement):\n            raise ValidationError(unsupported_element_error)\n\n        return model_type\n\n    def _is_valid_element(self, element_name):\n        \"\"\"Check whether metadata element is valid.\"\"\"\n        allowed_elements = [el.lower() for el in self.get_supported_element_names()]\n        return element_name.lower() in allowed_elements\n\n    def update_non_repeatable_element(self, element_name, metadata, property_name=None):\n        \"\"\"Update a non-repeatable metadata element.\n\n        This helper function is to create/update a specific metadata element as specified by\n        *element_name*\n        :param element_name: metadata element class name (e.g. title)\n        :param metadata: a list of dicts - each dict has data to update/create a specific metadata\n        element (e.g. {'title': {'value': 'my resource title'}}\n        :param property_name: name of the property/attribute name in this class or its sub class\n        to access the metadata element instance of *metadata_element*. This is needed only when\n        the property/attribute name differs from the element class name\n\n            Example:\n            class ModelProgramMetaData(CoreMetaData):\n                _mpmetadata = GenericRelation(MpMetadata)\n\n                @property\n                def program(self):\n                    return self._mpmetadata.all().first()\n\n            For the above class to update the metadata element MpMetadata, this function needs to\n            be called with element_name='mpmetadata' and property_name='program'\n        :return:\n        \"\"\"\n        for dict_item in metadata:\n            if element_name in dict_item:\n                if property_name is None:\n                    element = getattr(self, element_name, None)\n                else:\n                    element = getattr(self, property_name, None)\n                if element:\n                    self.update_element(element_id=element.id,\n                                        element_model_name=element_name,\n                                        **dict_item[element_name])\n                else:\n                    self.create_element(element_model_name=element_name,\n                                        **dict_item[element_name])\n\n    def update_repeatable_element(self, element_name, metadata, property_name=None):\n        \"\"\"Update a repeatable metadata element.\n\n        Creates new metadata elements of type *element_name*. Any existing metadata elements of\n        matching type get deleted first.\n        :param element_name: class name of the metadata element (e.g. creator)\n        :param metadata: a list of dicts containing data for each of the metadata elements that\n        needs to be created/updated as part of bulk update\n        :param property_name: (Optional) the property/attribute name used in this instance of\n        CoreMetaData (or its sub class) to access all the objects of type *element_type*\n            Example:\n            class MODFLOWModelInstanceMetaData(ModelInstanceMetaData):\n                 _model_input = GenericRelation(ModelInput)\n\n                @property\n                def model_inputs(self):\n                    return self._model_input.all()\n\n            For the above class to update the metadata element ModelInput, this function needs to\n            be called with element_name='modelinput' and property_name='model_inputs'. If in the\n            above class instead of using the attribute name '_model_inputs' we have used\n            'modelinputs' then this function needs to be called with element_name='modelinput' and\n            no need to pass a value for the property_name.\n\n        :return:\n        \"\"\"\n        element_list = [element_dict for element_dict in metadata if element_name in element_dict]\n        if len(element_list) &gt; 0:\n            if property_name is None:\n                elements = getattr(self, element_name + 's')\n            else:\n                elements = getattr(self, property_name)\n\n            elements.all().delete()\n            for element in element_list:\n                self.create_element(element_model_name=element_name, **element[element_name])\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.description","title":"<code>description</code>  <code>property</code>","text":"<p>Return the first description object from metadata.</p>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.language","title":"<code>language</code>  <code>property</code>","text":"<p>Return the first _language object from metadata.</p>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.publisher","title":"<code>publisher</code>  <code>property</code>","text":"<p>Return the first _publisher object from metadata.</p>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.resource","title":"<code>resource</code>  <code>property</code>","text":"<p>Return base resource object that the metadata defines.</p>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.rights","title":"<code>rights</code>  <code>property</code>","text":"<p>Return the first rights object from metadata.</p>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.serializer","title":"<code>serializer</code>  <code>property</code>","text":"<p>Return an instance of rest_framework Serializer for self Note: Subclass must override this property</p>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.title","title":"<code>title</code>  <code>property</code>","text":"<p>Return the first title object from metadata.</p>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.type","title":"<code>type</code>  <code>property</code>","text":"<p>Return the first _type object from metadata.</p>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.copy_all_elements_from","title":"<code>copy_all_elements_from(src_md, exclude_elements=None)</code>","text":"<p>Copy all metadata elements from another resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>def copy_all_elements_from(self, src_md, exclude_elements=None):\n    \"\"\"Copy all metadata elements from another resource.\"\"\"\n    logger = logging.getLogger(__name__)\n    md_type = ContentType.objects.get_for_model(src_md)\n    supported_element_names = src_md.get_supported_element_names()\n    for element_name in supported_element_names:\n        element_model_type = src_md._get_metadata_element_model_type(element_name)\n        elements_to_copy = element_model_type.model_class().objects.filter(\n            object_id=src_md.id, content_type=md_type).all()\n        for element in elements_to_copy:\n            element_args = model_to_dict(element)\n            element_args.pop('content_type')\n            element_args.pop('id')\n            element_args.pop('object_id')\n            try:\n                if exclude_elements:\n                    if not element_name.lower() in exclude_elements:\n                        self.create_element(element_name, **element_args)\n                else:\n                    self.create_element(element_name, **element_args)\n            except UserValidationError as uve:\n                logger.error(f\"Error copying {element}: {str(uve)}\")\n                element_args[\"hydroshare_user_id\"] = None\n                del element_args[\"is_active_user\"]\n                self.create_element(element_name, **element_args)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.create_element","title":"<code>create_element(element_model_name, **kwargs)</code>","text":"<p>Create any supported metadata element.</p> Source code in <code>hs_core/models.py</code> <pre><code>def create_element(self, element_model_name, **kwargs):\n    \"\"\"Create any supported metadata element.\"\"\"\n    model_type = self._get_metadata_element_model_type(element_model_name)\n    kwargs['content_object'] = self\n    element_model_name = element_model_name.lower()\n    resource = self.resource\n    if resource.raccess.published:\n        if element_model_name == 'creator':\n            raise ValidationError(\"{} can't be created for a published resource\".format(element_model_name))\n        elif element_model_name == 'identifier':\n            name_value = kwargs.get('name', '')\n            if name_value != 'doi':\n                # for published resource the 'name' attribute of the identifier must be set to 'doi'\n                raise ValidationError(\"For a published resource only a doi identifier can be created\")\n        elif element_model_name == 'date':\n            date_type = kwargs.get('type', '')\n            if date_type and date_type not in ('modified', 'published'):\n                raise ValidationError(\"{} date can't be created for a published resource\".format(date_type))\n    element = model_type.model_class().create(**kwargs)\n\n    return element\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.delete_all_elements","title":"<code>delete_all_elements()</code>","text":"<p>Delete all metadata elements.</p> <p>This method needs to be overriden by any subclass of this class if that class has additional metadata elements</p> Source code in <code>hs_core/models.py</code> <pre><code>def delete_all_elements(self):\n    \"\"\"Delete all metadata elements.\n\n    This method needs to be overriden by any subclass of this class if that class\n    has additional metadata elements\n    \"\"\"\n    if self.title:\n        self.title.delete()\n    if self.description:\n        self.description.delete()\n    if self.language:\n        self.language.delete()\n    if self.rights:\n        self.rights.delete()\n    if self.publisher:\n        self.publisher.delete()\n    if self.type:\n        self.type.delete()\n\n    self.creators.all().delete()\n    self.contributors.all().delete()\n    self.dates.all().delete()\n    self.identifiers.all().delete()\n    self.coverages.all().delete()\n    self.formats.all().delete()\n    self.subjects.all().delete()\n    self.relations.all().delete()\n    self.funding_agencies.all().delete()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.delete_element","title":"<code>delete_element(element_model_name, element_id)</code>","text":"<p>Delete Metadata element.</p> Source code in <code>hs_core/models.py</code> <pre><code>def delete_element(self, element_model_name, element_id):\n    \"\"\"Delete Metadata element.\"\"\"\n    model_type = self._get_metadata_element_model_type(element_model_name)\n    element_model_name = element_model_name.lower()\n    resource = self.resource\n    if resource.raccess.published:\n        if element_model_name not in ('subject', 'contributor', 'source', 'relation', 'fundingagency', 'format'):\n            raise ValidationError(\"{} can't be deleted for a published resource\".format(element_model_name))\n    model_type.model_class().remove(element_id)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.get_form_errors_as_string","title":"<code>get_form_errors_as_string(form)</code>  <code>classmethod</code>","text":"<p>Helper method to generate a string from form.errors :param  form: an instance of Django Form class</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef get_form_errors_as_string(cls, form):\n    \"\"\"Helper method to generate a string from form.errors\n    :param  form: an instance of Django Form class\n    \"\"\"\n    error_string = \", \".join(key + \":\" + form.errors[key][0]\n                             for key in list(form.errors.keys()))\n    return error_string\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.get_recommended_missing_elements","title":"<code>get_recommended_missing_elements()</code>","text":"<p>Return a list of recommended missing metadata elements.</p> <p>This method needs to be overriden by any subclass of this class if they implement additional metadata elements that are required</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_recommended_missing_elements(self):\n    \"\"\"Return a list of recommended missing metadata elements.\n\n    This method needs to be overriden by any subclass of this class\n    if they implement additional metadata elements that are required\n    \"\"\"\n\n    missing_recommended_elements = []\n    resource = self.resource\n    if not resource.cached_metadata.get('funding_agencies', []):\n        missing_recommended_elements.append('Funding Agency')\n\n    if not resource.readme_file and resource.resource_type == \"CompositeResource\":\n        missing_recommended_elements.append('Readme file containing variables, '\n                                            'abbreviations/acronyms, and non-standard file formats')\n    if not resource.cached_metadata.get('spatial_coverage', {}) and \\\n            not resource.cached_metadata.get('temporal_coverage', {}):\n        missing_recommended_elements.append('Coverage that describes locations that are related to the dataset')\n\n    return missing_recommended_elements\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.get_required_missing_elements","title":"<code>get_required_missing_elements(desired_state='discoverable')</code>","text":"<p>Return a list of required missing metadata elements.</p> <p>This method needs to be overriden by any subclass of this class if they implement additional metadata elements that are required</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_required_missing_elements(self, desired_state='discoverable'):\n    \"\"\"Return a list of required missing metadata elements.\n\n    This method needs to be overriden by any subclass of this class\n    if they implement additional metadata elements that are required\n    \"\"\"\n\n    resource_states = ('discoverable', 'public', 'published')\n    if desired_state not in resource_states:\n        raise ValidationError(f\"Desired resource state is not in: {','.join(resource_states)}\")\n\n    resource = self.resource\n    missing_required_elements = []\n    if desired_state != 'published':\n        # check for title, abstract, rights, and keywords using cached_metadata\n        if not resource.cached_metadata.get('title', {}):\n            missing_required_elements.append('Title (at least 30 characters)')\n        elif resource.cached_metadata['title']['value'].lower() == 'untitled resource':\n            missing_required_elements.append('Title (at least 30 characters)')\n        if not resource.cached_metadata.get('abstract', {}):\n            missing_required_elements.append('Abstract (at least 150 characters)')\n        if not resource.cached_metadata.get('rights', {}):\n            missing_required_elements.append('Rights')\n        if not resource.cached_metadata.get('subjects', []):\n            missing_required_elements.append('Keywords (at least 3)')\n    else:\n        # check for title, abstract, and keywords using cached_metadata\n        cached_metadata = resource.cached_metadata\n        if not cached_metadata.get('title', {}) or len(cached_metadata['title']['value']) &lt; 30:\n            missing_required_elements.append('The title must be at least 30 characters.')\n        if not cached_metadata.get('abstract', {}) or len(cached_metadata['abstract']['value']) &lt; 150:\n            missing_required_elements.append('The abstract must be at least 150 characters.')\n        if not cached_metadata.get('subjects', []) or len(cached_metadata['subjects']) &lt; 3:\n            missing_required_elements.append('You must include at least 3 keywords.')\n\n    return missing_required_elements\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.get_supported_element_names","title":"<code>get_supported_element_names()</code>  <code>classmethod</code>","text":"<p>Return a list of supported metadata element names.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef get_supported_element_names(cls):\n    \"\"\"Return a list of supported metadata element names.\"\"\"\n    return ['Description',\n            'Citation',\n            'Creator',\n            'Contributor',\n            'Coverage',\n            'Format',\n            'Rights',\n            'Title',\n            'Type',\n            'Date',\n            'Identifier',\n            'Language',\n            'Subject',\n            'Relation',\n            'GeospatialRelation',\n            'Publisher',\n            'FundingAgency']\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.has_all_required_elements","title":"<code>has_all_required_elements()</code>","text":"<p>Determine whether metadata has all required elements.</p> <p>This method needs to be overriden by any subclass of this class if they implement additional metadata elements that are required</p> Source code in <code>hs_core/models.py</code> <pre><code>def has_all_required_elements(self):\n    \"\"\"Determine whether metadata has all required elements.\n\n    This method needs to be overriden by any subclass of this class\n    if they implement additional metadata elements that are required\n    \"\"\"\n    resource = self.resource\n    if not resource.cached_metadata.get('title', {}):\n        return False\n    elif resource.cached_metadata['title']['value'].lower() == 'untitled resource':\n        return False\n\n    if not resource.cached_metadata.get('abstract', {}):\n        return False\n    elif len(resource.cached_metadata['abstract']['value'].strip()) == 0:\n        return False\n\n    if not resource.cached_metadata.get('creators', []):\n        return False\n\n    if not resource.cached_metadata.get('rights', {}):\n        return False\n    elif len(resource.cached_metadata['rights']['statement'].strip()) == 0:\n        return False\n\n    if not resource.cached_metadata.get('subjects', []):\n        return False\n\n    return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.ignored_generic_relations","title":"<code>ignored_generic_relations()</code>","text":"<p>Override to exclude generic relations from the rdf/xml.  This is built specifically for Format, which is the only AbstractMetadataElement that is on a metadata model and not included in the rdf/xml.  Returns a list of classes to be ignored</p> Source code in <code>hs_core/models.py</code> <pre><code>def ignored_generic_relations(self):\n    \"\"\"Override to exclude generic relations from the rdf/xml.  This is built specifically for Format, which is the\n    only AbstractMetadataElement that is on a metadata model and not included in the rdf/xml.  Returns a list\n    of classes to be ignored\"\"\"\n    return [Format]\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.parse_for_bulk_update","title":"<code>parse_for_bulk_update(metadata, parsed_metadata)</code>  <code>classmethod</code>","text":"<p>Parse the input metadata dict to needed format and store it in parsed_metadata list :param  metadata: a dict of metadata that needs to be parsed to get the metadata in the format needed for updating the metadata elements supported by resource type :param  parsed_metadata: a list of dicts that will be appended with parsed data</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef parse_for_bulk_update(cls, metadata, parsed_metadata):\n    \"\"\"Parse the input *metadata* dict to needed format and store it in\n    *parsed_metadata* list\n    :param  metadata: a dict of metadata that needs to be parsed to get the metadata in the\n    format needed for updating the metadata elements supported by resource type\n    :param  parsed_metadata: a list of dicts that will be appended with parsed data\n    \"\"\"\n\n    keys_to_update = list(metadata.keys())\n    if 'title' in keys_to_update:\n        parsed_metadata.append({\"title\": {\"value\": metadata.pop('title')}})\n\n    if 'creators' in keys_to_update:\n        if not isinstance(metadata['creators'], list):\n            metadata['creators'] = json.loads(metadata['creators'])\n        for creator in metadata.pop('creators'):\n            parsed_metadata.append({\"creator\": creator})\n\n    if 'contributors' in keys_to_update:\n        if not isinstance(metadata['contributors'], list):\n            metadata['contributors'] = json.loads(metadata['contributors'])\n        for contributor in metadata.pop('contributors'):\n            parsed_metadata.append({\"contributor\": contributor})\n\n    if 'coverages' in keys_to_update:\n        for coverage in metadata.pop('coverages'):\n            parsed_metadata.append({\"coverage\": coverage})\n\n    if 'dates' in keys_to_update:\n        for date in metadata.pop('dates'):\n            parsed_metadata.append({\"date\": date})\n\n    if 'description' in keys_to_update:\n        parsed_metadata.append({\"description\": {\"abstract\": metadata.pop('description')}})\n\n    if 'language' in keys_to_update:\n        parsed_metadata.append({\"language\": {\"code\": metadata.pop('language')}})\n\n    if 'rights' in keys_to_update:\n        parsed_metadata.append({\"rights\": metadata.pop('rights')})\n\n    if 'sources' in keys_to_update:\n        for source in metadata.pop('sources'):\n            parsed_metadata.append({\"source\": source})\n\n    if 'subjects' in keys_to_update:\n        for subject in metadata.pop('subjects'):\n            parsed_metadata.append({\"subject\": {\"value\": subject['value']}})\n\n    if 'funding_agencies' in keys_to_update:\n        for agency in metadata.pop(\"funding_agencies\"):\n            # using fundingagency instead of funding_agency to be consistent with UI\n            # add-metadata logic as well as the term for the metadata element.\n            parsed_metadata.append({\"fundingagency\": agency})\n\n    if 'relations' in keys_to_update:\n        for relation in metadata.pop('relations'):\n            parsed_metadata.append({\"relation\": relation})\n\n    if 'geospatialrelations' in keys_to_update:\n        for relation in metadata.pop('geospatialrelations'):\n            parsed_metadata.append({\"geospatialrelation\": relation})\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.set_dirty","title":"<code>set_dirty(flag)</code>","text":"<p>Track whethrer metadata object is dirty.</p> <p>Subclasses that have the attribute to track whether metadata object is dirty should override this method to allow setting that attribute</p> <p>:param flag: a boolean value :return:</p> Source code in <code>hs_core/models.py</code> <pre><code>def set_dirty(self, flag):\n    \"\"\"Track whethrer metadata object is dirty.\n\n    Subclasses that have the attribute to track whether metadata object is dirty\n    should override this method to allow setting that attribute\n\n    :param flag: a boolean value\n    :return:\n    \"\"\"\n    pass\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.update","title":"<code>update(metadata, user)</code>","text":"<p>Define custom update method for CoreMetaData model.</p> <p>:param metadata: a list of dicts - each dict in the format of {element_name: **kwargs} element_name must be in lowercase. example of a dict in metadata list:     {'creator': {'name': 'John Howard', 'email: 'jh@gmail.com'}} :param  user: user who is updating metadata :return:</p> Source code in <code>hs_core/models.py</code> <pre><code>def update(self, metadata, user):\n    \"\"\"Define custom update method for CoreMetaData model.\n\n    :param metadata: a list of dicts - each dict in the format of {element_name: **kwargs}\n    element_name must be in lowercase.\n    example of a dict in metadata list:\n        {'creator': {'name': 'John Howard', 'email: 'jh@gmail.com'}}\n    :param  user: user who is updating metadata\n    :return:\n    \"\"\"\n    from .forms import (AbstractValidationForm, ContributorValidationForm,\n                        CreatorValidationForm, FundingAgencyValidationForm,\n                        GeospatialRelationValidationForm,\n                        LanguageValidationForm, RelationValidationForm,\n                        RightsValidationForm, TitleValidationForm)\n\n    validation_forms_mapping = {'title': TitleValidationForm,\n                                'description': AbstractValidationForm,\n                                'language': LanguageValidationForm,\n                                'rights': RightsValidationForm,\n                                'creator': CreatorValidationForm,\n                                'contributor': ContributorValidationForm,\n                                'relation': RelationValidationForm,\n                                'geospatialrelation': GeospatialRelationValidationForm,\n                                'fundingagency': FundingAgencyValidationForm\n                                }\n    # updating non-repeatable elements\n    with transaction.atomic():\n        for element_name in ('title', 'description', 'language', 'rights'):\n            for dict_item in metadata:\n                if element_name in dict_item:\n                    validation_form = validation_forms_mapping[element_name](\n                        dict_item[element_name])\n                    if not validation_form.is_valid():\n                        err_string = self.get_form_errors_as_string(validation_form)\n                        raise ValidationError(err_string)\n            self.update_non_repeatable_element(element_name, metadata)\n        for element_name in ('creator', 'contributor', 'coverage', 'source', 'relation',\n                             'geospatialrelation', 'subject'):\n            subjects = []\n            for dict_item in metadata:\n                if element_name in dict_item:\n                    if element_name == 'subject':\n                        subject_data = dict_item['subject']\n                        if 'value' not in subject_data:\n                            raise ValidationError(\"Subject value is missing\")\n                        subjects.append(dict_item['subject']['value'])\n                        continue\n                    if element_name == 'coverage':\n                        coverage_data = dict_item[element_name]\n                        if 'type' not in coverage_data:\n                            raise ValidationError(\"Coverage type data is missing\")\n                        if 'value' not in coverage_data:\n                            raise ValidationError(\"Coverage value data is missing\")\n                        coverage_value_dict = coverage_data['value']\n                        coverage_type = coverage_data['type']\n                        Coverage.validate_coverage_type_value_attributes(coverage_type,\n                                                                         coverage_value_dict)\n                        continue\n                    if element_name in ['creator', 'contributor']:\n                        try:\n                            party_data = dict_item[element_name]\n                            if 'identifiers' in party_data:\n                                if isinstance(party_data['identifiers'], dict):\n                                    # convert dict to json for form validation\n                                    party_data['identifiers'] = json.dumps(\n                                        party_data['identifiers'])\n                        except Exception:\n                            raise ValidationError(\"Invalid identifier data for \"\n                                                  \"creator/contributor\")\n                        validation_form = validation_forms_mapping[element_name](\n                            party_data)\n                    else:\n                        validation_form = validation_forms_mapping[element_name](\n                            dict_item[element_name])\n\n                    if not validation_form.is_valid():\n                        err_string = self.get_form_errors_as_string(validation_form)\n                        err_string += \" element name:{}\".format(element_name)\n                        raise ValidationError(err_string)\n            if subjects:\n                subjects_set = set([s.lower() for s in subjects])\n                if len(subjects_set) &lt; len(subjects):\n                    raise ValidationError(\"Duplicate subject values found\")\n            self.update_repeatable_element(element_name=element_name, metadata=metadata)\n\n        # allow only updating or creating date element of type valid\n        element_name = 'date'\n        date_list = [date_dict for date_dict in metadata if element_name in date_dict]\n        if len(date_list) &gt; 0:\n            for date_item in date_list:\n                if 'type' in date_item[element_name]:\n                    if date_item[element_name]['type'] == 'valid':\n                        self.dates.filter(type='valid').delete()\n                        self.create_element(element_model_name=element_name,\n                                            **date_item[element_name])\n                        break\n\n        # allow only updating or creating identifiers which does not have name value\n        # 'hydroShareIdentifier'\n        element_name = 'identifier'\n        identifier_list = [id_dict for id_dict in metadata if element_name in id_dict]\n        if len(identifier_list) &gt; 0:\n            for id_item in identifier_list:\n                if 'name' in id_item[element_name]:\n                    if id_item[element_name]['name'].lower() != 'hydroshareidentifier':\n                        self.identifiers.filter(name=id_item[element_name]['name']).delete()\n                        self.create_element(element_model_name=element_name,\n                                            **id_item[element_name])\n\n        element_name = 'fundingagency'\n        identifier_list = [id_dict for id_dict in metadata if element_name in id_dict]\n        if len(identifier_list) &gt; 0:\n            for id_item in identifier_list:\n                validation_form = validation_forms_mapping[element_name](\n                    id_item[element_name])\n                if not validation_form.is_valid():\n                    err_string = self.get_form_errors_as_string(validation_form)\n                    raise ValidationError(err_string)\n            # update_repeatable_elements will append an 's' to element_name before getattr,\n            # unless property_name is provided.  I'd like to remove English grammar rules from\n            # our codebase, but in the interest of time, I'll just add a special case for\n            # handling funding_agencies\n            self.update_repeatable_element(element_name=element_name, metadata=metadata,\n                                           property_name=\"funding_agencies\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.update_element","title":"<code>update_element(element_model_name, element_id, **kwargs)</code>","text":"<p>Update metadata element.</p> Source code in <code>hs_core/models.py</code> <pre><code>def update_element(self, element_model_name, element_id, **kwargs):\n    \"\"\"Update metadata element.\"\"\"\n    model_type = self._get_metadata_element_model_type(element_model_name)\n    kwargs['content_object'] = self\n    element_model_name = element_model_name.lower()\n    resource = self.resource\n    if resource.raccess.published:\n        if element_model_name in ('title', 'creator', 'rights', 'identifier', 'format', 'publisher'):\n            raise ValidationError(\"{} can't be updated for a published resource\".format(element_model_name))\n        elif element_model_name == 'date':\n            date_type = kwargs.get('type', '')\n            if date_type and date_type != 'modified':\n                raise ValidationError(\"{} date can't be updated for a published resource\".format(date_type))\n    model_type.model_class().update(element_id, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.update_non_repeatable_element","title":"<code>update_non_repeatable_element(element_name, metadata, property_name=None)</code>","text":"<p>Update a non-repeatable metadata element.</p> <p>This helper function is to create/update a specific metadata element as specified by element_name :param element_name: metadata element class name (e.g. title) :param metadata: a list of dicts - each dict has data to update/create a specific metadata element (e.g. {'title': {'value': 'my resource title'}} :param property_name: name of the property/attribute name in this class or its sub class to access the metadata element instance of metadata_element. This is needed only when the property/attribute name differs from the element class name</p> <pre><code>Example:\nclass ModelProgramMetaData(CoreMetaData):\n    _mpmetadata = GenericRelation(MpMetadata)\n\n    @property\n    def program(self):\n        return self._mpmetadata.all().first()\n\nFor the above class to update the metadata element MpMetadata, this function needs to\nbe called with element_name='mpmetadata' and property_name='program'\n</code></pre> <p>:return:</p> Source code in <code>hs_core/models.py</code> <pre><code>def update_non_repeatable_element(self, element_name, metadata, property_name=None):\n    \"\"\"Update a non-repeatable metadata element.\n\n    This helper function is to create/update a specific metadata element as specified by\n    *element_name*\n    :param element_name: metadata element class name (e.g. title)\n    :param metadata: a list of dicts - each dict has data to update/create a specific metadata\n    element (e.g. {'title': {'value': 'my resource title'}}\n    :param property_name: name of the property/attribute name in this class or its sub class\n    to access the metadata element instance of *metadata_element*. This is needed only when\n    the property/attribute name differs from the element class name\n\n        Example:\n        class ModelProgramMetaData(CoreMetaData):\n            _mpmetadata = GenericRelation(MpMetadata)\n\n            @property\n            def program(self):\n                return self._mpmetadata.all().first()\n\n        For the above class to update the metadata element MpMetadata, this function needs to\n        be called with element_name='mpmetadata' and property_name='program'\n    :return:\n    \"\"\"\n    for dict_item in metadata:\n        if element_name in dict_item:\n            if property_name is None:\n                element = getattr(self, element_name, None)\n            else:\n                element = getattr(self, property_name, None)\n            if element:\n                self.update_element(element_id=element.id,\n                                    element_model_name=element_name,\n                                    **dict_item[element_name])\n            else:\n                self.create_element(element_model_name=element_name,\n                                    **dict_item[element_name])\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.CoreMetaData.update_repeatable_element","title":"<code>update_repeatable_element(element_name, metadata, property_name=None)</code>","text":"<p>Update a repeatable metadata element.</p> <p>Creates new metadata elements of type element_name. Any existing metadata elements of matching type get deleted first. :param element_name: class name of the metadata element (e.g. creator) :param metadata: a list of dicts containing data for each of the metadata elements that needs to be created/updated as part of bulk update :param property_name: (Optional) the property/attribute name used in this instance of CoreMetaData (or its sub class) to access all the objects of type element_type     Example:     class MODFLOWModelInstanceMetaData(ModelInstanceMetaData):          _model_input = GenericRelation(ModelInput)</p> <pre><code>    @property\n    def model_inputs(self):\n        return self._model_input.all()\n\nFor the above class to update the metadata element ModelInput, this function needs to\nbe called with element_name='modelinput' and property_name='model_inputs'. If in the\nabove class instead of using the attribute name '_model_inputs' we have used\n'modelinputs' then this function needs to be called with element_name='modelinput' and\nno need to pass a value for the property_name.\n</code></pre> <p>:return:</p> Source code in <code>hs_core/models.py</code> <pre><code>def update_repeatable_element(self, element_name, metadata, property_name=None):\n    \"\"\"Update a repeatable metadata element.\n\n    Creates new metadata elements of type *element_name*. Any existing metadata elements of\n    matching type get deleted first.\n    :param element_name: class name of the metadata element (e.g. creator)\n    :param metadata: a list of dicts containing data for each of the metadata elements that\n    needs to be created/updated as part of bulk update\n    :param property_name: (Optional) the property/attribute name used in this instance of\n    CoreMetaData (or its sub class) to access all the objects of type *element_type*\n        Example:\n        class MODFLOWModelInstanceMetaData(ModelInstanceMetaData):\n             _model_input = GenericRelation(ModelInput)\n\n            @property\n            def model_inputs(self):\n                return self._model_input.all()\n\n        For the above class to update the metadata element ModelInput, this function needs to\n        be called with element_name='modelinput' and property_name='model_inputs'. If in the\n        above class instead of using the attribute name '_model_inputs' we have used\n        'modelinputs' then this function needs to be called with element_name='modelinput' and\n        no need to pass a value for the property_name.\n\n    :return:\n    \"\"\"\n    element_list = [element_dict for element_dict in metadata if element_name in element_dict]\n    if len(element_list) &gt; 0:\n        if property_name is None:\n            elements = getattr(self, element_name + 's')\n        else:\n            elements = getattr(self, property_name)\n\n        elements.all().delete()\n        for element in element_list:\n            self.create_element(element_model_name=element_name, **element[element_name])\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Coverage","title":"<code>Coverage</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define Coverage custom metadata element model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.coverage)\nclass Coverage(AbstractMetaDataElement):\n    \"\"\"Define Coverage custom metadata element model.\"\"\"\n\n    COVERAGE_TYPES = (\n        ('box', 'Box'),\n        ('point', 'Point'),\n        ('period', 'Period')\n    )\n\n    term = 'Coverage'\n    type = models.CharField(max_length=20, choices=COVERAGE_TYPES)\n\n    def __unicode__(self):\n        \"\"\"Return {type} {value} for unicode representation.\"\"\"\n        return \"{type} {value}\".format(type=self.type, value=self._value)\n\n    class Meta:\n        \"\"\"Define meta properties for Coverage model.\"\"\"\n\n        unique_together = (\"type\", \"content_type\", \"object_id\")\n    \"\"\"\n    _value field stores a json string. The content of the json\n     string depends on the type of coverage as shown below. All keys shown in\n     json string are required.\n\n     For coverage type: period\n         _value = \"{'name':coverage name value here (optional), 'start':start date value,\n         'end':end date value, 'scheme':'W3C-DTF}\"\n\n     For coverage type: point\n         _value = \"{'east':east coordinate value,\n                    'north':north coordinate value,\n                    'units:units applying to (east. north),\n                    'name':coverage name value here (optional),\n                    'elevation': coordinate in the vertical direction (optional),\n                    'zunits': units for elevation (optional),\n                    'projection': name of the projection (optional),\n                    }\"\n\n     For coverage type: box\n         _value = \"{'northlimit':northenmost coordinate value,\n                    'eastlimit':easternmost coordinate value,\n                    'southlimit':southernmost coordinate value,\n                    'westlimit':westernmost coordinate value,\n                    'units:units applying to 4 limits (north, east, south &amp; east),\n                    'name':coverage name value here (optional),\n                    'uplimit':uppermost coordinate value (optional),\n                    'downlimit':lowermost coordinate value (optional),\n                    'zunits': units for uplimit/downlimit (optional),\n                    'projection': name of the projection (optional)}\"\n    \"\"\"\n    _value = models.CharField(max_length=1024)\n\n    @property\n    def value(self):\n        \"\"\"Return json representation of coverage values.\"\"\"\n        return json.loads(self._value)\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Define custom create method for Coverage model.\n\n        data for the coverage value attribute must be provided as a dictionary\n        Note that kwargs['_value'] is a JSON-serialized unicode string dictionary\n        generated from django.forms.models.model_to_dict() which converts model values\n        to dictionaries.\n        \"\"\"\n        if 'type' in kwargs:\n            # check the type doesn't already exists - we allow only one coverage type per resource\n            metadata_obj = kwargs['content_object']\n            metadata_type = ContentType.objects.get_for_model(metadata_obj)\n\n            if not kwargs['type'] in list(dict(cls.COVERAGE_TYPES).keys()):\n                raise ValidationError('Invalid coverage type:%s' % kwargs['type'])\n\n            if kwargs['type'] == 'box':\n                # check that there is not already a coverage of point type\n                coverage = Coverage.objects.filter(type='point', object_id=metadata_obj.id,\n                                                   content_type=metadata_type).first()\n                if coverage:\n                    raise ValidationError(\"Coverage type 'Box' can't be created when there \"\n                                          \"is a coverage of type 'Point'\")\n            elif kwargs['type'] == 'point':\n                # check that there is not already a coverage of box type\n                coverage = Coverage.objects.filter(type='box', object_id=metadata_obj.id,\n                                                   content_type=metadata_type).first()\n                if coverage:\n                    raise ValidationError(\"Coverage type 'Point' can't be created when \"\n                                          \"there is a coverage of type 'Box'\")\n\n            value_arg_dict = None\n            if 'value' in kwargs:\n                value_arg_dict = kwargs['value']\n            elif '_value' in kwargs:\n                value_arg_dict = json.loads(kwargs['_value'])\n\n            if value_arg_dict is not None:\n                cls.validate_coverage_type_value_attributes(kwargs['type'], value_arg_dict)\n\n                if kwargs['type'] == 'period':\n                    value_dict = {k: v for k, v in list(value_arg_dict.items())\n                                  if k in ('name', 'start', 'end')}\n                elif kwargs['type'] == 'point':\n                    value_dict = {k: v for k, v in list(value_arg_dict.items())\n                                  if k in ('name', 'east', 'north', 'units', 'elevation',\n                                           'zunits', 'projection')}\n                elif kwargs['type'] == 'box':\n                    value_dict = {k: v for k, v in list(value_arg_dict.items())\n                                  if k in ('units', 'northlimit', 'eastlimit', 'southlimit',\n                                           'westlimit', 'name', 'uplimit', 'downlimit',\n                                           'zunits', 'projection')}\n\n                if kwargs['type'] == 'box' or kwargs['type'] == 'point':\n                    if 'projection' not in value_dict:\n                        value_dict['projection'] = 'WGS 84 EPSG:4326'\n\n                value_json = json.dumps(value_dict)\n                if 'value' in kwargs:\n                    del kwargs['value']\n                kwargs['_value'] = value_json\n                return super(Coverage, cls).create(**kwargs)\n\n            else:\n                raise ValidationError('Coverage value is missing.')\n\n        else:\n            raise ValidationError(\"Type of coverage element is missing.\")\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        \"\"\"Define custom create method for Coverage model.\n\n        data for the coverage value attribute must be provided as a dictionary\n        \"\"\"\n        cov = Coverage.objects.get(id=element_id)\n\n        changing_coverage_type = False\n\n        if 'type' in kwargs:\n            changing_coverage_type = cov.type != kwargs['type']\n            if 'value' in kwargs:\n                cls.validate_coverage_type_value_attributes(kwargs['type'], kwargs['value'])\n            else:\n                raise ValidationError('Coverage value is missing.')\n\n        if 'value' in kwargs:\n            if changing_coverage_type:\n                value_dict = {}\n                cov.type = kwargs['type']\n            else:\n                value_dict = cov.value\n\n            if 'name' in kwargs['value']:\n                value_dict['name'] = kwargs['value']['name']\n\n            if cov.type == 'period':\n                for item_name in ('start', 'end'):\n                    if item_name in kwargs['value']:\n                        value_dict[item_name] = kwargs['value'][item_name]\n            elif cov.type == 'point':\n                for item_name in ('east', 'north', 'units', 'elevation', 'zunits', 'projection'):\n                    if item_name in kwargs['value']:\n                        value_dict[item_name] = kwargs['value'][item_name]\n            elif cov.type == 'box':\n                for item_name in ('units', 'northlimit', 'eastlimit', 'southlimit', 'westlimit',\n                                  'uplimit', 'downlimit', 'zunits', 'projection'):\n                    if item_name in kwargs['value']:\n                        value_dict[item_name] = kwargs['value'][item_name]\n\n            value_json = json.dumps(value_dict)\n            del kwargs['value']\n            kwargs['_value'] = value_json\n\n        super(Coverage, cls).update(element_id, **kwargs)\n\n    @classmethod\n    def remove(cls, element_id):\n        \"\"\"Define custom remove method for Coverage model.\"\"\"\n        raise ValidationError(\"Coverage element can't be deleted.\")\n\n    def add_to_xml_container(self, container):\n        \"\"\"Update etree SubElement container with coverage values.\"\"\"\n        NAMESPACES = CoreMetaData.NAMESPACES\n        dc_coverage = etree.SubElement(container, '{%s}coverage' % NAMESPACES['dc'])\n        cov_dcterm = '{%s}' + self.type\n        dc_coverage_dcterms = etree.SubElement(dc_coverage,\n                                               cov_dcterm % NAMESPACES['dcterms'])\n        rdf_coverage_value = etree.SubElement(dc_coverage_dcterms,\n                                              '{%s}value' % NAMESPACES['rdf'])\n        if self.type == 'period':\n            start_date = parser.parse(self.value['start'])\n            end_date = parser.parse(self.value['end'])\n            cov_value = 'start=%s; end=%s; scheme=W3C-DTF' % (start_date.isoformat(),\n                                                              end_date.isoformat())\n\n            if 'name' in self.value:\n                cov_value = 'name=%s; ' % self.value['name'] + cov_value\n\n        elif self.type == 'point':\n            cov_value = 'east=%s; north=%s; units=%s' % (self.value['east'],\n                                                         self.value['north'],\n                                                         self.value['units'])\n            if 'name' in self.value:\n                cov_value = 'name=%s; ' % self.value['name'] + cov_value\n            if 'elevation' in self.value:\n                cov_value += '; elevation=%s' % self.value['elevation']\n                if 'zunits' in self.value:\n                    cov_value += '; zunits=%s' % self.value['zunits']\n            if 'projection' in self.value:\n                cov_value += '; projection=%s' % self.value['projection']\n\n        else:\n            # this is box type\n            cov_value = 'northlimit=%s; eastlimit=%s; southlimit=%s; westlimit=%s; units=%s' \\\n                        % (self.value['northlimit'], self.value['eastlimit'],\n                           self.value['southlimit'], self.value['westlimit'],\n                           self.value['units'])\n\n            if 'name' in self.value:\n                cov_value = 'name=%s; ' % self.value['name'] + cov_value\n            if 'uplimit' in self.value:\n                cov_value += '; uplimit=%s' % self.value['uplimit']\n            if 'downlimit' in self.value:\n                cov_value += '; downlimit=%s' % self.value['downlimit']\n            if 'uplimit' in self.value or 'downlimit' in self.value:\n                cov_value += '; zunits=%s' % self.value['zunits']\n            if 'projection' in self.value:\n                cov_value += '; projection=%s' % self.value['projection']\n\n        rdf_coverage_value.text = cov_value\n\n    @classmethod\n    def ingest_rdf(cls, graph, subject, content_object):\n        for _, _, cov in graph.triples((subject, cls.get_class_term(), None)):\n            type = graph.value(subject=cov, predicate=RDF.type)\n            value = graph.value(subject=cov, predicate=RDF.value)\n            type = type.split('/')[-1]\n            value_dict = {}\n            for key_value in value.split(\";\"):\n                key_value = key_value.strip()\n                k, v = key_value.split(\"=\")\n                if k in ['start', 'end']:\n                    v = parser.parse(v).strftime(\"%Y/%m/%d %H:%M:%S\")\n                value_dict[k] = v\n            Coverage.create(type=type, value=value_dict, content_object=content_object)\n\n    def rdf_triples(self, subject, graph):\n        coverage = BNode()\n        graph.add((subject, self.get_class_term(), coverage))\n        DCTERMS_type = getattr(DCTERMS, self.type)\n        graph.add((coverage, RDF.type, DCTERMS_type))\n        value_dict = {}\n        for k, v in self.value.items():\n            if k in ['start', 'end']:\n                v = parser.parse(v).isoformat()\n            value_dict[k] = v\n        value_string = \"; \".join([\"=\".join([key, str(val)]) for key, val in value_dict.items()])\n        graph.add((coverage, RDF.value, Literal(value_string)))\n\n    @classmethod\n    def validate_coverage_type_value_attributes(cls, coverage_type, value_dict, use_limit_postfix=True):\n        \"\"\"Validate values based on coverage type.\"\"\"\n        def compute_longitude(key_name):\n            if value_dict[key_name] &lt;= -180 and value_dict[key_name] &gt;= -360:\n                value_dict[key_name] = value_dict[key_name] + 360\n            elif value_dict[key_name] &gt;= 180 and value_dict[key_name] &lt;= 360:\n                value_dict[key_name] = value_dict[key_name] - 360\n            if value_dict[key_name] &lt; -180 or value_dict[key_name] &gt; 180:\n                err_msg = \"Invalid value for {}:{}. Value for {} longitude should be in the range of -180 to 180\"\n                err_msg = err_msg.format(key_name, value_dict[key_name], key_name)\n                raise ValidationError(err_msg)\n\n        if coverage_type == 'period':\n            # check that all the required sub-elements exist\n            if 'start' not in value_dict or 'end' not in value_dict:\n                raise ValidationError(\"For coverage of type 'period' values for both start date \"\n                                      \"and end date are needed.\")\n        elif coverage_type == 'point':\n            # check that all the required sub-elements exist\n            if 'east' not in value_dict or 'north' not in value_dict or 'units' not in value_dict:\n                raise ValidationError(\"For coverage of type 'point' values for 'east', 'north' \"\n                                      \"and 'units' are needed.\")\n\n            for value_item in ('east', 'north'):\n                try:\n                    value_dict[value_item] = float(value_dict[value_item])\n                except TypeError:\n                    raise ValidationError(\"Value for '{}' must be numeric\".format(value_item))\n\n            compute_longitude(key_name='east')\n            if value_dict['north'] &lt; -90 or value_dict['north'] &gt; 90:\n                raise ValidationError(\"Value for North latitude should be \"\n                                      \"in the range of -90 to 90\")\n\n        elif coverage_type == 'box':\n            # check that all the required sub-elements exist\n            box_key_names = {'north': 'north', 'east': 'east', 'south': 'south', 'west': 'west'}\n            if use_limit_postfix:\n                for key, value in box_key_names.items():\n                    box_key_names[key] = f\"{value}limit\"\n            required_keys = list(box_key_names.values()) + ['units']\n            for value_item in required_keys:\n                if value_item not in value_dict:\n                    raise ValidationError(\"For coverage of type 'box' values for one or more \"\n                                          \"bounding box limits or 'units' is missing.\")\n                else:\n                    if value_item != 'units':\n                        try:\n                            value_dict[value_item] = float(value_dict[value_item])\n                        except TypeError:\n                            raise ValidationError(\"Value for '{}' must be numeric\"\n                                                  .format(value_item))\n\n            if value_dict[box_key_names['north']] &lt; -90 or value_dict[box_key_names['north']] &gt; 90:\n                raise ValidationError(\"Value for North latitude should be \"\n                                      \"in the range of -90 to 90\")\n\n            if value_dict[box_key_names['south']] &lt; -90 or value_dict[box_key_names['south']] &gt; 90:\n                raise ValidationError(\"Value for South latitude should be \"\n                                      \"in the range of -90 to 90\")\n\n            if (value_dict[box_key_names['north']] &lt; 0 and value_dict[box_key_names['south']] &lt; 0) or (\n                    value_dict[box_key_names['north']] &gt; 0 and value_dict[box_key_names['south']] &gt; 0):\n                if value_dict[box_key_names['north']] &lt; value_dict[box_key_names['south']]:\n                    raise ValidationError(\"Value for North latitude must be greater than or \"\n                                          \"equal to that of South latitude.\")\n\n            compute_longitude(key_name=box_key_names['east'])\n            compute_longitude(key_name=box_key_names['west'])\n\n    def get_html(self, pretty=True):\n        \"\"\"Use the dominate module to generate element display HTML.\n\n        This function should be used for displaying one spatial coverage element\n        or one temporal coverage element\n        \"\"\"\n        root_div = div(cls='content-block')\n\n        def get_th(heading_name):\n            return th(heading_name, cls=\"text-muted\")\n\n        with root_div:\n            if self.type == 'box' or self.type == 'point':\n                legend('Spatial Coverage')\n                div('Coordinate Reference System', cls='text-muted')\n                div(self.value['projection'])\n                div('Coordinate Reference System Unit', cls='text-muted has-space-top')\n                div(self.value['units'])\n                h4('Extent', cls='space-top')\n                with table(cls='custom-table'):\n                    if self.type == 'box':\n                        with tbody():\n                            with tr():\n                                get_th('North')\n                                td(self.value['northlimit'])\n                            with tr():\n                                get_th('West')\n                                td(self.value['westlimit'])\n                            with tr():\n                                get_th('South')\n                                td(self.value['southlimit'])\n                            with tr():\n                                get_th('East')\n                                td(self.value['eastlimit'])\n                    else:\n                        with tr():\n                            get_th('North')\n                            td(self.value['north'])\n                        with tr():\n                            get_th('East')\n                            td(self.value['east'])\n            else:\n                legend('Temporal Coverage')\n                start_date = parser.parse(self.value['start'])\n                end_date = parser.parse(self.value['end'])\n                with table(cls='custom-table'):\n                    with tbody():\n                        with tr():\n                            get_th('Start Date')\n                            td(start_date.strftime('%m/%d/%Y'))\n                        with tr():\n                            get_th('End Date')\n                            td(end_date.strftime('%m/%d/%Y'))\n\n        return root_div.render(pretty=pretty)\n\n    @classmethod\n    def get_temporal_html_form(cls, resource, element=None, file_type=False, allow_edit=True):\n        \"\"\"Return CoverageTemporalForm for Coverage model.\"\"\"\n        from .forms import CoverageTemporalForm\n        coverage_data_dict = dict()\n        if element is not None:\n            start_date = parser.parse(element.value['start'])\n            end_date = parser.parse(element.value['end'])\n            # change the date format to match with datepicker date format\n            coverage_data_dict['start'] = start_date.strftime('%m/%d/%Y')\n            coverage_data_dict['end'] = end_date.strftime('%m/%d/%Y')\n\n        coverage_form = CoverageTemporalForm(initial=coverage_data_dict, allow_edit=allow_edit,\n                                             res_short_id=resource.short_id if resource else None,\n                                             element_id=element.id if element else None,\n                                             file_type=file_type)\n        return coverage_form\n\n    @classmethod\n    def get_spatial_html_form(cls, resource, element=None, allow_edit=True, file_type=False):\n        \"\"\"Return SpatialCoverageForm for Coverage model.\"\"\"\n        from .forms import CoverageSpatialForm\n        coverage_data_dict = dict()\n\n        if element is not None:\n            coverage_data_dict['type'] = element.type\n            coverage_data_dict['name'] = element.value.get('name', \"\")\n            if element.type == 'box':\n                coverage_data_dict['northlimit'] = element.value['northlimit']\n                coverage_data_dict['eastlimit'] = element.value['eastlimit']\n                coverage_data_dict['southlimit'] = element.value['southlimit']\n                coverage_data_dict['westlimit'] = element.value['westlimit']\n            else:\n                coverage_data_dict['east'] = element.value['east']\n                coverage_data_dict['north'] = element.value['north']\n                coverage_data_dict['elevation'] = element.value.get('elevation', None)\n\n        coverage_form = CoverageSpatialForm(initial=coverage_data_dict, allow_edit=allow_edit,\n                                            res_short_id=resource.short_id if resource else None,\n                                            element_id=element.id if element else None,\n                                            file_type=file_type)\n        return coverage_form\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Coverage.value","title":"<code>value</code>  <code>property</code>","text":"<p>Return json representation of coverage values.</p>"},{"location":"hs_core/models/#hs_core.models.Coverage.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Coverage model.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Coverage model.\"\"\"\n\n    unique_together = (\"type\", \"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Coverage.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return {type} {value} for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return {type} {value} for unicode representation.\"\"\"\n    return \"{type} {value}\".format(type=self.type, value=self._value)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Coverage.add_to_xml_container","title":"<code>add_to_xml_container(container)</code>","text":"<p>Update etree SubElement container with coverage values.</p> Source code in <code>hs_core/models.py</code> <pre><code>def add_to_xml_container(self, container):\n    \"\"\"Update etree SubElement container with coverage values.\"\"\"\n    NAMESPACES = CoreMetaData.NAMESPACES\n    dc_coverage = etree.SubElement(container, '{%s}coverage' % NAMESPACES['dc'])\n    cov_dcterm = '{%s}' + self.type\n    dc_coverage_dcterms = etree.SubElement(dc_coverage,\n                                           cov_dcterm % NAMESPACES['dcterms'])\n    rdf_coverage_value = etree.SubElement(dc_coverage_dcterms,\n                                          '{%s}value' % NAMESPACES['rdf'])\n    if self.type == 'period':\n        start_date = parser.parse(self.value['start'])\n        end_date = parser.parse(self.value['end'])\n        cov_value = 'start=%s; end=%s; scheme=W3C-DTF' % (start_date.isoformat(),\n                                                          end_date.isoformat())\n\n        if 'name' in self.value:\n            cov_value = 'name=%s; ' % self.value['name'] + cov_value\n\n    elif self.type == 'point':\n        cov_value = 'east=%s; north=%s; units=%s' % (self.value['east'],\n                                                     self.value['north'],\n                                                     self.value['units'])\n        if 'name' in self.value:\n            cov_value = 'name=%s; ' % self.value['name'] + cov_value\n        if 'elevation' in self.value:\n            cov_value += '; elevation=%s' % self.value['elevation']\n            if 'zunits' in self.value:\n                cov_value += '; zunits=%s' % self.value['zunits']\n        if 'projection' in self.value:\n            cov_value += '; projection=%s' % self.value['projection']\n\n    else:\n        # this is box type\n        cov_value = 'northlimit=%s; eastlimit=%s; southlimit=%s; westlimit=%s; units=%s' \\\n                    % (self.value['northlimit'], self.value['eastlimit'],\n                       self.value['southlimit'], self.value['westlimit'],\n                       self.value['units'])\n\n        if 'name' in self.value:\n            cov_value = 'name=%s; ' % self.value['name'] + cov_value\n        if 'uplimit' in self.value:\n            cov_value += '; uplimit=%s' % self.value['uplimit']\n        if 'downlimit' in self.value:\n            cov_value += '; downlimit=%s' % self.value['downlimit']\n        if 'uplimit' in self.value or 'downlimit' in self.value:\n            cov_value += '; zunits=%s' % self.value['zunits']\n        if 'projection' in self.value:\n            cov_value += '; projection=%s' % self.value['projection']\n\n    rdf_coverage_value.text = cov_value\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Coverage.create","title":"<code>create(**kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom create method for Coverage model.</p> <p>data for the coverage value attribute must be provided as a dictionary Note that kwargs['_value'] is a JSON-serialized unicode string dictionary generated from django.forms.models.model_to_dict() which converts model values to dictionaries.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"Define custom create method for Coverage model.\n\n    data for the coverage value attribute must be provided as a dictionary\n    Note that kwargs['_value'] is a JSON-serialized unicode string dictionary\n    generated from django.forms.models.model_to_dict() which converts model values\n    to dictionaries.\n    \"\"\"\n    if 'type' in kwargs:\n        # check the type doesn't already exists - we allow only one coverage type per resource\n        metadata_obj = kwargs['content_object']\n        metadata_type = ContentType.objects.get_for_model(metadata_obj)\n\n        if not kwargs['type'] in list(dict(cls.COVERAGE_TYPES).keys()):\n            raise ValidationError('Invalid coverage type:%s' % kwargs['type'])\n\n        if kwargs['type'] == 'box':\n            # check that there is not already a coverage of point type\n            coverage = Coverage.objects.filter(type='point', object_id=metadata_obj.id,\n                                               content_type=metadata_type).first()\n            if coverage:\n                raise ValidationError(\"Coverage type 'Box' can't be created when there \"\n                                      \"is a coverage of type 'Point'\")\n        elif kwargs['type'] == 'point':\n            # check that there is not already a coverage of box type\n            coverage = Coverage.objects.filter(type='box', object_id=metadata_obj.id,\n                                               content_type=metadata_type).first()\n            if coverage:\n                raise ValidationError(\"Coverage type 'Point' can't be created when \"\n                                      \"there is a coverage of type 'Box'\")\n\n        value_arg_dict = None\n        if 'value' in kwargs:\n            value_arg_dict = kwargs['value']\n        elif '_value' in kwargs:\n            value_arg_dict = json.loads(kwargs['_value'])\n\n        if value_arg_dict is not None:\n            cls.validate_coverage_type_value_attributes(kwargs['type'], value_arg_dict)\n\n            if kwargs['type'] == 'period':\n                value_dict = {k: v for k, v in list(value_arg_dict.items())\n                              if k in ('name', 'start', 'end')}\n            elif kwargs['type'] == 'point':\n                value_dict = {k: v for k, v in list(value_arg_dict.items())\n                              if k in ('name', 'east', 'north', 'units', 'elevation',\n                                       'zunits', 'projection')}\n            elif kwargs['type'] == 'box':\n                value_dict = {k: v for k, v in list(value_arg_dict.items())\n                              if k in ('units', 'northlimit', 'eastlimit', 'southlimit',\n                                       'westlimit', 'name', 'uplimit', 'downlimit',\n                                       'zunits', 'projection')}\n\n            if kwargs['type'] == 'box' or kwargs['type'] == 'point':\n                if 'projection' not in value_dict:\n                    value_dict['projection'] = 'WGS 84 EPSG:4326'\n\n            value_json = json.dumps(value_dict)\n            if 'value' in kwargs:\n                del kwargs['value']\n            kwargs['_value'] = value_json\n            return super(Coverage, cls).create(**kwargs)\n\n        else:\n            raise ValidationError('Coverage value is missing.')\n\n    else:\n        raise ValidationError(\"Type of coverage element is missing.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Coverage.get_html","title":"<code>get_html(pretty=True)</code>","text":"<p>Use the dominate module to generate element display HTML.</p> <p>This function should be used for displaying one spatial coverage element or one temporal coverage element</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_html(self, pretty=True):\n    \"\"\"Use the dominate module to generate element display HTML.\n\n    This function should be used for displaying one spatial coverage element\n    or one temporal coverage element\n    \"\"\"\n    root_div = div(cls='content-block')\n\n    def get_th(heading_name):\n        return th(heading_name, cls=\"text-muted\")\n\n    with root_div:\n        if self.type == 'box' or self.type == 'point':\n            legend('Spatial Coverage')\n            div('Coordinate Reference System', cls='text-muted')\n            div(self.value['projection'])\n            div('Coordinate Reference System Unit', cls='text-muted has-space-top')\n            div(self.value['units'])\n            h4('Extent', cls='space-top')\n            with table(cls='custom-table'):\n                if self.type == 'box':\n                    with tbody():\n                        with tr():\n                            get_th('North')\n                            td(self.value['northlimit'])\n                        with tr():\n                            get_th('West')\n                            td(self.value['westlimit'])\n                        with tr():\n                            get_th('South')\n                            td(self.value['southlimit'])\n                        with tr():\n                            get_th('East')\n                            td(self.value['eastlimit'])\n                else:\n                    with tr():\n                        get_th('North')\n                        td(self.value['north'])\n                    with tr():\n                        get_th('East')\n                        td(self.value['east'])\n        else:\n            legend('Temporal Coverage')\n            start_date = parser.parse(self.value['start'])\n            end_date = parser.parse(self.value['end'])\n            with table(cls='custom-table'):\n                with tbody():\n                    with tr():\n                        get_th('Start Date')\n                        td(start_date.strftime('%m/%d/%Y'))\n                    with tr():\n                        get_th('End Date')\n                        td(end_date.strftime('%m/%d/%Y'))\n\n    return root_div.render(pretty=pretty)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Coverage.get_spatial_html_form","title":"<code>get_spatial_html_form(resource, element=None, allow_edit=True, file_type=False)</code>  <code>classmethod</code>","text":"<p>Return SpatialCoverageForm for Coverage model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef get_spatial_html_form(cls, resource, element=None, allow_edit=True, file_type=False):\n    \"\"\"Return SpatialCoverageForm for Coverage model.\"\"\"\n    from .forms import CoverageSpatialForm\n    coverage_data_dict = dict()\n\n    if element is not None:\n        coverage_data_dict['type'] = element.type\n        coverage_data_dict['name'] = element.value.get('name', \"\")\n        if element.type == 'box':\n            coverage_data_dict['northlimit'] = element.value['northlimit']\n            coverage_data_dict['eastlimit'] = element.value['eastlimit']\n            coverage_data_dict['southlimit'] = element.value['southlimit']\n            coverage_data_dict['westlimit'] = element.value['westlimit']\n        else:\n            coverage_data_dict['east'] = element.value['east']\n            coverage_data_dict['north'] = element.value['north']\n            coverage_data_dict['elevation'] = element.value.get('elevation', None)\n\n    coverage_form = CoverageSpatialForm(initial=coverage_data_dict, allow_edit=allow_edit,\n                                        res_short_id=resource.short_id if resource else None,\n                                        element_id=element.id if element else None,\n                                        file_type=file_type)\n    return coverage_form\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Coverage.get_temporal_html_form","title":"<code>get_temporal_html_form(resource, element=None, file_type=False, allow_edit=True)</code>  <code>classmethod</code>","text":"<p>Return CoverageTemporalForm for Coverage model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef get_temporal_html_form(cls, resource, element=None, file_type=False, allow_edit=True):\n    \"\"\"Return CoverageTemporalForm for Coverage model.\"\"\"\n    from .forms import CoverageTemporalForm\n    coverage_data_dict = dict()\n    if element is not None:\n        start_date = parser.parse(element.value['start'])\n        end_date = parser.parse(element.value['end'])\n        # change the date format to match with datepicker date format\n        coverage_data_dict['start'] = start_date.strftime('%m/%d/%Y')\n        coverage_data_dict['end'] = end_date.strftime('%m/%d/%Y')\n\n    coverage_form = CoverageTemporalForm(initial=coverage_data_dict, allow_edit=allow_edit,\n                                         res_short_id=resource.short_id if resource else None,\n                                         element_id=element.id if element else None,\n                                         file_type=file_type)\n    return coverage_form\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Coverage.remove","title":"<code>remove(element_id)</code>  <code>classmethod</code>","text":"<p>Define custom remove method for Coverage model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id):\n    \"\"\"Define custom remove method for Coverage model.\"\"\"\n    raise ValidationError(\"Coverage element can't be deleted.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Coverage.update","title":"<code>update(element_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom create method for Coverage model.</p> <p>data for the coverage value attribute must be provided as a dictionary</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef update(cls, element_id, **kwargs):\n    \"\"\"Define custom create method for Coverage model.\n\n    data for the coverage value attribute must be provided as a dictionary\n    \"\"\"\n    cov = Coverage.objects.get(id=element_id)\n\n    changing_coverage_type = False\n\n    if 'type' in kwargs:\n        changing_coverage_type = cov.type != kwargs['type']\n        if 'value' in kwargs:\n            cls.validate_coverage_type_value_attributes(kwargs['type'], kwargs['value'])\n        else:\n            raise ValidationError('Coverage value is missing.')\n\n    if 'value' in kwargs:\n        if changing_coverage_type:\n            value_dict = {}\n            cov.type = kwargs['type']\n        else:\n            value_dict = cov.value\n\n        if 'name' in kwargs['value']:\n            value_dict['name'] = kwargs['value']['name']\n\n        if cov.type == 'period':\n            for item_name in ('start', 'end'):\n                if item_name in kwargs['value']:\n                    value_dict[item_name] = kwargs['value'][item_name]\n        elif cov.type == 'point':\n            for item_name in ('east', 'north', 'units', 'elevation', 'zunits', 'projection'):\n                if item_name in kwargs['value']:\n                    value_dict[item_name] = kwargs['value'][item_name]\n        elif cov.type == 'box':\n            for item_name in ('units', 'northlimit', 'eastlimit', 'southlimit', 'westlimit',\n                              'uplimit', 'downlimit', 'zunits', 'projection'):\n                if item_name in kwargs['value']:\n                    value_dict[item_name] = kwargs['value'][item_name]\n\n        value_json = json.dumps(value_dict)\n        del kwargs['value']\n        kwargs['_value'] = value_json\n\n    super(Coverage, cls).update(element_id, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Coverage.validate_coverage_type_value_attributes","title":"<code>validate_coverage_type_value_attributes(coverage_type, value_dict, use_limit_postfix=True)</code>  <code>classmethod</code>","text":"<p>Validate values based on coverage type.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef validate_coverage_type_value_attributes(cls, coverage_type, value_dict, use_limit_postfix=True):\n    \"\"\"Validate values based on coverage type.\"\"\"\n    def compute_longitude(key_name):\n        if value_dict[key_name] &lt;= -180 and value_dict[key_name] &gt;= -360:\n            value_dict[key_name] = value_dict[key_name] + 360\n        elif value_dict[key_name] &gt;= 180 and value_dict[key_name] &lt;= 360:\n            value_dict[key_name] = value_dict[key_name] - 360\n        if value_dict[key_name] &lt; -180 or value_dict[key_name] &gt; 180:\n            err_msg = \"Invalid value for {}:{}. Value for {} longitude should be in the range of -180 to 180\"\n            err_msg = err_msg.format(key_name, value_dict[key_name], key_name)\n            raise ValidationError(err_msg)\n\n    if coverage_type == 'period':\n        # check that all the required sub-elements exist\n        if 'start' not in value_dict or 'end' not in value_dict:\n            raise ValidationError(\"For coverage of type 'period' values for both start date \"\n                                  \"and end date are needed.\")\n    elif coverage_type == 'point':\n        # check that all the required sub-elements exist\n        if 'east' not in value_dict or 'north' not in value_dict or 'units' not in value_dict:\n            raise ValidationError(\"For coverage of type 'point' values for 'east', 'north' \"\n                                  \"and 'units' are needed.\")\n\n        for value_item in ('east', 'north'):\n            try:\n                value_dict[value_item] = float(value_dict[value_item])\n            except TypeError:\n                raise ValidationError(\"Value for '{}' must be numeric\".format(value_item))\n\n        compute_longitude(key_name='east')\n        if value_dict['north'] &lt; -90 or value_dict['north'] &gt; 90:\n            raise ValidationError(\"Value for North latitude should be \"\n                                  \"in the range of -90 to 90\")\n\n    elif coverage_type == 'box':\n        # check that all the required sub-elements exist\n        box_key_names = {'north': 'north', 'east': 'east', 'south': 'south', 'west': 'west'}\n        if use_limit_postfix:\n            for key, value in box_key_names.items():\n                box_key_names[key] = f\"{value}limit\"\n        required_keys = list(box_key_names.values()) + ['units']\n        for value_item in required_keys:\n            if value_item not in value_dict:\n                raise ValidationError(\"For coverage of type 'box' values for one or more \"\n                                      \"bounding box limits or 'units' is missing.\")\n            else:\n                if value_item != 'units':\n                    try:\n                        value_dict[value_item] = float(value_dict[value_item])\n                    except TypeError:\n                        raise ValidationError(\"Value for '{}' must be numeric\"\n                                              .format(value_item))\n\n        if value_dict[box_key_names['north']] &lt; -90 or value_dict[box_key_names['north']] &gt; 90:\n            raise ValidationError(\"Value for North latitude should be \"\n                                  \"in the range of -90 to 90\")\n\n        if value_dict[box_key_names['south']] &lt; -90 or value_dict[box_key_names['south']] &gt; 90:\n            raise ValidationError(\"Value for South latitude should be \"\n                                  \"in the range of -90 to 90\")\n\n        if (value_dict[box_key_names['north']] &lt; 0 and value_dict[box_key_names['south']] &lt; 0) or (\n                value_dict[box_key_names['north']] &gt; 0 and value_dict[box_key_names['south']] &gt; 0):\n            if value_dict[box_key_names['north']] &lt; value_dict[box_key_names['south']]:\n                raise ValidationError(\"Value for North latitude must be greater than or \"\n                                      \"equal to that of South latitude.\")\n\n        compute_longitude(key_name=box_key_names['east'])\n        compute_longitude(key_name=box_key_names['west'])\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Creator","title":"<code>Creator</code>","text":"<p>               Bases: <code>Party</code></p> <p>Extend Party model with the term of 'Creator' and a proper ordering.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.creator, order=HSTERMS.creatorOrder)\nclass Creator(Party):\n    \"\"\"Extend Party model with the term of 'Creator' and a proper ordering.\"\"\"\n\n    term = \"Creator\"\n    order = models.PositiveIntegerField()\n\n    class Meta:\n        \"\"\"Define meta properties for Creator class.\"\"\"\n\n        ordering = ['order']\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Creator.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Creator class.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Creator class.\"\"\"\n\n    ordering = ['order']\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Date","title":"<code>Date</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define Date metadata model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.date)\nclass Date(AbstractMetaDataElement):\n    \"\"\"Define Date metadata model.\"\"\"\n\n    DC_DATE_TYPE_CHOICES = (\n        ('created', 'Created'),\n        ('modified', 'Modified'),\n        ('valid', 'Valid'),\n        ('available', 'Available')\n    )\n    HS_DATE_TYPE_CHOICES = (\n        ('reviewStarted', 'Review Started'),\n        ('published', 'Published'),\n    )\n    DATE_TYPE_CHOICES = DC_DATE_TYPE_CHOICES + HS_DATE_TYPE_CHOICES\n\n    term = 'Date'\n    type = models.CharField(max_length=20, choices=DATE_TYPE_CHOICES)\n    start_date = models.DateTimeField()\n    end_date = models.DateTimeField(null=True, blank=True)\n\n    def __unicode__(self):\n        \"\"\"Return either {type} {start} or {type} {start} {end} for unicode representation.\"\"\"\n        if self.end_date:\n            return \"{type} {start} {end}\".format(type=self.type, start=self.start_date,\n                                                 end=self.end_date)\n        return \"{type} {start}\".format(type=self.type, start=self.start_date)\n\n    class Meta:\n        \"\"\"Define meta properties for Date class.\"\"\"\n\n        unique_together = (\"type\", \"content_type\", \"object_id\")\n\n    def rdf_triples(self, subject, graph):\n        date_node = BNode()\n        graph.add((subject, self.get_class_term(), date_node))\n        if self.type in [inner[0] for inner in self.DC_DATE_TYPE_CHOICES]:\n            graph.add((date_node, RDF.type, getattr(DCTERMS, self.type)))\n        else:\n            graph.add((date_node, RDF.type, getattr(HSTERMS, self.type)))\n        graph.add((date_node, RDF.value, Literal(self.start_date.isoformat())))\n\n    @classmethod\n    def ingest_rdf(cls, graph, subject, content_object):\n        for _, _, date_node in graph.triples((subject, cls.get_class_term(), None)):\n            type = graph.value(subject=date_node, predicate=RDF.type)\n            value = graph.value(subject=date_node, predicate=RDF.value)\n            if type and value:\n                type = type.split('/')[-1]\n                start_date = parser.parse(str(value))\n                Date.create(type=type, start_date=start_date, content_object=content_object)\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Define custom create method for Date model.\"\"\"\n        if 'type' in kwargs:\n            if not kwargs['type'] in list(dict(cls.DATE_TYPE_CHOICES).keys()):\n                raise ValidationError('Invalid date type:%s' % kwargs['type'])\n\n            # get matching resource\n            metadata_obj = kwargs['content_object']\n            resource = BaseResource.objects.filter(object_id=metadata_obj.id).first()\n\n            if kwargs['type'] != 'valid':\n                if 'end_date' in kwargs:\n                    del kwargs['end_date']\n\n            if 'start_date' in kwargs:\n                if isinstance(kwargs['start_date'], str):\n                    kwargs['start_date'] = parser.parse(kwargs['start_date'])\n            if kwargs['type'] == 'published':\n                if not resource.raccess.published:\n                    raise ValidationError(\"Resource is not published yet.\")\n            if kwargs['type'] == 'reviewStarted':\n                if resource.raccess.review_pending:\n                    raise ValidationError(\"Review is already pending.\")\n            elif kwargs['type'] == 'available':\n                if not resource.raccess.public:\n                    raise ValidationError(\"Resource has not been made public yet.\")\n            elif kwargs['type'] == 'valid':\n                if 'end_date' in kwargs:\n                    if isinstance(kwargs['end_date'], str):\n                        kwargs['end_date'] = parser.parse(kwargs['end_date'])\n                    if kwargs['start_date'] &gt; kwargs['end_date']:\n                        raise ValidationError(\"For date type valid, end date must be a date \"\n                                              \"after the start date.\")\n\n            return super(Date, cls).create(**kwargs)\n\n        else:\n            raise ValidationError(\"Type of date element is missing.\")\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        \"\"\"Define custom update model for Date model.\"\"\"\n        dt = Date.objects.get(id=element_id)\n\n        if 'start_date' in kwargs:\n            if isinstance(kwargs['start_date'], str):\n                kwargs['start_date'] = parser.parse(kwargs['start_date'])\n            if dt.type == 'created':\n                raise ValidationError(\"Resource creation date can't be changed\")\n            elif dt.type == 'modified':\n                dt.start_date = now().isoformat()\n                dt.save()\n            elif dt.type == 'valid':\n                if 'end_date' in kwargs:\n                    if isinstance(kwargs['end_date'], str):\n                        kwargs['end_date'] = parser.parse(kwargs['end_date'])\n                    if kwargs['start_date'] &gt; kwargs['end_date']:\n                        raise ValidationError(\"For date type valid, end date must be a date \"\n                                              \"after the start date.\")\n                    dt.start_date = kwargs['start_date']\n                    dt.end_date = kwargs['end_date']\n                    dt.save()\n                else:\n                    if dt.end_date:\n                        if kwargs['start_date'] &gt; dt.end_date:\n                            raise ValidationError(\"For date type valid, end date must be a date \"\n                                                  \"after the start date.\")\n                    dt.start_date = kwargs['start_date']\n                    dt.save()\n            else:\n                dt.start_date = kwargs['start_date']\n                dt.save()\n        elif dt.type == 'modified':\n            dt.start_date = now().isoformat()\n            dt.save()\n\n    @classmethod\n    def remove(cls, element_id):\n        \"\"\"Define custom remove method for Date model.\"\"\"\n        dt = Date.objects.get(id=element_id)\n\n        if dt.type in ['created', 'modified']:\n            raise ValidationError(\"Date element of type:%s can't be deleted.\" % dt.type)\n\n        dt.delete()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Date.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Date class.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Date class.\"\"\"\n\n    unique_together = (\"type\", \"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Date.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return either {type} {start} or {type} {start} {end} for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return either {type} {start} or {type} {start} {end} for unicode representation.\"\"\"\n    if self.end_date:\n        return \"{type} {start} {end}\".format(type=self.type, start=self.start_date,\n                                             end=self.end_date)\n    return \"{type} {start}\".format(type=self.type, start=self.start_date)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Date.create","title":"<code>create(**kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom create method for Date model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"Define custom create method for Date model.\"\"\"\n    if 'type' in kwargs:\n        if not kwargs['type'] in list(dict(cls.DATE_TYPE_CHOICES).keys()):\n            raise ValidationError('Invalid date type:%s' % kwargs['type'])\n\n        # get matching resource\n        metadata_obj = kwargs['content_object']\n        resource = BaseResource.objects.filter(object_id=metadata_obj.id).first()\n\n        if kwargs['type'] != 'valid':\n            if 'end_date' in kwargs:\n                del kwargs['end_date']\n\n        if 'start_date' in kwargs:\n            if isinstance(kwargs['start_date'], str):\n                kwargs['start_date'] = parser.parse(kwargs['start_date'])\n        if kwargs['type'] == 'published':\n            if not resource.raccess.published:\n                raise ValidationError(\"Resource is not published yet.\")\n        if kwargs['type'] == 'reviewStarted':\n            if resource.raccess.review_pending:\n                raise ValidationError(\"Review is already pending.\")\n        elif kwargs['type'] == 'available':\n            if not resource.raccess.public:\n                raise ValidationError(\"Resource has not been made public yet.\")\n        elif kwargs['type'] == 'valid':\n            if 'end_date' in kwargs:\n                if isinstance(kwargs['end_date'], str):\n                    kwargs['end_date'] = parser.parse(kwargs['end_date'])\n                if kwargs['start_date'] &gt; kwargs['end_date']:\n                    raise ValidationError(\"For date type valid, end date must be a date \"\n                                          \"after the start date.\")\n\n        return super(Date, cls).create(**kwargs)\n\n    else:\n        raise ValidationError(\"Type of date element is missing.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Date.remove","title":"<code>remove(element_id)</code>  <code>classmethod</code>","text":"<p>Define custom remove method for Date model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id):\n    \"\"\"Define custom remove method for Date model.\"\"\"\n    dt = Date.objects.get(id=element_id)\n\n    if dt.type in ['created', 'modified']:\n        raise ValidationError(\"Date element of type:%s can't be deleted.\" % dt.type)\n\n    dt.delete()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Date.update","title":"<code>update(element_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom update model for Date model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef update(cls, element_id, **kwargs):\n    \"\"\"Define custom update model for Date model.\"\"\"\n    dt = Date.objects.get(id=element_id)\n\n    if 'start_date' in kwargs:\n        if isinstance(kwargs['start_date'], str):\n            kwargs['start_date'] = parser.parse(kwargs['start_date'])\n        if dt.type == 'created':\n            raise ValidationError(\"Resource creation date can't be changed\")\n        elif dt.type == 'modified':\n            dt.start_date = now().isoformat()\n            dt.save()\n        elif dt.type == 'valid':\n            if 'end_date' in kwargs:\n                if isinstance(kwargs['end_date'], str):\n                    kwargs['end_date'] = parser.parse(kwargs['end_date'])\n                if kwargs['start_date'] &gt; kwargs['end_date']:\n                    raise ValidationError(\"For date type valid, end date must be a date \"\n                                          \"after the start date.\")\n                dt.start_date = kwargs['start_date']\n                dt.end_date = kwargs['end_date']\n                dt.save()\n            else:\n                if dt.end_date:\n                    if kwargs['start_date'] &gt; dt.end_date:\n                        raise ValidationError(\"For date type valid, end date must be a date \"\n                                              \"after the start date.\")\n                dt.start_date = kwargs['start_date']\n                dt.save()\n        else:\n            dt.start_date = kwargs['start_date']\n            dt.save()\n    elif dt.type == 'modified':\n        dt.start_date = now().isoformat()\n        dt.save()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Description","title":"<code>Description</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define Description metadata element model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.description, abstract=DCTERMS.abstract)\nclass Description(AbstractMetaDataElement):\n    \"\"\"Define Description metadata element model.\"\"\"\n\n    term = 'Description'\n    abstract = models.TextField(validators=[validate_abstract])\n\n    def __unicode__(self):\n        \"\"\"Return abstract field for unicode representation.\"\"\"\n        return self.abstract\n\n    class Meta:\n        \"\"\"Define meta properties for Description model.\"\"\"\n\n        unique_together = (\"content_type\", \"object_id\")\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Define custom update method for Description model.\"\"\"\n        kwargs['abstract'] = clean_abstract(kwargs['abstract'])\n        return super(Description, cls).create(**kwargs)\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        \"\"\"Define custom update method for Description model.\"\"\"\n        kwargs['abstract'] = clean_abstract(kwargs['abstract'])\n        super(Description, cls).update(element_id, **kwargs)\n\n    @classmethod\n    def remove(cls, element_id):\n        \"\"\"Create custom remove method for Description model.\"\"\"\n        raise ValidationError(\"Description element of a resource can't be deleted.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Description.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Description model.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Description model.\"\"\"\n\n    unique_together = (\"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Description.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return abstract field for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return abstract field for unicode representation.\"\"\"\n    return self.abstract\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Description.create","title":"<code>create(**kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom update method for Description model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"Define custom update method for Description model.\"\"\"\n    kwargs['abstract'] = clean_abstract(kwargs['abstract'])\n    return super(Description, cls).create(**kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Description.remove","title":"<code>remove(element_id)</code>  <code>classmethod</code>","text":"<p>Create custom remove method for Description model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id):\n    \"\"\"Create custom remove method for Description model.\"\"\"\n    raise ValidationError(\"Description element of a resource can't be deleted.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Description.update","title":"<code>update(element_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom update method for Description model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef update(cls, element_id, **kwargs):\n    \"\"\"Define custom update method for Description model.\"\"\"\n    kwargs['abstract'] = clean_abstract(kwargs['abstract'])\n    super(Description, cls).update(element_id, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.DiscoverableResourceManager","title":"<code>DiscoverableResourceManager</code>","text":"<p>               Bases: <code>Manager</code></p> <p>Extend Django model Manager to filter for public or discoverable resources.</p> Source code in <code>hs_core/models.py</code> <pre><code>class DiscoverableResourceManager(models.Manager):\n    \"\"\"Extend Django model Manager to filter for public or discoverable resources.\"\"\"\n\n    def get_queryset(self):\n        \"\"\"Extend Django model Manager to filter for public or discoverable resources.\"\"\"\n        return super(DiscoverableResourceManager, self).get_queryset().filter(\n            Q(raccess__discoverable=True)\n            | Q(raccess__public=True))\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.DiscoverableResourceManager.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Extend Django model Manager to filter for public or discoverable resources.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_queryset(self):\n    \"\"\"Extend Django model Manager to filter for public or discoverable resources.\"\"\"\n    return super(DiscoverableResourceManager, self).get_queryset().filter(\n        Q(raccess__discoverable=True)\n        | Q(raccess__public=True))\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Format","title":"<code>Format</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define Format custom metadata element model.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Format(AbstractMetaDataElement):\n    \"\"\"Define Format custom metadata element model.\"\"\"\n\n    term = 'Format'\n    value = models.CharField(max_length=150)\n\n    class Meta:\n        \"\"\"Define meta properties for Format model.\"\"\"\n\n        unique_together = (\"value\", \"content_type\", \"object_id\")\n\n    def __unicode__(self):\n        \"\"\"Return value field for unicode representation.\"\"\"\n        return self.value\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Format.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Format model.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Format model.\"\"\"\n\n    unique_together = (\"value\", \"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Format.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return value field for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return value field for unicode representation.\"\"\"\n    return self.value\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.FundingAgency","title":"<code>FundingAgency</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define FundingAgency custom metadata element mode.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(HSTERMS.awardInfo, agency_name=HSTERMS.fundingAgencyName, award_title=HSTERMS.awardTitle,\n           award_number=HSTERMS.awardNumber, agency_url=HSTERMS.fundingAgencyURL)\nclass FundingAgency(AbstractMetaDataElement):\n    \"\"\"Define FundingAgency custom metadata element mode.\"\"\"\n\n    term = 'FundingAgency'\n    agency_name = models.TextField(null=False)\n    award_title = models.TextField(null=True, blank=True)\n    award_number = models.TextField(null=True, blank=True)\n    agency_url = models.URLField(null=True, blank=True)\n\n    def __unicode__(self):\n        \"\"\"Return agency_name field for unicode representation.\"\"\"\n        return self.agency_name\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Define custom create method for FundingAgency model.\"\"\"\n        agency_name = kwargs.get('agency_name', None)\n        if agency_name is None or len(agency_name.strip()) == 0:\n            raise ValidationError(\"Agency name is missing\")\n\n        return super(FundingAgency, cls).create(**kwargs)\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        \"\"\"Define custom update method for Agency model.\"\"\"\n        agency_name = kwargs.get('agency_name', None)\n        if agency_name and len(agency_name.strip()) == 0:\n            raise ValidationError(\"Agency name is missing\")\n\n        super(FundingAgency, cls).update(element_id, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.FundingAgency.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return agency_name field for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return agency_name field for unicode representation.\"\"\"\n    return self.agency_name\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.FundingAgency.create","title":"<code>create(**kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom create method for FundingAgency model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"Define custom create method for FundingAgency model.\"\"\"\n    agency_name = kwargs.get('agency_name', None)\n    if agency_name is None or len(agency_name.strip()) == 0:\n        raise ValidationError(\"Agency name is missing\")\n\n    return super(FundingAgency, cls).create(**kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.FundingAgency.update","title":"<code>update(element_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom update method for Agency model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef update(cls, element_id, **kwargs):\n    \"\"\"Define custom update method for Agency model.\"\"\"\n    agency_name = kwargs.get('agency_name', None)\n    if agency_name and len(agency_name.strip()) == 0:\n        raise ValidationError(\"Agency name is missing\")\n\n    super(FundingAgency, cls).update(element_id, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.GroupOwnership","title":"<code>GroupOwnership</code>","text":"<p>               Bases: <code>Model</code></p> <p>Define lookup table allowing django auth users to own django auth groups.</p> Source code in <code>hs_core/models.py</code> <pre><code>class GroupOwnership(models.Model):\n    \"\"\"Define lookup table allowing django auth users to own django auth groups.\"\"\"\n\n    group = models.ForeignKey(Group, on_delete=models.CASCADE)\n    owner = models.ForeignKey(User, on_delete=models.CASCADE)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.HSAdaptorEditInline","title":"<code>HSAdaptorEditInline</code>","text":"<p>               Bases: <code>object</code></p> <p>Define permissions-based helper to determine if user can edit adapter field.</p> <p>Adaptor class added for Django inplace editing to honor HydroShare user-resource permissions</p> Source code in <code>hs_core/models.py</code> <pre><code>class HSAdaptorEditInline(object):\n    \"\"\"Define permissions-based helper to determine if user can edit adapter field.\n\n    Adaptor class added for Django inplace editing to honor HydroShare user-resource permissions\n    \"\"\"\n\n    @classmethod\n    def can_edit(cls, adaptor_field):\n        \"\"\"Define permissions-based helper to determine if user can edit adapter field.\"\"\"\n        obj = adaptor_field.obj\n        cm = obj.get_content_model()\n        return cm.can_change(adaptor_field.request)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.HSAdaptorEditInline.can_edit","title":"<code>can_edit(adaptor_field)</code>  <code>classmethod</code>","text":"<p>Define permissions-based helper to determine if user can edit adapter field.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef can_edit(cls, adaptor_field):\n    \"\"\"Define permissions-based helper to determine if user can edit adapter field.\"\"\"\n    obj = adaptor_field.obj\n    cm = obj.get_content_model()\n    return cm.can_change(adaptor_field.request)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Identifier","title":"<code>Identifier</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Create Identifier custom metadata element.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.identifier)\nclass Identifier(AbstractMetaDataElement):\n    \"\"\"Create Identifier custom metadata element.\"\"\"\n\n    term = 'Identifier'\n    name = models.CharField(max_length=100)\n    url = models.URLField(unique=True)\n\n    def __unicode__(self):\n        \"\"\"Return {name} {url} for unicode representation.\"\"\"\n        return \"{name} {url}\".format(name=self.name, url=self.url)\n\n    def rdf_triples(self, subject, graph):\n        identifier_node = BNode()\n        graph.add((subject, self.get_class_term(), identifier_node))\n        if self.name.lower() == 'doi':\n            graph.add((identifier_node, HSTERMS.doi, URIRef(self.url)))\n        else:\n            graph.add((identifier_node, HSTERMS.hydroShareIdentifier, URIRef(self.url)))\n\n    @classmethod\n    def ingest_rdf(cls, graph, subject, content_object):\n        for _, _, identifier_node in graph.triples((subject, cls.get_class_term(), None)):\n            url = graph.value(subject=identifier_node, predicate=HSTERMS.doi)\n            name = 'doi'\n            if not url:\n                name = 'hydroShareIdentifier'\n                url = graph.value(subject=identifier_node, predicate=HSTERMS.hydroShareIdentifier)\n                if url:\n                    # overwrite hydroShareIdentifier url with this resource's url\n                    url = content_object.rdf_subject()\n            if url:\n                Identifier.create(url=str(url), name=name, content_object=content_object)\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Define custom create method for Identifier model.\"\"\"\n        if 'name' in kwargs:\n            metadata_obj = kwargs['content_object']\n            # get matching resource\n            resource = BaseResource.objects.filter(object_id=metadata_obj.id).first()\n            metadata_type = ContentType.objects.get_for_model(metadata_obj)\n            # check the identifier name doesn't already exist - identifier name\n            # needs to be unique per resource\n            idf = Identifier.objects.filter(name__iexact=kwargs['name'],\n                                            object_id=metadata_obj.id,\n                                            content_type=metadata_type).first()\n            if idf:\n                raise ValidationError('Identifier name:%s already exists' % kwargs['name'])\n            if kwargs['name'].lower() == 'doi':\n                if not resource.doi:\n                    raise ValidationError(\"Identifier of 'DOI' type can't be created for a \"\n                                          \"resource that has not been assigned a DOI yet.\")\n\n            return super(Identifier, cls).create(**kwargs)\n\n        else:\n            raise ValidationError(\"Name of identifier element is missing.\")\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        \"\"\"Define custom update method for Identifier model.\"\"\"\n        idf = Identifier.objects.get(id=element_id)\n\n        if 'name' in kwargs:\n            if idf.name.lower() != kwargs['name'].lower():\n                if idf.name.lower() == 'hydroshareidentifier':\n                    if 'migration' not in kwargs:\n                        raise ValidationError(\"Identifier name 'hydroshareIdentifier' can't \"\n                                              \"be changed.\")\n\n                if idf.name.lower() == 'doi':\n                    raise ValidationError(\"Identifier name 'DOI' can't be changed.\")\n\n                # check this new identifier name not already exists\n                if Identifier.objects.filter(name__iexact=kwargs['name'], object_id=idf.object_id,\n                                             content_type__pk=idf.content_type.id).count() &gt; 0:\n                    if 'migration' not in kwargs:\n                        raise ValidationError('Identifier name:%s already exists.'\n                                              % kwargs['name'])\n\n        if 'url' in kwargs:\n            if idf.url.lower() != kwargs['url'].lower():\n                if idf.name.lower() == 'hydroshareidentifier':\n                    if 'migration' not in kwargs:\n                        raise ValidationError(\"Hydroshare identifier url value can't be changed.\")\n\n                # check this new identifier url not already exists\n                if Identifier.objects.filter(url__iexact=kwargs['url'], object_id=idf.object_id,\n                                             content_type__pk=idf.content_type.id).count() &gt; 0:\n                    raise ValidationError('Identifier URL:%s already exists.' % kwargs['url'])\n\n        super(Identifier, cls).update(element_id, **kwargs)\n\n    @classmethod\n    def remove(cls, element_id):\n        \"\"\"Define custom remove method for Idenfitier method.\"\"\"\n        idf = Identifier.objects.get(id=element_id)\n\n        # get matching resource\n        resource = BaseResource.objects.filter(object_id=idf.content_object.id).first()\n        if idf.name.lower() == 'hydroshareidentifier':\n            raise ValidationError(\"Hydroshare identifier:%s can't be deleted.\" % idf.name)\n\n        if idf.name.lower() == 'doi':\n            if resource.doi:\n                raise ValidationError(\"Hydroshare identifier:%s can't be deleted for a resource \"\n                                      \"that has been assigned a DOI.\" % idf.name)\n        idf.delete()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Identifier.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return {name} {url} for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return {name} {url} for unicode representation.\"\"\"\n    return \"{name} {url}\".format(name=self.name, url=self.url)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Identifier.create","title":"<code>create(**kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom create method for Identifier model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"Define custom create method for Identifier model.\"\"\"\n    if 'name' in kwargs:\n        metadata_obj = kwargs['content_object']\n        # get matching resource\n        resource = BaseResource.objects.filter(object_id=metadata_obj.id).first()\n        metadata_type = ContentType.objects.get_for_model(metadata_obj)\n        # check the identifier name doesn't already exist - identifier name\n        # needs to be unique per resource\n        idf = Identifier.objects.filter(name__iexact=kwargs['name'],\n                                        object_id=metadata_obj.id,\n                                        content_type=metadata_type).first()\n        if idf:\n            raise ValidationError('Identifier name:%s already exists' % kwargs['name'])\n        if kwargs['name'].lower() == 'doi':\n            if not resource.doi:\n                raise ValidationError(\"Identifier of 'DOI' type can't be created for a \"\n                                      \"resource that has not been assigned a DOI yet.\")\n\n        return super(Identifier, cls).create(**kwargs)\n\n    else:\n        raise ValidationError(\"Name of identifier element is missing.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Identifier.remove","title":"<code>remove(element_id)</code>  <code>classmethod</code>","text":"<p>Define custom remove method for Idenfitier method.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id):\n    \"\"\"Define custom remove method for Idenfitier method.\"\"\"\n    idf = Identifier.objects.get(id=element_id)\n\n    # get matching resource\n    resource = BaseResource.objects.filter(object_id=idf.content_object.id).first()\n    if idf.name.lower() == 'hydroshareidentifier':\n        raise ValidationError(\"Hydroshare identifier:%s can't be deleted.\" % idf.name)\n\n    if idf.name.lower() == 'doi':\n        if resource.doi:\n            raise ValidationError(\"Hydroshare identifier:%s can't be deleted for a resource \"\n                                  \"that has been assigned a DOI.\" % idf.name)\n    idf.delete()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Identifier.update","title":"<code>update(element_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom update method for Identifier model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef update(cls, element_id, **kwargs):\n    \"\"\"Define custom update method for Identifier model.\"\"\"\n    idf = Identifier.objects.get(id=element_id)\n\n    if 'name' in kwargs:\n        if idf.name.lower() != kwargs['name'].lower():\n            if idf.name.lower() == 'hydroshareidentifier':\n                if 'migration' not in kwargs:\n                    raise ValidationError(\"Identifier name 'hydroshareIdentifier' can't \"\n                                          \"be changed.\")\n\n            if idf.name.lower() == 'doi':\n                raise ValidationError(\"Identifier name 'DOI' can't be changed.\")\n\n            # check this new identifier name not already exists\n            if Identifier.objects.filter(name__iexact=kwargs['name'], object_id=idf.object_id,\n                                         content_type__pk=idf.content_type.id).count() &gt; 0:\n                if 'migration' not in kwargs:\n                    raise ValidationError('Identifier name:%s already exists.'\n                                          % kwargs['name'])\n\n    if 'url' in kwargs:\n        if idf.url.lower() != kwargs['url'].lower():\n            if idf.name.lower() == 'hydroshareidentifier':\n                if 'migration' not in kwargs:\n                    raise ValidationError(\"Hydroshare identifier url value can't be changed.\")\n\n            # check this new identifier url not already exists\n            if Identifier.objects.filter(url__iexact=kwargs['url'], object_id=idf.object_id,\n                                         content_type__pk=idf.content_type.id).count() &gt; 0:\n                raise ValidationError('Identifier URL:%s already exists.' % kwargs['url'])\n\n    super(Identifier, cls).update(element_id, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Language","title":"<code>Language</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define language custom metadata model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.language)\nclass Language(AbstractMetaDataElement):\n    \"\"\"Define language custom metadata model.\"\"\"\n\n    term = 'Language'\n    code = models.CharField(max_length=7, choices=iso_languages)\n\n    class Meta:\n        \"\"\"Define meta properties for Language model.\"\"\"\n\n        unique_together = (\"content_type\", \"object_id\")\n\n    def __unicode__(self):\n        \"\"\"Return code field for unicode representation.\"\"\"\n        return self.code\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Define custom create method for Language model.\"\"\"\n        if 'code' in kwargs:\n            # check the code is a valid code\n            if not [t for t in iso_languages if t[0] == kwargs['code']]:\n                raise ValidationError('Invalid language code:%s' % kwargs['code'])\n\n            return super(Language, cls).create(**kwargs)\n        else:\n            raise ValidationError(\"Language code is missing.\")\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        \"\"\"Define custom update method for Language model.\"\"\"\n        if 'code' in kwargs:\n            # validate language code\n            if not [t for t in iso_languages if t[0] == kwargs['code']]:\n                raise ValidationError('Invalid language code:%s' % kwargs['code'])\n\n            super(Language, cls).update(element_id, **kwargs)\n        else:\n            raise ValidationError('Language code is missing.')\n\n    def rdf_triples(self, subject, graph):\n        graph.add((subject, self.get_class_term(), Literal(self.code)))\n\n    @classmethod\n    def ingest_rdf(cls, graph, subject, content_object):\n        code = graph.value(subject=subject, predicate=cls.get_class_term())\n        if code:\n            Language.create(code=str(code), content_object=content_object)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Language.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Language model.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Language model.\"\"\"\n\n    unique_together = (\"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Language.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return code field for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return code field for unicode representation.\"\"\"\n    return self.code\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Language.create","title":"<code>create(**kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom create method for Language model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"Define custom create method for Language model.\"\"\"\n    if 'code' in kwargs:\n        # check the code is a valid code\n        if not [t for t in iso_languages if t[0] == kwargs['code']]:\n            raise ValidationError('Invalid language code:%s' % kwargs['code'])\n\n        return super(Language, cls).create(**kwargs)\n    else:\n        raise ValidationError(\"Language code is missing.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Language.update","title":"<code>update(element_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom update method for Language model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef update(cls, element_id, **kwargs):\n    \"\"\"Define custom update method for Language model.\"\"\"\n    if 'code' in kwargs:\n        # validate language code\n        if not [t for t in iso_languages if t[0] == kwargs['code']]:\n            raise ValidationError('Invalid language code:%s' % kwargs['code'])\n\n        super(Language, cls).update(element_id, **kwargs)\n    else:\n        raise ValidationError('Language code is missing.')\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Party","title":"<code>Party</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define party model to define a person.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Party(AbstractMetaDataElement):\n    \"\"\"Define party model to define a person.\"\"\"\n\n    hydroshare_user_id = models.IntegerField(null=True, blank=True, validators=[validate_hydroshare_user_id])\n    name = models.CharField(max_length=100, null=True, blank=True)\n    organization = models.CharField(max_length=200, null=True, blank=True)\n    email = models.EmailField(null=True, blank=True)\n    address = models.CharField(max_length=250, null=True, blank=True)\n    phone = models.CharField(max_length=25, null=True, blank=True)\n    homepage = models.URLField(null=True, blank=True)\n\n    # flag to track if a creator/contributor is an active hydroshare user\n    # this flag is set by the system based on the field 'hydoshare_user_id'\n    is_active_user = models.BooleanField(default=False)\n\n    # to store one or more external identifier (Google Scholar, ResearchGate, ORCID etc)\n    # each identifier is stored as a key/value pair {name:link}\n    identifiers = HStoreField(default=dict)\n\n    # list of identifiers currently supported\n    supported_identifiers = {'ResearchGateID':\n                             re.compile(r'^https:\\/\\/www\\.researchgate\\.net\\/profile\\/[^\\s]+$'),\n                             'ORCID':\n                             re.compile(r'^https:\\/\\/orcid\\.org\\/[0-9]{4}-[0-9]{4}-[0-9]{4}-[0-9]{4}$'),\n                             'GoogleScholarID':\n                             re.compile(r'^https:\\/\\/scholar\\.google\\.com\\/citations\\?.*user=[^\\s]+$'),\n                             'ResearcherID':\n                             'https://www.researcherid.com/'}\n\n    def __unicode__(self):\n        \"\"\"Return name field for unicode representation.\"\"\"\n        return self.name\n\n    class Meta:\n        \"\"\"Define meta properties for Party class.\"\"\"\n\n        abstract = True\n\n    def rdf_triples(self, subject, graph):\n        party_type = self.get_class_term()\n        party = BNode()\n        graph.add((subject, party_type, party))\n        for field_term, field_value in self.get_field_terms_and_values(['identifiers', 'is_active_user']):\n            # TODO: remove this once we are no longer concerned with backwards compatibility\n            if field_term == HSTERMS.hydroshare_user_id:\n                graph.add((party, HSTERMS.description, field_value))\n            graph.add((party, field_term, field_value))\n        for k, v in self.identifiers.items():\n            graph.add((party, getattr(HSTERMS, k), URIRef(v)))\n\n    @classmethod\n    def ingest_rdf(cls, graph, subject, content_object):\n        \"\"\"Default implementation that ingests by convention\"\"\"\n        party_type = cls.get_class_term()\n        for party in graph.objects(subject=subject, predicate=party_type):\n            value_dict = {}\n            identifiers = {}\n            fields_by_term = {cls.get_field_term(field.name): field for field in cls._meta.fields}\n            for _, p, o in graph.triples((party, None, None)):\n                # TODO: remove this once we are no longer concerned with backwards compatibility\n                if p == HSTERMS.description:\n                    # parse the description into a hydroshare_user_id\n                    p = HSTERMS.hydroshare_user_id\n                    o = o.split('user/')[-1]\n                    o = o.replace(\"/\", \"\")\n                if p not in fields_by_term:\n                    identifiers[p.rsplit(\"/\", 1)[1]] = str(o)\n                else:\n                    value_dict[fields_by_term[p].name] = str(o)\n            if value_dict or identifiers:\n                if identifiers:\n                    cls.create(content_object=content_object, identifiers=identifiers, **value_dict)\n                else:\n                    cls.create(content_object=content_object, **value_dict)\n\n    @classmethod\n    def get_post_data_with_identifiers(cls, request, as_json=True):\n        identifier_names = request.POST.getlist('identifier_name')\n        identifier_links = request.POST.getlist('identifier_link')\n        identifiers = {}\n        if identifier_links and identifier_names:\n            if len(identifier_names) != len(identifier_links):\n                raise Exception(\"Invalid data for identifiers\")\n            identifiers = dict(list(zip(identifier_names, identifier_links)))\n            if len(identifier_names) != len(list(identifiers.keys())):\n                raise Exception(\"Invalid data for identifiers\")\n\n            if as_json:\n                identifiers = json.dumps(identifiers)\n\n        post_data_dict = request.POST.dict()\n        post_data_dict['identifiers'] = identifiers\n\n        return post_data_dict\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Define custom create method for Party model.\"\"\"\n        element_name = cls.__name__\n\n        identifiers = kwargs.get('identifiers', '')\n        if identifiers:\n            identifiers = cls.validate_identifiers(identifiers)\n            kwargs['identifiers'] = identifiers\n\n        hs_user_id = kwargs.get('hydroshare_user_id', '')\n        if hs_user_id:\n            validate_hydroshare_user_id(hs_user_id)\n\n        metadata_obj = kwargs['content_object']\n        metadata_type = ContentType.objects.get_for_model(metadata_obj)\n        if element_name == 'Creator':\n            party = Creator.objects.filter(object_id=metadata_obj.id,\n                                           content_type=metadata_type).last()\n            creator_order = 1\n            if party:\n                creator_order = party.order + 1\n\n            if ('name' not in kwargs or kwargs['name'] is None) and \\\n                    ('organization' not in kwargs or kwargs['organization'] is None):\n                raise PartyValidationError(\n                    \"Either an organization or name is required for a creator element\")\n\n            if 'name' in kwargs and kwargs['name'] is not None:\n                if len(kwargs['name'].strip()) == 0:\n                    if 'organization' in kwargs and kwargs['organization'] is not None:\n                        if len(kwargs['organization'].strip()) == 0:\n                            raise PartyValidationError(\n                                \"Either the name or organization must not be blank for the creator \"\n                                \"element\")\n\n            if 'order' not in kwargs or kwargs['order'] is None:\n                kwargs['order'] = creator_order\n\n        party = super(Party, cls).create(**kwargs)\n\n        if party.hydroshare_user_id:\n            user = User.objects.get(id=party.hydroshare_user_id)\n            party.is_active_user = user.is_active\n            party.save()\n        return party\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        \"\"\"Define custom update method for Party model.\"\"\"\n        element_name = cls.__name__\n        creator_order = None\n        if 'hydroshare_user_id' in kwargs:\n            party = cls.objects.get(id=element_id)\n            if party.hydroshare_user_id is not None and kwargs['hydroshare_user_id'] is not None:\n                if party.hydroshare_user_id != kwargs['hydroshare_user_id']:\n                    raise PartyValidationError(\"HydroShare user identifier can't be changed.\")\n\n        if 'order' in kwargs and element_name == 'Creator':\n            creator_order = kwargs['order']\n            if creator_order &lt;= 0:\n                creator_order = 1\n            del kwargs['order']\n\n        identifiers = kwargs.get('identifiers', '')\n        if identifiers:\n            identifiers = cls.validate_identifiers(identifiers)\n            kwargs['identifiers'] = identifiers\n\n        party = super(Party, cls).update(element_id, **kwargs)\n        if party.hydroshare_user_id is not None:\n            user = User.objects.get(id=party.hydroshare_user_id)\n            party.is_active_user = user.is_active\n        else:\n            party.is_active_user = False\n        party.save(update_fields=[\"is_active_user\"])\n\n        if isinstance(party, Creator) and creator_order is not None:\n            if party.order != creator_order:\n                resource_creators = Creator.objects.filter(\n                    object_id=party.object_id, content_type__pk=party.content_type.id).all()\n\n                if creator_order &gt; len(resource_creators):\n                    creator_order = len(resource_creators)\n\n                for res_cr in resource_creators:\n                    if party.order &gt; creator_order:\n                        if res_cr.order &lt; party.order and not res_cr.order &lt; creator_order:\n                            res_cr.order += 1\n                            res_cr.save(update_fields=[\"order\"])\n                    else:\n                        if res_cr.order &gt; party.order and res_cr.order &lt;= creator_order:\n                            res_cr.order -= 1\n                            res_cr.save(update_fields=[\"order\"])\n\n                party.order = creator_order\n                party.save(update_fields=[\"order\"])\n\n    @property\n    def relative_uri(self):\n        return f\"/user/{self.hydroshare_user_id}/\" if self.hydroshare_user_id else None\n\n    @property\n    def is_active(self):\n        return self.is_active_user\n\n    @classmethod\n    def remove(cls, element_id, delete=True):\n        \"\"\"Define custom remove method for Party model.\"\"\"\n        party = cls.objects.get(id=element_id)\n\n        # if we are deleting a creator, then we have to update the order attribute of remaining\n        # creators associated with a resource\n        # make sure we are not deleting all creators of a resource\n        if isinstance(party, Creator):\n            if Creator.objects.filter(object_id=party.object_id,\n                                      content_type__pk=party.content_type.id).count() == 1:\n                raise PartyValidationError(\"The only creator of the resource can't be deleted.\")\n\n            creators_to_update = Creator.objects.filter(\n                object_id=party.object_id,\n                content_type__pk=party.content_type.id).exclude(order=party.order).all()\n\n            for cr in creators_to_update:\n                if cr.order &gt; party.order:\n                    cr.order -= 1\n                    cr.save(update_fields=[\"order\"])\n        if delete:\n            party.delete()\n\n    def delete(self, using=None, keep_parents=False):\n        \"\"\"Overriding the django model delete() method to update creator order attribute for\n        remaining creators.\"\"\"\n        self.remove(element_id=self.id, delete=False)\n        super(Party, self).delete(using=using, keep_parents=keep_parents)\n\n    @classmethod\n    def validate_identifiers(cls, identifiers):\n        \"\"\"Validates optional identifiers for user/creator/contributor\n        :param  identifiers: identifier data as a json string or as a dict\n        \"\"\"\n\n        if not isinstance(identifiers, dict):\n            if identifiers:\n                # validation form can populate the dict(kwargs) with key 'identifiers\" with\n                # value of empty string if data passed to the validation form did not had this\n                # key. In that case no need to convert the string to dict\n                try:\n                    identifiers = json.loads(identifiers)\n                except ValueError:\n                    raise PartyValidationError(\"Value for identifiers not in the correct format\")\n        # identifiers = kwargs['identifiers']\n        if identifiers:\n            # validate the identifiers are one of the supported ones\n            for name in identifiers:\n                if name not in cls.supported_identifiers:\n                    raise PartyValidationError(\"Invalid data found for identifiers. \"\n                                               \"{} not a supported identifier.\". format(name))\n            # validate identifier values - check for duplicate links\n            links = [link.lower() for link in list(identifiers.values())]\n            if len(links) != len(set(links)):\n                raise PartyValidationError(\"Invalid data found for identifiers. \"\n                                           \"Duplicate identifier links found.\")\n\n            for link in links:\n                validator = URLValidator()\n                try:\n                    validator(link)\n                except ValidationError:\n                    raise PartyValidationError(\"Invalid data found for identifiers. \"\n                                               \"Identifier link must be a URL.\")\n\n            # validate identifier keys - check for duplicate names\n            names = [n.lower() for n in list(identifiers.keys())]\n            if len(names) != len(set(names)):\n                raise PartyValidationError(\"Invalid data found for identifiers. \"\n                                           \"Duplicate identifier names found\")\n\n            # validate that the links for the known identifiers are valid\n            for id_name in cls.supported_identifiers:\n                id_link = identifiers.get(id_name, '')\n                if id_link:\n                    regex = cls.supported_identifiers[id_name]\n                    if not re.match(regex, id_link):\n                        raise PartyValidationError(\"Invalid data found for identifiers. \"\n                                                   f\"\\'{id_link}\\' is not a valid {id_name}.\")\n        return identifiers\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Party.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Party class.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Party class.\"\"\"\n\n    abstract = True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Party.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return name field for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return name field for unicode representation.\"\"\"\n    return self.name\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Party.create","title":"<code>create(**kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom create method for Party model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"Define custom create method for Party model.\"\"\"\n    element_name = cls.__name__\n\n    identifiers = kwargs.get('identifiers', '')\n    if identifiers:\n        identifiers = cls.validate_identifiers(identifiers)\n        kwargs['identifiers'] = identifiers\n\n    hs_user_id = kwargs.get('hydroshare_user_id', '')\n    if hs_user_id:\n        validate_hydroshare_user_id(hs_user_id)\n\n    metadata_obj = kwargs['content_object']\n    metadata_type = ContentType.objects.get_for_model(metadata_obj)\n    if element_name == 'Creator':\n        party = Creator.objects.filter(object_id=metadata_obj.id,\n                                       content_type=metadata_type).last()\n        creator_order = 1\n        if party:\n            creator_order = party.order + 1\n\n        if ('name' not in kwargs or kwargs['name'] is None) and \\\n                ('organization' not in kwargs or kwargs['organization'] is None):\n            raise PartyValidationError(\n                \"Either an organization or name is required for a creator element\")\n\n        if 'name' in kwargs and kwargs['name'] is not None:\n            if len(kwargs['name'].strip()) == 0:\n                if 'organization' in kwargs and kwargs['organization'] is not None:\n                    if len(kwargs['organization'].strip()) == 0:\n                        raise PartyValidationError(\n                            \"Either the name or organization must not be blank for the creator \"\n                            \"element\")\n\n        if 'order' not in kwargs or kwargs['order'] is None:\n            kwargs['order'] = creator_order\n\n    party = super(Party, cls).create(**kwargs)\n\n    if party.hydroshare_user_id:\n        user = User.objects.get(id=party.hydroshare_user_id)\n        party.is_active_user = user.is_active\n        party.save()\n    return party\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Party.delete","title":"<code>delete(using=None, keep_parents=False)</code>","text":"<p>Overriding the django model delete() method to update creator order attribute for remaining creators.</p> Source code in <code>hs_core/models.py</code> <pre><code>def delete(self, using=None, keep_parents=False):\n    \"\"\"Overriding the django model delete() method to update creator order attribute for\n    remaining creators.\"\"\"\n    self.remove(element_id=self.id, delete=False)\n    super(Party, self).delete(using=using, keep_parents=keep_parents)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Party.ingest_rdf","title":"<code>ingest_rdf(graph, subject, content_object)</code>  <code>classmethod</code>","text":"<p>Default implementation that ingests by convention</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef ingest_rdf(cls, graph, subject, content_object):\n    \"\"\"Default implementation that ingests by convention\"\"\"\n    party_type = cls.get_class_term()\n    for party in graph.objects(subject=subject, predicate=party_type):\n        value_dict = {}\n        identifiers = {}\n        fields_by_term = {cls.get_field_term(field.name): field for field in cls._meta.fields}\n        for _, p, o in graph.triples((party, None, None)):\n            # TODO: remove this once we are no longer concerned with backwards compatibility\n            if p == HSTERMS.description:\n                # parse the description into a hydroshare_user_id\n                p = HSTERMS.hydroshare_user_id\n                o = o.split('user/')[-1]\n                o = o.replace(\"/\", \"\")\n            if p not in fields_by_term:\n                identifiers[p.rsplit(\"/\", 1)[1]] = str(o)\n            else:\n                value_dict[fields_by_term[p].name] = str(o)\n        if value_dict or identifiers:\n            if identifiers:\n                cls.create(content_object=content_object, identifiers=identifiers, **value_dict)\n            else:\n                cls.create(content_object=content_object, **value_dict)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Party.remove","title":"<code>remove(element_id, delete=True)</code>  <code>classmethod</code>","text":"<p>Define custom remove method for Party model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id, delete=True):\n    \"\"\"Define custom remove method for Party model.\"\"\"\n    party = cls.objects.get(id=element_id)\n\n    # if we are deleting a creator, then we have to update the order attribute of remaining\n    # creators associated with a resource\n    # make sure we are not deleting all creators of a resource\n    if isinstance(party, Creator):\n        if Creator.objects.filter(object_id=party.object_id,\n                                  content_type__pk=party.content_type.id).count() == 1:\n            raise PartyValidationError(\"The only creator of the resource can't be deleted.\")\n\n        creators_to_update = Creator.objects.filter(\n            object_id=party.object_id,\n            content_type__pk=party.content_type.id).exclude(order=party.order).all()\n\n        for cr in creators_to_update:\n            if cr.order &gt; party.order:\n                cr.order -= 1\n                cr.save(update_fields=[\"order\"])\n    if delete:\n        party.delete()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Party.update","title":"<code>update(element_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom update method for Party model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef update(cls, element_id, **kwargs):\n    \"\"\"Define custom update method for Party model.\"\"\"\n    element_name = cls.__name__\n    creator_order = None\n    if 'hydroshare_user_id' in kwargs:\n        party = cls.objects.get(id=element_id)\n        if party.hydroshare_user_id is not None and kwargs['hydroshare_user_id'] is not None:\n            if party.hydroshare_user_id != kwargs['hydroshare_user_id']:\n                raise PartyValidationError(\"HydroShare user identifier can't be changed.\")\n\n    if 'order' in kwargs and element_name == 'Creator':\n        creator_order = kwargs['order']\n        if creator_order &lt;= 0:\n            creator_order = 1\n        del kwargs['order']\n\n    identifiers = kwargs.get('identifiers', '')\n    if identifiers:\n        identifiers = cls.validate_identifiers(identifiers)\n        kwargs['identifiers'] = identifiers\n\n    party = super(Party, cls).update(element_id, **kwargs)\n    if party.hydroshare_user_id is not None:\n        user = User.objects.get(id=party.hydroshare_user_id)\n        party.is_active_user = user.is_active\n    else:\n        party.is_active_user = False\n    party.save(update_fields=[\"is_active_user\"])\n\n    if isinstance(party, Creator) and creator_order is not None:\n        if party.order != creator_order:\n            resource_creators = Creator.objects.filter(\n                object_id=party.object_id, content_type__pk=party.content_type.id).all()\n\n            if creator_order &gt; len(resource_creators):\n                creator_order = len(resource_creators)\n\n            for res_cr in resource_creators:\n                if party.order &gt; creator_order:\n                    if res_cr.order &lt; party.order and not res_cr.order &lt; creator_order:\n                        res_cr.order += 1\n                        res_cr.save(update_fields=[\"order\"])\n                else:\n                    if res_cr.order &gt; party.order and res_cr.order &lt;= creator_order:\n                        res_cr.order -= 1\n                        res_cr.save(update_fields=[\"order\"])\n\n            party.order = creator_order\n            party.save(update_fields=[\"order\"])\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Party.validate_identifiers","title":"<code>validate_identifiers(identifiers)</code>  <code>classmethod</code>","text":"<p>Validates optional identifiers for user/creator/contributor :param  identifiers: identifier data as a json string or as a dict</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef validate_identifiers(cls, identifiers):\n    \"\"\"Validates optional identifiers for user/creator/contributor\n    :param  identifiers: identifier data as a json string or as a dict\n    \"\"\"\n\n    if not isinstance(identifiers, dict):\n        if identifiers:\n            # validation form can populate the dict(kwargs) with key 'identifiers\" with\n            # value of empty string if data passed to the validation form did not had this\n            # key. In that case no need to convert the string to dict\n            try:\n                identifiers = json.loads(identifiers)\n            except ValueError:\n                raise PartyValidationError(\"Value for identifiers not in the correct format\")\n    # identifiers = kwargs['identifiers']\n    if identifiers:\n        # validate the identifiers are one of the supported ones\n        for name in identifiers:\n            if name not in cls.supported_identifiers:\n                raise PartyValidationError(\"Invalid data found for identifiers. \"\n                                           \"{} not a supported identifier.\". format(name))\n        # validate identifier values - check for duplicate links\n        links = [link.lower() for link in list(identifiers.values())]\n        if len(links) != len(set(links)):\n            raise PartyValidationError(\"Invalid data found for identifiers. \"\n                                       \"Duplicate identifier links found.\")\n\n        for link in links:\n            validator = URLValidator()\n            try:\n                validator(link)\n            except ValidationError:\n                raise PartyValidationError(\"Invalid data found for identifiers. \"\n                                           \"Identifier link must be a URL.\")\n\n        # validate identifier keys - check for duplicate names\n        names = [n.lower() for n in list(identifiers.keys())]\n        if len(names) != len(set(names)):\n            raise PartyValidationError(\"Invalid data found for identifiers. \"\n                                       \"Duplicate identifier names found\")\n\n        # validate that the links for the known identifiers are valid\n        for id_name in cls.supported_identifiers:\n            id_link = identifiers.get(id_name, '')\n            if id_link:\n                regex = cls.supported_identifiers[id_name]\n                if not re.match(regex, id_link):\n                    raise PartyValidationError(\"Invalid data found for identifiers. \"\n                                               f\"\\'{id_link}\\' is not a valid {id_name}.\")\n    return identifiers\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.PublicResourceManager","title":"<code>PublicResourceManager</code>","text":"<p>               Bases: <code>Manager</code></p> <p>Extend Django model Manager to allow for public resource access.</p> Source code in <code>hs_core/models.py</code> <pre><code>class PublicResourceManager(models.Manager):\n    \"\"\"Extend Django model Manager to allow for public resource access.\"\"\"\n\n    def get_queryset(self):\n        \"\"\"Extend Django model Manager to allow for public resource access.\"\"\"\n        return super(PublicResourceManager, self).get_queryset().filter(raccess__public=True)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.PublicResourceManager.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Extend Django model Manager to allow for public resource access.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_queryset(self):\n    \"\"\"Extend Django model Manager to allow for public resource access.\"\"\"\n    return super(PublicResourceManager, self).get_queryset().filter(raccess__public=True)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Publisher","title":"<code>Publisher</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define Publisher custom metadata model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.publisher, name=HSTERMS.publisherName, url=HSTERMS.publisherURL)\nclass Publisher(AbstractMetaDataElement):\n    \"\"\"Define Publisher custom metadata model.\"\"\"\n\n    term = 'Publisher'\n    name = models.CharField(max_length=200)\n    url = models.URLField()\n\n    def __unicode__(self):\n        \"\"\"Return {name} {url} for unicode representation of Publisher model.\"\"\"\n        return \"{name} {url}\".format(name=self.name, url=self.url)\n\n    class Meta:\n        \"\"\"Define meta properties for Publisher model.\"\"\"\n\n        unique_together = (\"content_type\", \"object_id\")\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Define custom create method for Publisher model.\"\"\"\n        metadata_obj = kwargs['content_object']\n        # get matching resource\n        resource = BaseResource.objects.filter(object_id=metadata_obj.id).first()\n        if not resource:\n            raise ValidationError(\"Resource not found\")\n        if not resource.raccess.published:\n            raise ValidationError(\"Publisher element can't be created for a resource that \"\n                                  \"is not yet published.\")\n\n        publisher_CUAHSI = \"Consortium of Universities for the Advancement of Hydrologic \" \\\n                           \"Science, Inc. (CUAHSI)\"\n\n        if resource.files.all():\n            # if the resource has content files, set CUAHSI as the publisher\n            if 'name' in kwargs:\n                if kwargs['name'].lower() != publisher_CUAHSI.lower():\n                    raise ValidationError(\"Invalid publisher name\")\n\n            kwargs['name'] = publisher_CUAHSI\n            if 'url' in kwargs:\n                if kwargs['url'].lower() != 'https://www.cuahsi.org':\n                    raise ValidationError(\"Invalid publisher URL\")\n\n            kwargs['url'] = 'https://www.cuahsi.org'\n        else:\n            # make sure we are not setting CUAHSI as publisher for a resource\n            # that has no content files, unless it is a Collection\n            if resource.resource_type.lower() != \"collectionresource\":\n                if 'name' in kwargs:\n                    if kwargs['name'].lower() == publisher_CUAHSI.lower():\n                        raise ValidationError(\"Invalid publisher name\")\n                if 'url' in kwargs:\n                    if kwargs['url'].lower() == 'https://www.cuahsi.org':\n                        raise ValidationError(\"Invalid publisher URL\")\n\n        return super(Publisher, cls).create(**kwargs)\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        \"\"\"Define custom update method for Publisher model.\"\"\"\n        raise ValidationError(\"Publisher element can't be updated.\")\n\n    @classmethod\n    def remove(cls, element_id):\n        \"\"\"Define custom remove method for Publisher model.\"\"\"\n        raise ValidationError(\"Publisher element can't be deleted.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Publisher.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Publisher model.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Publisher model.\"\"\"\n\n    unique_together = (\"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Publisher.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return {name} {url} for unicode representation of Publisher model.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return {name} {url} for unicode representation of Publisher model.\"\"\"\n    return \"{name} {url}\".format(name=self.name, url=self.url)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Publisher.create","title":"<code>create(**kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom create method for Publisher model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"Define custom create method for Publisher model.\"\"\"\n    metadata_obj = kwargs['content_object']\n    # get matching resource\n    resource = BaseResource.objects.filter(object_id=metadata_obj.id).first()\n    if not resource:\n        raise ValidationError(\"Resource not found\")\n    if not resource.raccess.published:\n        raise ValidationError(\"Publisher element can't be created for a resource that \"\n                              \"is not yet published.\")\n\n    publisher_CUAHSI = \"Consortium of Universities for the Advancement of Hydrologic \" \\\n                       \"Science, Inc. (CUAHSI)\"\n\n    if resource.files.all():\n        # if the resource has content files, set CUAHSI as the publisher\n        if 'name' in kwargs:\n            if kwargs['name'].lower() != publisher_CUAHSI.lower():\n                raise ValidationError(\"Invalid publisher name\")\n\n        kwargs['name'] = publisher_CUAHSI\n        if 'url' in kwargs:\n            if kwargs['url'].lower() != 'https://www.cuahsi.org':\n                raise ValidationError(\"Invalid publisher URL\")\n\n        kwargs['url'] = 'https://www.cuahsi.org'\n    else:\n        # make sure we are not setting CUAHSI as publisher for a resource\n        # that has no content files, unless it is a Collection\n        if resource.resource_type.lower() != \"collectionresource\":\n            if 'name' in kwargs:\n                if kwargs['name'].lower() == publisher_CUAHSI.lower():\n                    raise ValidationError(\"Invalid publisher name\")\n            if 'url' in kwargs:\n                if kwargs['url'].lower() == 'https://www.cuahsi.org':\n                    raise ValidationError(\"Invalid publisher URL\")\n\n    return super(Publisher, cls).create(**kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Publisher.remove","title":"<code>remove(element_id)</code>  <code>classmethod</code>","text":"<p>Define custom remove method for Publisher model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id):\n    \"\"\"Define custom remove method for Publisher model.\"\"\"\n    raise ValidationError(\"Publisher element can't be deleted.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Publisher.update","title":"<code>update(element_id, **kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom update method for Publisher model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef update(cls, element_id, **kwargs):\n    \"\"\"Define custom update method for Publisher model.\"\"\"\n    raise ValidationError(\"Publisher element can't be updated.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.RangedFileReader","title":"<code>RangedFileReader</code>","text":"<p>Wraps a file like object with an iterator that runs over part (or all) of the file defined by start and stop. Blocks of block_size will be returned from the starting position, up to, but not including the stop point. https://github.com/satchamo/django/commit/2ce75c5c4bee2a858c0214d136bfcd351fcde11d</p> Source code in <code>hs_core/models.py</code> <pre><code>class RangedFileReader:\n    \"\"\"\n    Wraps a file like object with an iterator that runs over part (or all) of\n    the file defined by start and stop. Blocks of block_size will be returned\n    from the starting position, up to, but not including the stop point.\n    https://github.com/satchamo/django/commit/2ce75c5c4bee2a858c0214d136bfcd351fcde11d\n    \"\"\"\n    block_size = getattr(settings, 'RANGED_FILE_READER_BLOCK_SIZE', 1024 * 1024)\n    dump_size = getattr(settings, 'RANGED_FILE_READER_DUMP_SIZE', 1024 * 1024 * 1024)\n\n    def __init__(self, file_like, start=0, stop=float(\"inf\"), block_size=None):\n        self.f = file_like\n        self.block_size = block_size or self.block_size\n        self.start = start\n        self.stop = stop\n\n    def __iter__(self):\n        # self.f proc.stdout is an _io.BufferedReader object\n        # so it will not have a seek method\n        if self.f.seekable():\n            self.f.seek(self.start)\n        else:\n            # if the file is not seekable, we read and discard\n            # until we reach the start position\n            remaining_to_dump = self.start\n            while remaining_to_dump &gt; 0:\n                read_size = min(self.dump_size, remaining_to_dump)\n                self.f.read(read_size)\n                remaining_to_dump -= read_size\n        position = self.start\n        while position &lt; self.stop:\n            data = self.f.read(min(self.block_size, self.stop - position))\n            if not data:\n                break\n\n            yield data\n            position += self.block_size\n\n    @staticmethod\n    def parse_range_header(header, resource_size):\n        \"\"\"\n        Parses a range header into a list of two-tuples (start, stop) where `start`\n        is the starting byte of the range (inclusive) and `stop` is the ending byte\n        position of the range (exclusive).\n        Returns None if the value of the header is not syntatically valid.\n        \"\"\"\n        if not header or '=' not in header:\n            return None\n\n        ranges = []\n        units, range_ = header.split('=', 1)\n        units = units.strip().lower()\n\n        if units != \"bytes\":\n            return None\n\n        for val in range_.split(\",\"):\n            val = val.strip()\n            if '-' not in val:\n                return None\n\n            if val.startswith(\"-\"):\n                # suffix-byte-range-spec: this form specifies the last N bytes of an\n                # entity-body\n                start = resource_size + int(val)\n                if start &lt; 0:\n                    start = 0\n                stop = resource_size\n            else:\n                # byte-range-spec: first-byte-pos \"-\" [last-byte-pos]\n                start, stop = val.split(\"-\", 1)\n                start = int(start)\n                # the +1 is here since we want the stopping point to be exclusive, whereas in\n                # the HTTP spec, the last-byte-pos is inclusive\n                stop = int(stop) + 1 if stop else resource_size\n                if start &gt;= stop:\n                    return None\n\n            ranges.append((start, stop))\n\n        return ranges\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.RangedFileReader.parse_range_header","title":"<code>parse_range_header(header, resource_size)</code>  <code>staticmethod</code>","text":"<p>Parses a range header into a list of two-tuples (start, stop) where <code>start</code> is the starting byte of the range (inclusive) and <code>stop</code> is the ending byte position of the range (exclusive). Returns None if the value of the header is not syntatically valid.</p> Source code in <code>hs_core/models.py</code> <pre><code>@staticmethod\ndef parse_range_header(header, resource_size):\n    \"\"\"\n    Parses a range header into a list of two-tuples (start, stop) where `start`\n    is the starting byte of the range (inclusive) and `stop` is the ending byte\n    position of the range (exclusive).\n    Returns None if the value of the header is not syntatically valid.\n    \"\"\"\n    if not header or '=' not in header:\n        return None\n\n    ranges = []\n    units, range_ = header.split('=', 1)\n    units = units.strip().lower()\n\n    if units != \"bytes\":\n        return None\n\n    for val in range_.split(\",\"):\n        val = val.strip()\n        if '-' not in val:\n            return None\n\n        if val.startswith(\"-\"):\n            # suffix-byte-range-spec: this form specifies the last N bytes of an\n            # entity-body\n            start = resource_size + int(val)\n            if start &lt; 0:\n                start = 0\n            stop = resource_size\n        else:\n            # byte-range-spec: first-byte-pos \"-\" [last-byte-pos]\n            start, stop = val.split(\"-\", 1)\n            start = int(start)\n            # the +1 is here since we want the stopping point to be exclusive, whereas in\n            # the HTTP spec, the last-byte-pos is inclusive\n            stop = int(stop) + 1 if stop else resource_size\n            if start &gt;= stop:\n                return None\n\n        ranges.append((start, stop))\n\n    return ranges\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Relation","title":"<code>Relation</code>","text":"<p>               Bases: <code>AbstractRelation</code></p> <p>Define Relation custom metadata model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.relation)\nclass Relation(AbstractRelation):\n    \"\"\"Define Relation custom metadata model.\"\"\"\n\n    SOURCE_TYPES = (\n        (RelationTypes.isPartOf.value, 'The content of this resource is part of'),\n        (RelationTypes.hasPart.value, 'This resource includes'),\n        (RelationTypes.isExecutedBy.value, 'The content of this resource can be executed by'),\n        (RelationTypes.isCreatedBy.value,\n         'The content of this resource was created by a related App or software program'),\n        (RelationTypes.isVersionOf.value, 'This resource updates and replaces a previous version'),\n        (RelationTypes.isReplacedBy.value, 'This resource has been replaced by a newer version'),\n        (RelationTypes.isDescribedBy.value, 'This resource is described by'),\n        (RelationTypes.conformsTo.value, 'This resource conforms to established standard described by'),\n        (RelationTypes.hasFormat.value, 'This resource has a related resource in another format'),\n        (RelationTypes.isFormatOf.value, 'This resource is a different format of'),\n        (RelationTypes.isRequiredBy.value, 'This resource is required by'),\n        (RelationTypes.requires.value, 'This resource requires'),\n        (RelationTypes.isReferencedBy.value, 'This resource is referenced by'),\n        (RelationTypes.references.value, 'The content of this resource references'),\n        (RelationTypes.replaces.value, 'This resource replaces'),\n        (RelationTypes.source.value, 'The content of this resource is derived from'),\n        (RelationTypes.isSimilarTo.value, 'The content of this resource is similar to')\n    )\n\n    # these are hydroshare custom terms that are not Dublin Core terms\n    HS_RELATION_TERMS = (RelationTypes.isExecutedBy, RelationTypes.isCreatedBy, RelationTypes.isDescribedBy,\n                         RelationTypes.isSimilarTo)\n    NOT_USER_EDITABLE = (RelationTypes.isVersionOf, RelationTypes.isReplacedBy,\n                         RelationTypes.isPartOf, RelationTypes.hasPart, RelationTypes.replaces)\n    term = 'Relation'\n    type = models.CharField(max_length=100, choices=SOURCE_TYPES)\n    value = models.TextField()\n\n    @classmethod\n    def create(cls, **kwargs):\n        return super(Relation, cls).create(**kwargs)\n\n    @classmethod\n    def update(cls, element_id, **kwargs):\n        return super(Relation, cls).update(element_id, **kwargs)\n\n    def rdf_triples(self, subject, graph):\n        relation_node = BNode()\n        graph.add((subject, self.get_class_term(), relation_node))\n        if self.type in self.HS_RELATION_TERMS:\n            graph.add((relation_node, getattr(HSTERMS, self.type), Literal(self.value)))\n        else:\n            graph.add((relation_node, getattr(DCTERMS, self.type), Literal(self.value)))\n\n    @classmethod\n    def ingest_rdf(cls, graph, subject, content_object):\n        for _, _, relation_node in graph.triples((subject, cls.get_class_term(), None)):\n            for _, p, o in graph.triples((relation_node, None, None)):\n                type_term = p\n                value = o\n                break\n            if type_term:\n                type = type_term.split('/')[-1]\n                value = str(value)\n                Relation.create(type=type, value=value, content_object=content_object)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile","title":"<code>ResourceFile</code>","text":"<p>               Bases: <code>ResourceFileS3Mixin</code></p> <p>Represent a file in a resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>class ResourceFile(ResourceFileS3Mixin):\n    \"\"\"\n    Represent a file in a resource.\n    \"\"\"\n    class Meta:\n        index_together = [['object_id', 'resource_file'],\n                          ]\n    # A ResourceFile is a sub-object of a resource, which can have several types.\n    object_id = models.PositiveIntegerField()\n    content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE)\n    content_object = GenericForeignKey('content_type', 'object_id')\n\n    # This is used to direct uploads to a subfolder of the root folder for the resource.\n    # See get_path and get_resource_file_path above.\n    file_folder = models.CharField(max_length=4096, null=False, default=\"\")\n\n    # This pair of FileFields deals with the fact that there are two kinds of storage\n    resource_file = models.FileField(upload_to=get_path, max_length=4096, unique=True,\n                                     storage=S3Storage())\n\n    # we are using GenericForeignKey to allow resource file to be associated with any\n    # HydroShare defined LogicalFile types (e.g., GeoRasterFile, NetCdfFile etc)\n    logical_file_object_id = models.PositiveIntegerField(null=True, blank=True)\n    logical_file_content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE,\n                                                  null=True, blank=True,\n                                                  related_name=\"files\")\n    logical_file_content_object = GenericForeignKey('logical_file_content_type',\n                                                    'logical_file_object_id')\n\n    # these file metadata (size, modified_time, and checksum) values are retrieved from S3 and stored in db\n    # for performance reason so that we don't have to query S3 for these values every time we need them\n    _size = models.BigIntegerField(default=-1)\n    _modified_time = models.DateTimeField(null=True, blank=True)\n    _checksum = models.CharField(max_length=255, null=True, blank=True)\n\n    # for tracking when size was last compared with S3\n    filesize_cache_updated = models.DateTimeField(null=True)\n\n    def __str__(self):\n        return self.resource_file.name\n\n    @classmethod\n    def banned_symbols(cls):\n        \"\"\"returns a list of banned characters for file/folder name\"\"\"\n        return r'\\/:*?\"&lt;&gt;|'\n\n    @classmethod\n    def system_meta_fields(cls):\n        \"\"\"returns a list of system metadata fields\"\"\"\n        return ['_size', '_modified_time', '_checksum', 'filesize_cache_updated']\n\n    @classmethod\n    def create(cls, resource, file, folder='', source=None):\n        \"\"\"Create custom create method for ResourceFile model.\n\n        Create takes arguments that are invariant of storage medium.\n        These are turned into a path that is suitable for the medium.\n        Federation must be initialized first at the resource level.\n\n        :param resource: resource that contains the file.\n        :param file: a File or a S3 path to an existing file already copied.\n        :param folder: the folder in which to store the file.\n        :param source: an S3 path in the same zone from which to copy the file.\n\n        There are two main usages to this constructor:\n\n        * uploading a file from a form or REST call:\n\n                ResourceFile.create(r, File(...something...), folder=d)\n\n        * copying a file internally from S3:\n\n                ResourceFile.create(r, file_name, folder=d, source=s)\n\n        In this case, source is a full S3 pathname of the place from which to copy\n        the file.\n\n        A third form is less common and presumes that the file already exists in S3\n        in the proper place:\n\n        * pointing to an existing file:\n\n                ResourceFile.create(r, file_name, folder=d)\n\n        \"\"\"\n        # bind to appropriate resource\n        kwargs = {}\n        if __debug__:\n            assert isinstance(resource, BaseResource)\n        kwargs['content_object'] = resource\n\n        kwargs['file_folder'] = folder\n\n        istorage = resource.get_s3_storage()\n\n        # if file is an open file, use native copy by setting appropriate variables\n        if isinstance(file, File):\n            filename = os.path.basename(file.name)\n            if not ResourceFile.is_filename_valid(filename):\n                raise SuspiciousFileOperation(\"Filename is not compliant with Hydroshare requirements\")\n\n            kwargs['resource_file'] = file\n\n        else:  # if file is not an open file, then it's a basename (string)\n            if file is None and source is not None:\n                if __debug__:\n                    assert (isinstance(source, str))\n                # source is a path to an S3 file to be copied here.\n                root, newfile = os.path.split(source)  # take file from source path\n                # newfile is where it should be copied to.\n                target = get_resource_file_path(resource, newfile, folder=folder)\n                if not istorage.exists(source):\n                    raise ValidationError(\"ResourceFile.create: source {} of copy not found\"\n                                          .format(source))\n                istorage.copyFiles(source, target)\n                if not istorage.exists(target):\n                    raise ValidationError(\"ResourceFile.create: copy to target {} failed\"\n                                          .format(target))\n            elif file is not None and source is None:\n                # file points to an existing S3 file\n                # no need to verify whether the file exists in S3 since the file\n                # name is returned from S3 ils list dir command which already\n                # confirmed the file exists already in S3\n                target = get_resource_file_path(resource, file, folder=folder)\n            else:\n                raise ValidationError(\n                    \"ResourceFile.create: exactly one of source or file must be specified\")\n\n            # we've copied or moved if necessary; now set the paths\n            kwargs['resource_file'] = target\n        if ResourceFile.objects.filter(resource_file=kwargs['resource_file']).exists():\n            raise ValidationError(\"ResourceFile.create: file {} already exists\"\n                                  .format(kwargs['resource_file']))\n\n        # Actually create the file record\n        # when file is a File, the file is copied to storage in this step\n        # otherwise, the copy must precede this step.\n\n        return ResourceFile.objects.create(**kwargs)\n\n    # TODO: automagically handle orphaned logical files\n    def delete(self, delete_logical_file=False):\n        \"\"\"Delete a resource file record and the file contents.\n        :param  delete_logical_file: if True deletes logical file associated with resource file\n\n        model.delete does not cascade to delete files themselves,\n        and these must be explicitly deleted.\n        \"\"\"\n        if self.exists:\n            if delete_logical_file and self.logical_file is not None:\n                # deleting logical file metadata deletes the logical file as well\n                self.logical_file.metadata.delete()\n            if self.resource_file:\n                self.resource_file.delete()\n        super(ResourceFile, self).delete()\n\n    @property\n    def resource(self):\n        \"\"\"Return content_object representing the resource from a resource file.\"\"\"\n        return self.content_object\n\n    @property\n    def size(self):\n        \"\"\"Return file size of the file.\n        Calculates the size first if it has not been calculated yet.\"\"\"\n        if self._size &lt; 0:\n            self.calculate_size()\n        return self._size\n\n    @property\n    def modified_time(self):\n        \"\"\"Return modified time of the file.\n        If the modified time is not already set, then it is first retrieved from S3 and stored in db.\n        \"\"\"\n        # self._size != 0 -&gt; file exists, or we have not set the size yet\n        if not self._modified_time and self._size != 0:\n            self.calculate_modified_time()\n        return self._modified_time\n\n    def calculate_modified_time(self, resource=None, save=True):\n        \"\"\"Updates modified time of the file in db.\n        Retrieves the modified time from S3 and stores it in db.\n        \"\"\"\n        if resource is None:\n            resource = self.resource\n\n        file_path = self.resource_file.name\n\n        try:\n            self._modified_time = self.resource_file.storage.get_modified_time(file_path)\n        except (SessionException, ValidationError):\n            logger = logging.getLogger(__name__)\n            logger.warning(\"file {} not found in S3\".format(self.storage_path))\n            self._modified_time = None\n        if save:\n            self.save(update_fields=[\"_modified_time\"])\n\n    @property\n    def checksum(self):\n        \"\"\"Return checksum of the file.\n        If the checksum is not already set, then it is first retrieved from S3 and stored in db.\n        \"\"\"\n        # self._size != 0 -&gt; file exists, or we have not set the size yet\n        if not self._checksum and self._size != 0:\n            self.calculate_checksum()\n        return self._checksum\n\n    def calculate_checksum(self, resource=None, save=True):\n        \"\"\"Updates checksum of the file in db.\n        Retrieves the checksum from S3 and stores it in db.\n        \"\"\"\n        if resource is None:\n            resource = self.resource\n\n        file_path = self.resource_file.name\n\n        try:\n            self._checksum = self.resource_file.storage.checksum(file_path)\n        except (SessionException, ValidationError):\n            logger = logging.getLogger(__name__)\n            logger.warning(\"file {} not found in S3\".format(self.storage_path))\n            self._checksum = None\n        if save:\n            self.save(update_fields=[\"_checksum\"])\n\n    # TODO: write unit test\n    @property\n    def exists(self):\n        istorage = self.resource.get_s3_storage()\n        return istorage.exists(self.resource_file.name)\n\n    # TODO: write unit test\n    def read(self):\n        return self.resource_file.read()\n\n    @property\n    def storage_path(self):\n        \"\"\"Return the qualified name for a file in the storage hierarchy.\n\n        This is a valid input to S3Storage for manipulating the file.\n        The output depends upon whether the S3Storage instance is running\n\n        \"\"\"\n        # instance.content_object can be stale after changes.\n        # Re-fetch based upon key; bypass type system; it is not relevant\n        resource = self.resource\n        return self.get_storage_path(resource)\n\n    def get_storage_path(self, resource):\n        \"\"\"Return the qualified name for a file in the storage hierarchy.\n        Note: This is the preferred way to get the storage path for a file when we are trying to find\n        the storage path for more than one file in a resource.\n        \"\"\"\n        return self.resource_file.name\n\n    def calculate_size(self, resource=None, save=True):\n        \"\"\"Reads the file size and saves to the DB\"\"\"\n        if resource is None:\n            resource = self.resource\n\n        try:\n            self._size = self.resource_file.size\n            self.filesize_cache_updated = now()\n        except (SessionException, ValidationError, FileNotFoundError):\n            logger = logging.getLogger(__name__)\n            logger.warning(\"file {} not found in S3\".format(self.storage_path))\n            self._size = 0\n        if save:\n            try:\n                self.save(update_fields=[\"_size\", \"filesize_cache_updated\"])\n            except Exception as e:\n                logger = logging.getLogger(__name__)\n                logger.error(f\"Error saving file size for {self.storage_path}: {e}\")\n                self._size = 0\n                self.save(update_fields=[\"_size\", \"filesize_cache_updated\"])\n\n    def set_system_metadata(self, resource=None, save=True):\n        \"\"\"Set system metadata (size, modified time, and checksum) for a file.\n        This method should be called after a file is uploaded to S3 and registered with Django.\n        \"\"\"\n\n        self.calculate_size(resource=resource, save=save)\n        if self._size &gt; 0:\n            # file exists in S3 - get modified time and checksum\n            self.calculate_modified_time(resource=resource, save=save)\n            self.calculate_checksum(resource=resource, save=save)\n        else:\n            # file was not found in S3\n            self._size = 0\n            self._modified_time = None\n            self._checksum = None\n        if save:\n            self.save(update_fields=self.system_meta_fields())\n\n    # ResourceFile API handles file operations\n    def set_storage_path(self, path, test_exists=True):\n        \"\"\"Bind this ResourceFile instance to an existing file.\n\n        :param path: the path of the object.\n        :param test_exists: if True, test for path existence in S3\n\n        Path can be absolute or relative.\n\n            * relative paths start with anything else and can start with optional folder\n\n        :raises ValidationError: if the pathname is inconsistent with resource configuration.\n        It is rather important that applications call this rather than simply calling\n        resource_file = \"text path\" because it takes the trouble of making that path\n        fully qualified so that S3Storage will work properly.\n\n        This records file_folder for future possible uploads and searches.\n\n        The heavy lifting in this routine is accomplished via path_is_acceptable and get_path,\n        which together normalize the file name.  Regardless of whether the internal file name\n        is qualified or not, this makes it fully qualified from the point of view of the\n        S3Storage module.\n\n        \"\"\"\n        folder, base = self.path_is_acceptable(path, test_exists=test_exists)\n        self.file_folder = folder\n\n        # switch FileFields based upon federation path\n        self.resource_file = get_path(self, base)\n        self.save()\n\n    @property\n    def short_path(self):\n        \"\"\"Return the unqualified path to the file object.\n\n        * This path is invariant of where the object is stored.\n\n        * Thus, it does not change if the resource is moved.\n\n        This is the path that should be used as a key to index things such as file type.\n        \"\"\"\n\n        # use of self.resource generates a query\n        return self.get_short_path()\n\n    def get_short_path(self):\n        \"\"\"Return the unqualified path to the file object.\n\n        * This path is invariant of where the object is stored.\n\n        * Thus, it does not change if the resource is moved.\n\n        This is the path that should be used as a key to index things such as file type.\n        Note: This is the preferred way to get the short path for a file when we are trying to find short path\n        for more than one file in a resource.\n        \"\"\"\n        folder, base = self.path_is_acceptable(self.resource_file.name, test_exists=False)\n        if folder is not None:\n            return os.path.join(folder, base)\n        else:\n            return base\n\n    def set_short_path(self, path):\n        \"\"\"Set a path to a given path, relative to resource root.\n\n        There is some question as to whether the short path should be stored explicitly or\n        derived as in short_path above. The latter is computationally expensive but results\n        in a single point of truth.\n        \"\"\"\n        folder, base = os.path.split(path)\n        self.file_folder = folder  # must precede call to get_path\n\n        self.resource_file = get_path(self, base)\n        self.save()\n\n    def parse(self):\n        \"\"\"Parse a path into folder and basename.\"\"\"\n        return self.path_is_acceptable(self.storage_path, test_exists=False)\n\n    def path_is_acceptable(self, path, test_exists=True):\n        \"\"\"Determine whether a path is acceptable for this resource file.\n\n        Called inside ResourceFile objects to check paths\n\n        :param path: path to test\n        :param test_exists: if True, test for path existence in S3\n\n        \"\"\"\n        return ResourceFile.resource_path_is_acceptable(self.resource, path, test_exists)\n\n    @classmethod\n    def resource_path_is_acceptable(cls, resource, path, test_exists=True):\n        \"\"\"Determine whether a path is acceptable for this resource file.\n\n        Called outside ResourceFile objects or before such an object exists\n\n        :param path: path to test\n        :param test_exists: if True, test for path existence in S3\n\n        This has the side effect of returning the short path for the resource\n        as a folder/filename pair.\n        \"\"\"\n        if test_exists:\n            storage = resource.get_s3_storage()\n        locpath = os.path.join(resource.short_id, \"data\", \"contents\") + \"/\"\n        relpath = path\n        if path.startswith(locpath):\n            # strip optional local path prefix\n            if test_exists and not storage.exists(path):\n                raise ValidationError(\"Local path ({}) does not exist in S3\".format(path))\n            plen = len(locpath)\n            relpath = relpath[plen:]  # strip local prefix, omit /\n\n        # now we have folder/file. We could have gotten this from the input, or\n        # from stripping qualification folders. Note that this can contain\n        # misnamed header content misinterpreted as a folder unless one tests\n        # for existence\n        if '/' in relpath:\n            folder, base = os.path.split(relpath)\n            abspath = get_resource_file_path(resource, base, folder=folder)\n            if test_exists and not storage.exists(abspath):\n                raise ValidationError(\"Local path does not exist in S3\")\n        else:\n            folder = ''\n            base = relpath\n            abspath = get_resource_file_path(resource, base, folder=folder)\n            if test_exists and not storage.exists(abspath):\n                raise ValidationError(\"Local path does not exist in S3\")\n\n        return folder, base\n\n    # classmethods do things that query or affect all files.\n\n    @classmethod\n    def check_for_preferred_name(cls, file_folder_name):\n        \"\"\"Checks if the file or folder name meets the preferred name requirements\"\"\"\n\n        # remove anything that is not an alphanumeric, dash, underscore, or dot\n        sanitized_name = re.sub(r'(?u)[^-\\w.]', '', file_folder_name)\n\n        if len(file_folder_name) != len(sanitized_name):\n            # one or more symbols that are not allowed was found\n            return False\n\n        if '..' in file_folder_name:\n            return False\n\n        return True\n\n    @classmethod\n    def is_filename_valid(cls, filename):\n        \"\"\"Checks if the uploaded file has filename that complies to the hydroshare requirements\n        :param  filename: Name of the file to check\n        \"\"\"\n        return cls._is_folder_file_name_valid(name_to_check=filename)\n\n    @classmethod\n    def is_folder_name_valid(cls, folder_name):\n        \"\"\"Checks if the folder name complies to the hydroshare requirements\n        :param  folder_name: Name of the folder to check\n        \"\"\"\n        return cls._is_folder_file_name_valid(name_to_check=folder_name, file=False)\n\n    @classmethod\n    def _is_folder_file_name_valid(cls, name_to_check, file=True):\n        \"\"\"Helper method to check if a file/folder name is compliant with hydroshare requirements\n        :param  name_to_check: Name of the file or folder to check\n        :param  file: A flag to indicate if name_to_check is the filename\n        \"\"\"\n\n        # space at the start or at the end is not allowed\n        if len(name_to_check.strip()) != len(name_to_check):\n            return False\n\n        # check for banned symbols\n        for symbol in cls.banned_symbols():\n            if symbol in name_to_check:\n                return False\n\n        if name_to_check in (\".\", \"..\", \"/\"):\n            # these represents special meaning in linux - current (.) dir, parent dir (..) and dir separator\n            return False\n\n        if not file:\n            folders = name_to_check.split(\"/\")\n            for folder in folders:\n                if len(folder.strip()) != len(folder):\n                    return False\n                if folder in (\".\", \"..\"):\n                    # these represents special meaning in linux - current (.) dir and parent dir (..)\n                    return False\n\n        return True\n\n    @classmethod\n    def validate_new_path(cls, new_path):\n        \"\"\"Validates a new file/folder path that will be created for a resource\n        :param  new_path: a file/folder path that is relative to the [res short_id]/data/contents\n        \"\"\"\n\n        # strip trailing slashes (if any)\n        path = str(new_path).strip().rstrip('/')\n        if not path:\n            raise SuspiciousFileOperation('Path cannot be empty')\n\n        if path.startswith('/'):\n            raise SuspiciousFileOperation(f\"Path ({path}) must not start with '/'\")\n\n        if path in ('.', '..'):\n            raise SuspiciousFileOperation(f\"Path ({path}) must not be '.' or '..\")\n\n        if any([\"./\" in path, \"../\" in path, \" /\" in path, \"/ \" in path, path.endswith(\"/.\"), path.endswith(\"/..\")]):\n            raise SuspiciousFileOperation(f\"Path ({path}) must not contain './', '../', '/.', or '/..'\")\n\n        return path\n\n    @classmethod\n    def get(cls, resource, file, folder=''):\n        \"\"\"Get a ResourceFile record via its short path.\"\"\"\n        resource_file_path = get_resource_file_path(resource, file, folder)\n        f = ResourceFile.objects.filter(object_id=resource.id, resource_file=resource_file_path).first()\n        if f:\n            return f\n        else:\n            raise ObjectDoesNotExist(f'ResourceFile {resource_file_path} does not exist.')\n\n    # TODO: move to BaseResource as instance method\n    @classmethod\n    def list_folder(cls, resource, folder, sub_folders=True):\n        \"\"\"List files (instances of ResourceFile) in a given folder.\n\n        :param resource: resource for which to list the folder\n        :param folder: folder listed as either short_path or fully qualified path\n        :param sub_folders: if true files from sub folders of *folder* will be included in the list\n        \"\"\"\n        file_folder_to_match = folder\n\n        if not folder:\n            folder = resource.file_path\n        elif not folder.startswith(resource.file_path):\n            folder = os.path.join(resource.file_path, folder)\n        else:\n            file_folder_to_match = folder[len(resource.file_path) + 1:]\n\n        if sub_folders:\n            # append trailing slash to match only this folder\n            if not folder.endswith(\"/\"):\n                folder += \"/\"\n            return ResourceFile.objects.filter(\n                object_id=resource.id,\n                resource_file__startswith=folder)\n        else:\n            return ResourceFile.objects.filter(\n                object_id=resource.id,\n                file_folder=file_folder_to_match)\n\n    # TODO: move to BaseResource as instance method\n    @classmethod\n    def create_folder(cls, resource, folder, migrating_resource=False):\n        \"\"\"Create a folder for a resource.\"\"\"\n        # avoid import loop\n        from hs_core.views.utils import create_folder\n        path_is_allowed(folder)\n        # TODO: move code from location used below to here\n        create_folder(resource.short_id, os.path.join('data', 'contents', folder),\n                      migrating_resource=migrating_resource)\n\n    # TODO: move to BaseResource as instance method\n    @classmethod\n    def remove_folder(cls, resource, folder, user):\n        \"\"\"Remove a folder for a resource.\"\"\"\n        # avoid import loop\n        from hs_core.views.utils import remove_folder\n        path_is_allowed(folder)\n        # TODO: move code from location used below to here\n        remove_folder(user, resource.short_id, os.path.join('data', 'contents', folder))\n\n    @property\n    def has_logical_file(self):\n        \"\"\"Check existence of logical file.\"\"\"\n        return self.logical_file_object_id is not None\n\n    @property\n    def logical_file(self):\n        \"\"\"Return content_object of logical file.\"\"\"\n        return self.logical_file_content_object\n\n    @property\n    def logical_file_type_name(self):\n        \"\"\"Return class name of logical file's content object.\"\"\"\n        return self.logical_file_content_object.__class__.__name__\n\n    @property\n    def aggregation_display_name(self):\n        \"\"\"Return a name for the logical file type (aggregation)- used in UI\"\"\"\n        return self.logical_file.get_aggregation_display_name()\n\n    @property\n    def has_generic_logical_file(self):\n        \"\"\"Return True of logical file type's classname is 'GenericLogicalFile'.\"\"\"\n        return self.logical_file_type_name == \"GenericLogicalFile\"\n\n    @property\n    def metadata(self):\n        \"\"\"Return logical file metadata.\"\"\"\n        if self.has_logical_file:\n            return self.logical_file.metadata\n        return None\n\n    @property\n    def mime_type(self):\n        \"\"\"Return MIME type of represented file.\"\"\"\n        from .hydroshare.utils import get_file_mime_type\n        return get_file_mime_type(self.file_name)\n\n    @property\n    def extension(self):\n        \"\"\"Return extension of resource file.\"\"\"\n        _, file_ext = os.path.splitext(self.storage_path)\n        return file_ext\n\n    @property\n    def dir_path(self):\n        \"\"\"Return directory path of resource file.\"\"\"\n        return os.path.dirname(self.storage_path)\n\n    @property\n    def full_path(self):\n        \"\"\"Return full path of resource file.\"\"\"\n        return self.storage_path\n\n    @property\n    def file_name(self):\n        \"\"\"Return filename of resource file.\"\"\"\n        return os.path.basename(self.storage_path)\n\n    @property\n    def url(self):\n        \"\"\"Return the URL of the file contained in this ResourceFile.\n\n        A GET of this URL simply returns the file. This URL is independent of federation.\n        PUT, POST, and DELETE are not supported.\n\n        This choice for a URL is dependent mainly upon conformance to DataOne URL standards\n        that are also conformant to the format in resourcemap.xml. This url does not contain\n        the site URL, which is prefixed when needed.\n\n        This is based upon the resourcemap_urls.py entry:\n\n            url(r'^resource/(?P&lt;shortkey&gt;[0-9a-f-]+)/data/contents/(?.+)/$',\n                views.file_download_url_mapper,\n                name='get_resource_file')\n\n        This url does NOT depend upon federation status.\n        \"\"\"\n        url_encoded_file_path = urllib.parse.quote(self.public_path)\n        return '/' + os.path.join('resource', url_encoded_file_path)\n\n    @property\n    def public_path(self):\n        \"\"\" return the public path (unqualified S3 path) for a resource.\n        \"\"\"\n        return os.path.join(self.resource.short_id, 'data', 'contents', self.short_path)\n\n    @property\n    def s3_path(self):\n        \"\"\" Return the S3 path for accessing a file, including possible federation information.\n            This consists of the resource id, /data/contents/, and the file path.\n        \"\"\"\n\n        return self.public_path\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.aggregation_display_name","title":"<code>aggregation_display_name</code>  <code>property</code>","text":"<p>Return a name for the logical file type (aggregation)- used in UI</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.checksum","title":"<code>checksum</code>  <code>property</code>","text":"<p>Return checksum of the file. If the checksum is not already set, then it is first retrieved from S3 and stored in db.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.dir_path","title":"<code>dir_path</code>  <code>property</code>","text":"<p>Return directory path of resource file.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.extension","title":"<code>extension</code>  <code>property</code>","text":"<p>Return extension of resource file.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.file_name","title":"<code>file_name</code>  <code>property</code>","text":"<p>Return filename of resource file.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.full_path","title":"<code>full_path</code>  <code>property</code>","text":"<p>Return full path of resource file.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.has_generic_logical_file","title":"<code>has_generic_logical_file</code>  <code>property</code>","text":"<p>Return True of logical file type's classname is 'GenericLogicalFile'.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.has_logical_file","title":"<code>has_logical_file</code>  <code>property</code>","text":"<p>Check existence of logical file.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.logical_file","title":"<code>logical_file</code>  <code>property</code>","text":"<p>Return content_object of logical file.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.logical_file_type_name","title":"<code>logical_file_type_name</code>  <code>property</code>","text":"<p>Return class name of logical file's content object.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Return logical file metadata.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.mime_type","title":"<code>mime_type</code>  <code>property</code>","text":"<p>Return MIME type of represented file.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.modified_time","title":"<code>modified_time</code>  <code>property</code>","text":"<p>Return modified time of the file. If the modified time is not already set, then it is first retrieved from S3 and stored in db.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.public_path","title":"<code>public_path</code>  <code>property</code>","text":"<p>return the public path (unqualified S3 path) for a resource.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.resource","title":"<code>resource</code>  <code>property</code>","text":"<p>Return content_object representing the resource from a resource file.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.s3_path","title":"<code>s3_path</code>  <code>property</code>","text":"<p>Return the S3 path for accessing a file, including possible federation information. This consists of the resource id, /data/contents/, and the file path.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.short_path","title":"<code>short_path</code>  <code>property</code>","text":"<p>Return the unqualified path to the file object.</p> <ul> <li> <p>This path is invariant of where the object is stored.</p> </li> <li> <p>Thus, it does not change if the resource is moved.</p> </li> </ul> <p>This is the path that should be used as a key to index things such as file type.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.size","title":"<code>size</code>  <code>property</code>","text":"<p>Return file size of the file. Calculates the size first if it has not been calculated yet.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.storage_path","title":"<code>storage_path</code>  <code>property</code>","text":"<p>Return the qualified name for a file in the storage hierarchy.</p> <p>This is a valid input to S3Storage for manipulating the file. The output depends upon whether the S3Storage instance is running</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.url","title":"<code>url</code>  <code>property</code>","text":"<p>Return the URL of the file contained in this ResourceFile.</p> <p>A GET of this URL simply returns the file. This URL is independent of federation. PUT, POST, and DELETE are not supported.</p> <p>This choice for a URL is dependent mainly upon conformance to DataOne URL standards that are also conformant to the format in resourcemap.xml. This url does not contain the site URL, which is prefixed when needed.</p> <p>This is based upon the resourcemap_urls.py entry:</p> <pre><code>url(r'^resource/(?P&lt;shortkey&gt;[0-9a-f-]+)/data/contents/(?.+)/$',\n    views.file_download_url_mapper,\n    name='get_resource_file')\n</code></pre> <p>This url does NOT depend upon federation status.</p>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.banned_symbols","title":"<code>banned_symbols()</code>  <code>classmethod</code>","text":"<p>returns a list of banned characters for file/folder name</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef banned_symbols(cls):\n    \"\"\"returns a list of banned characters for file/folder name\"\"\"\n    return r'\\/:*?\"&lt;&gt;|'\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.calculate_checksum","title":"<code>calculate_checksum(resource=None, save=True)</code>","text":"<p>Updates checksum of the file in db. Retrieves the checksum from S3 and stores it in db.</p> Source code in <code>hs_core/models.py</code> <pre><code>def calculate_checksum(self, resource=None, save=True):\n    \"\"\"Updates checksum of the file in db.\n    Retrieves the checksum from S3 and stores it in db.\n    \"\"\"\n    if resource is None:\n        resource = self.resource\n\n    file_path = self.resource_file.name\n\n    try:\n        self._checksum = self.resource_file.storage.checksum(file_path)\n    except (SessionException, ValidationError):\n        logger = logging.getLogger(__name__)\n        logger.warning(\"file {} not found in S3\".format(self.storage_path))\n        self._checksum = None\n    if save:\n        self.save(update_fields=[\"_checksum\"])\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.calculate_modified_time","title":"<code>calculate_modified_time(resource=None, save=True)</code>","text":"<p>Updates modified time of the file in db. Retrieves the modified time from S3 and stores it in db.</p> Source code in <code>hs_core/models.py</code> <pre><code>def calculate_modified_time(self, resource=None, save=True):\n    \"\"\"Updates modified time of the file in db.\n    Retrieves the modified time from S3 and stores it in db.\n    \"\"\"\n    if resource is None:\n        resource = self.resource\n\n    file_path = self.resource_file.name\n\n    try:\n        self._modified_time = self.resource_file.storage.get_modified_time(file_path)\n    except (SessionException, ValidationError):\n        logger = logging.getLogger(__name__)\n        logger.warning(\"file {} not found in S3\".format(self.storage_path))\n        self._modified_time = None\n    if save:\n        self.save(update_fields=[\"_modified_time\"])\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.calculate_size","title":"<code>calculate_size(resource=None, save=True)</code>","text":"<p>Reads the file size and saves to the DB</p> Source code in <code>hs_core/models.py</code> <pre><code>def calculate_size(self, resource=None, save=True):\n    \"\"\"Reads the file size and saves to the DB\"\"\"\n    if resource is None:\n        resource = self.resource\n\n    try:\n        self._size = self.resource_file.size\n        self.filesize_cache_updated = now()\n    except (SessionException, ValidationError, FileNotFoundError):\n        logger = logging.getLogger(__name__)\n        logger.warning(\"file {} not found in S3\".format(self.storage_path))\n        self._size = 0\n    if save:\n        try:\n            self.save(update_fields=[\"_size\", \"filesize_cache_updated\"])\n        except Exception as e:\n            logger = logging.getLogger(__name__)\n            logger.error(f\"Error saving file size for {self.storage_path}: {e}\")\n            self._size = 0\n            self.save(update_fields=[\"_size\", \"filesize_cache_updated\"])\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.check_for_preferred_name","title":"<code>check_for_preferred_name(file_folder_name)</code>  <code>classmethod</code>","text":"<p>Checks if the file or folder name meets the preferred name requirements</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef check_for_preferred_name(cls, file_folder_name):\n    \"\"\"Checks if the file or folder name meets the preferred name requirements\"\"\"\n\n    # remove anything that is not an alphanumeric, dash, underscore, or dot\n    sanitized_name = re.sub(r'(?u)[^-\\w.]', '', file_folder_name)\n\n    if len(file_folder_name) != len(sanitized_name):\n        # one or more symbols that are not allowed was found\n        return False\n\n    if '..' in file_folder_name:\n        return False\n\n    return True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.create","title":"<code>create(resource, file, folder='', source=None)</code>  <code>classmethod</code>","text":"<p>Create custom create method for ResourceFile model.</p> <p>Create takes arguments that are invariant of storage medium. These are turned into a path that is suitable for the medium. Federation must be initialized first at the resource level.</p> <p>:param resource: resource that contains the file. :param file: a File or a S3 path to an existing file already copied. :param folder: the folder in which to store the file. :param source: an S3 path in the same zone from which to copy the file.</p> <p>There are two main usages to this constructor:</p> <ul> <li> <p>uploading a file from a form or REST call:</p> <pre><code>ResourceFile.create(r, File(...something...), folder=d)\n</code></pre> </li> <li> <p>copying a file internally from S3:</p> <pre><code>ResourceFile.create(r, file_name, folder=d, source=s)\n</code></pre> </li> </ul> <p>In this case, source is a full S3 pathname of the place from which to copy the file.</p> <p>A third form is less common and presumes that the file already exists in S3 in the proper place:</p> <ul> <li>pointing to an existing file:<pre><code>ResourceFile.create(r, file_name, folder=d)\n</code></pre> </li> </ul> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, resource, file, folder='', source=None):\n    \"\"\"Create custom create method for ResourceFile model.\n\n    Create takes arguments that are invariant of storage medium.\n    These are turned into a path that is suitable for the medium.\n    Federation must be initialized first at the resource level.\n\n    :param resource: resource that contains the file.\n    :param file: a File or a S3 path to an existing file already copied.\n    :param folder: the folder in which to store the file.\n    :param source: an S3 path in the same zone from which to copy the file.\n\n    There are two main usages to this constructor:\n\n    * uploading a file from a form or REST call:\n\n            ResourceFile.create(r, File(...something...), folder=d)\n\n    * copying a file internally from S3:\n\n            ResourceFile.create(r, file_name, folder=d, source=s)\n\n    In this case, source is a full S3 pathname of the place from which to copy\n    the file.\n\n    A third form is less common and presumes that the file already exists in S3\n    in the proper place:\n\n    * pointing to an existing file:\n\n            ResourceFile.create(r, file_name, folder=d)\n\n    \"\"\"\n    # bind to appropriate resource\n    kwargs = {}\n    if __debug__:\n        assert isinstance(resource, BaseResource)\n    kwargs['content_object'] = resource\n\n    kwargs['file_folder'] = folder\n\n    istorage = resource.get_s3_storage()\n\n    # if file is an open file, use native copy by setting appropriate variables\n    if isinstance(file, File):\n        filename = os.path.basename(file.name)\n        if not ResourceFile.is_filename_valid(filename):\n            raise SuspiciousFileOperation(\"Filename is not compliant with Hydroshare requirements\")\n\n        kwargs['resource_file'] = file\n\n    else:  # if file is not an open file, then it's a basename (string)\n        if file is None and source is not None:\n            if __debug__:\n                assert (isinstance(source, str))\n            # source is a path to an S3 file to be copied here.\n            root, newfile = os.path.split(source)  # take file from source path\n            # newfile is where it should be copied to.\n            target = get_resource_file_path(resource, newfile, folder=folder)\n            if not istorage.exists(source):\n                raise ValidationError(\"ResourceFile.create: source {} of copy not found\"\n                                      .format(source))\n            istorage.copyFiles(source, target)\n            if not istorage.exists(target):\n                raise ValidationError(\"ResourceFile.create: copy to target {} failed\"\n                                      .format(target))\n        elif file is not None and source is None:\n            # file points to an existing S3 file\n            # no need to verify whether the file exists in S3 since the file\n            # name is returned from S3 ils list dir command which already\n            # confirmed the file exists already in S3\n            target = get_resource_file_path(resource, file, folder=folder)\n        else:\n            raise ValidationError(\n                \"ResourceFile.create: exactly one of source or file must be specified\")\n\n        # we've copied or moved if necessary; now set the paths\n        kwargs['resource_file'] = target\n    if ResourceFile.objects.filter(resource_file=kwargs['resource_file']).exists():\n        raise ValidationError(\"ResourceFile.create: file {} already exists\"\n                              .format(kwargs['resource_file']))\n\n    # Actually create the file record\n    # when file is a File, the file is copied to storage in this step\n    # otherwise, the copy must precede this step.\n\n    return ResourceFile.objects.create(**kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.create_folder","title":"<code>create_folder(resource, folder, migrating_resource=False)</code>  <code>classmethod</code>","text":"<p>Create a folder for a resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create_folder(cls, resource, folder, migrating_resource=False):\n    \"\"\"Create a folder for a resource.\"\"\"\n    # avoid import loop\n    from hs_core.views.utils import create_folder\n    path_is_allowed(folder)\n    # TODO: move code from location used below to here\n    create_folder(resource.short_id, os.path.join('data', 'contents', folder),\n                  migrating_resource=migrating_resource)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.delete","title":"<code>delete(delete_logical_file=False)</code>","text":"<p>Delete a resource file record and the file contents. :param  delete_logical_file: if True deletes logical file associated with resource file</p> <p>model.delete does not cascade to delete files themselves, and these must be explicitly deleted.</p> Source code in <code>hs_core/models.py</code> <pre><code>def delete(self, delete_logical_file=False):\n    \"\"\"Delete a resource file record and the file contents.\n    :param  delete_logical_file: if True deletes logical file associated with resource file\n\n    model.delete does not cascade to delete files themselves,\n    and these must be explicitly deleted.\n    \"\"\"\n    if self.exists:\n        if delete_logical_file and self.logical_file is not None:\n            # deleting logical file metadata deletes the logical file as well\n            self.logical_file.metadata.delete()\n        if self.resource_file:\n            self.resource_file.delete()\n    super(ResourceFile, self).delete()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.get","title":"<code>get(resource, file, folder='')</code>  <code>classmethod</code>","text":"<p>Get a ResourceFile record via its short path.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef get(cls, resource, file, folder=''):\n    \"\"\"Get a ResourceFile record via its short path.\"\"\"\n    resource_file_path = get_resource_file_path(resource, file, folder)\n    f = ResourceFile.objects.filter(object_id=resource.id, resource_file=resource_file_path).first()\n    if f:\n        return f\n    else:\n        raise ObjectDoesNotExist(f'ResourceFile {resource_file_path} does not exist.')\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.get_short_path","title":"<code>get_short_path()</code>","text":"<p>Return the unqualified path to the file object.</p> <ul> <li> <p>This path is invariant of where the object is stored.</p> </li> <li> <p>Thus, it does not change if the resource is moved.</p> </li> </ul> <p>This is the path that should be used as a key to index things such as file type. Note: This is the preferred way to get the short path for a file when we are trying to find short path for more than one file in a resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_short_path(self):\n    \"\"\"Return the unqualified path to the file object.\n\n    * This path is invariant of where the object is stored.\n\n    * Thus, it does not change if the resource is moved.\n\n    This is the path that should be used as a key to index things such as file type.\n    Note: This is the preferred way to get the short path for a file when we are trying to find short path\n    for more than one file in a resource.\n    \"\"\"\n    folder, base = self.path_is_acceptable(self.resource_file.name, test_exists=False)\n    if folder is not None:\n        return os.path.join(folder, base)\n    else:\n        return base\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.get_storage_path","title":"<code>get_storage_path(resource)</code>","text":"<p>Return the qualified name for a file in the storage hierarchy. Note: This is the preferred way to get the storage path for a file when we are trying to find the storage path for more than one file in a resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_storage_path(self, resource):\n    \"\"\"Return the qualified name for a file in the storage hierarchy.\n    Note: This is the preferred way to get the storage path for a file when we are trying to find\n    the storage path for more than one file in a resource.\n    \"\"\"\n    return self.resource_file.name\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.is_filename_valid","title":"<code>is_filename_valid(filename)</code>  <code>classmethod</code>","text":"<p>Checks if the uploaded file has filename that complies to the hydroshare requirements :param  filename: Name of the file to check</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef is_filename_valid(cls, filename):\n    \"\"\"Checks if the uploaded file has filename that complies to the hydroshare requirements\n    :param  filename: Name of the file to check\n    \"\"\"\n    return cls._is_folder_file_name_valid(name_to_check=filename)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.is_folder_name_valid","title":"<code>is_folder_name_valid(folder_name)</code>  <code>classmethod</code>","text":"<p>Checks if the folder name complies to the hydroshare requirements :param  folder_name: Name of the folder to check</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef is_folder_name_valid(cls, folder_name):\n    \"\"\"Checks if the folder name complies to the hydroshare requirements\n    :param  folder_name: Name of the folder to check\n    \"\"\"\n    return cls._is_folder_file_name_valid(name_to_check=folder_name, file=False)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.list_folder","title":"<code>list_folder(resource, folder, sub_folders=True)</code>  <code>classmethod</code>","text":"<p>List files (instances of ResourceFile) in a given folder.</p> <p>:param resource: resource for which to list the folder :param folder: folder listed as either short_path or fully qualified path :param sub_folders: if true files from sub folders of folder will be included in the list</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef list_folder(cls, resource, folder, sub_folders=True):\n    \"\"\"List files (instances of ResourceFile) in a given folder.\n\n    :param resource: resource for which to list the folder\n    :param folder: folder listed as either short_path or fully qualified path\n    :param sub_folders: if true files from sub folders of *folder* will be included in the list\n    \"\"\"\n    file_folder_to_match = folder\n\n    if not folder:\n        folder = resource.file_path\n    elif not folder.startswith(resource.file_path):\n        folder = os.path.join(resource.file_path, folder)\n    else:\n        file_folder_to_match = folder[len(resource.file_path) + 1:]\n\n    if sub_folders:\n        # append trailing slash to match only this folder\n        if not folder.endswith(\"/\"):\n            folder += \"/\"\n        return ResourceFile.objects.filter(\n            object_id=resource.id,\n            resource_file__startswith=folder)\n    else:\n        return ResourceFile.objects.filter(\n            object_id=resource.id,\n            file_folder=file_folder_to_match)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.parse","title":"<code>parse()</code>","text":"<p>Parse a path into folder and basename.</p> Source code in <code>hs_core/models.py</code> <pre><code>def parse(self):\n    \"\"\"Parse a path into folder and basename.\"\"\"\n    return self.path_is_acceptable(self.storage_path, test_exists=False)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.path_is_acceptable","title":"<code>path_is_acceptable(path, test_exists=True)</code>","text":"<p>Determine whether a path is acceptable for this resource file.</p> <p>Called inside ResourceFile objects to check paths</p> <p>:param path: path to test :param test_exists: if True, test for path existence in S3</p> Source code in <code>hs_core/models.py</code> <pre><code>def path_is_acceptable(self, path, test_exists=True):\n    \"\"\"Determine whether a path is acceptable for this resource file.\n\n    Called inside ResourceFile objects to check paths\n\n    :param path: path to test\n    :param test_exists: if True, test for path existence in S3\n\n    \"\"\"\n    return ResourceFile.resource_path_is_acceptable(self.resource, path, test_exists)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.remove_folder","title":"<code>remove_folder(resource, folder, user)</code>  <code>classmethod</code>","text":"<p>Remove a folder for a resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove_folder(cls, resource, folder, user):\n    \"\"\"Remove a folder for a resource.\"\"\"\n    # avoid import loop\n    from hs_core.views.utils import remove_folder\n    path_is_allowed(folder)\n    # TODO: move code from location used below to here\n    remove_folder(user, resource.short_id, os.path.join('data', 'contents', folder))\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.resource_path_is_acceptable","title":"<code>resource_path_is_acceptable(resource, path, test_exists=True)</code>  <code>classmethod</code>","text":"<p>Determine whether a path is acceptable for this resource file.</p> <p>Called outside ResourceFile objects or before such an object exists</p> <p>:param path: path to test :param test_exists: if True, test for path existence in S3</p> <p>This has the side effect of returning the short path for the resource as a folder/filename pair.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef resource_path_is_acceptable(cls, resource, path, test_exists=True):\n    \"\"\"Determine whether a path is acceptable for this resource file.\n\n    Called outside ResourceFile objects or before such an object exists\n\n    :param path: path to test\n    :param test_exists: if True, test for path existence in S3\n\n    This has the side effect of returning the short path for the resource\n    as a folder/filename pair.\n    \"\"\"\n    if test_exists:\n        storage = resource.get_s3_storage()\n    locpath = os.path.join(resource.short_id, \"data\", \"contents\") + \"/\"\n    relpath = path\n    if path.startswith(locpath):\n        # strip optional local path prefix\n        if test_exists and not storage.exists(path):\n            raise ValidationError(\"Local path ({}) does not exist in S3\".format(path))\n        plen = len(locpath)\n        relpath = relpath[plen:]  # strip local prefix, omit /\n\n    # now we have folder/file. We could have gotten this from the input, or\n    # from stripping qualification folders. Note that this can contain\n    # misnamed header content misinterpreted as a folder unless one tests\n    # for existence\n    if '/' in relpath:\n        folder, base = os.path.split(relpath)\n        abspath = get_resource_file_path(resource, base, folder=folder)\n        if test_exists and not storage.exists(abspath):\n            raise ValidationError(\"Local path does not exist in S3\")\n    else:\n        folder = ''\n        base = relpath\n        abspath = get_resource_file_path(resource, base, folder=folder)\n        if test_exists and not storage.exists(abspath):\n            raise ValidationError(\"Local path does not exist in S3\")\n\n    return folder, base\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.set_short_path","title":"<code>set_short_path(path)</code>","text":"<p>Set a path to a given path, relative to resource root.</p> <p>There is some question as to whether the short path should be stored explicitly or derived as in short_path above. The latter is computationally expensive but results in a single point of truth.</p> Source code in <code>hs_core/models.py</code> <pre><code>def set_short_path(self, path):\n    \"\"\"Set a path to a given path, relative to resource root.\n\n    There is some question as to whether the short path should be stored explicitly or\n    derived as in short_path above. The latter is computationally expensive but results\n    in a single point of truth.\n    \"\"\"\n    folder, base = os.path.split(path)\n    self.file_folder = folder  # must precede call to get_path\n\n    self.resource_file = get_path(self, base)\n    self.save()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.set_storage_path","title":"<code>set_storage_path(path, test_exists=True)</code>","text":"<p>Bind this ResourceFile instance to an existing file.</p> <p>:param path: the path of the object. :param test_exists: if True, test for path existence in S3</p> <p>Path can be absolute or relative.</p> <pre><code>* relative paths start with anything else and can start with optional folder\n</code></pre> <p>:raises ValidationError: if the pathname is inconsistent with resource configuration. It is rather important that applications call this rather than simply calling resource_file = \"text path\" because it takes the trouble of making that path fully qualified so that S3Storage will work properly.</p> <p>This records file_folder for future possible uploads and searches.</p> <p>The heavy lifting in this routine is accomplished via path_is_acceptable and get_path, which together normalize the file name.  Regardless of whether the internal file name is qualified or not, this makes it fully qualified from the point of view of the S3Storage module.</p> Source code in <code>hs_core/models.py</code> <pre><code>def set_storage_path(self, path, test_exists=True):\n    \"\"\"Bind this ResourceFile instance to an existing file.\n\n    :param path: the path of the object.\n    :param test_exists: if True, test for path existence in S3\n\n    Path can be absolute or relative.\n\n        * relative paths start with anything else and can start with optional folder\n\n    :raises ValidationError: if the pathname is inconsistent with resource configuration.\n    It is rather important that applications call this rather than simply calling\n    resource_file = \"text path\" because it takes the trouble of making that path\n    fully qualified so that S3Storage will work properly.\n\n    This records file_folder for future possible uploads and searches.\n\n    The heavy lifting in this routine is accomplished via path_is_acceptable and get_path,\n    which together normalize the file name.  Regardless of whether the internal file name\n    is qualified or not, this makes it fully qualified from the point of view of the\n    S3Storage module.\n\n    \"\"\"\n    folder, base = self.path_is_acceptable(path, test_exists=test_exists)\n    self.file_folder = folder\n\n    # switch FileFields based upon federation path\n    self.resource_file = get_path(self, base)\n    self.save()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.set_system_metadata","title":"<code>set_system_metadata(resource=None, save=True)</code>","text":"<p>Set system metadata (size, modified time, and checksum) for a file. This method should be called after a file is uploaded to S3 and registered with Django.</p> Source code in <code>hs_core/models.py</code> <pre><code>def set_system_metadata(self, resource=None, save=True):\n    \"\"\"Set system metadata (size, modified time, and checksum) for a file.\n    This method should be called after a file is uploaded to S3 and registered with Django.\n    \"\"\"\n\n    self.calculate_size(resource=resource, save=save)\n    if self._size &gt; 0:\n        # file exists in S3 - get modified time and checksum\n        self.calculate_modified_time(resource=resource, save=save)\n        self.calculate_checksum(resource=resource, save=save)\n    else:\n        # file was not found in S3\n        self._size = 0\n        self._modified_time = None\n        self._checksum = None\n    if save:\n        self.save(update_fields=self.system_meta_fields())\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.system_meta_fields","title":"<code>system_meta_fields()</code>  <code>classmethod</code>","text":"<p>returns a list of system metadata fields</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef system_meta_fields(cls):\n    \"\"\"returns a list of system metadata fields\"\"\"\n    return ['_size', '_modified_time', '_checksum', 'filesize_cache_updated']\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceFile.validate_new_path","title":"<code>validate_new_path(new_path)</code>  <code>classmethod</code>","text":"<p>Validates a new file/folder path that will be created for a resource :param  new_path: a file/folder path that is relative to the [res short_id]/data/contents</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef validate_new_path(cls, new_path):\n    \"\"\"Validates a new file/folder path that will be created for a resource\n    :param  new_path: a file/folder path that is relative to the [res short_id]/data/contents\n    \"\"\"\n\n    # strip trailing slashes (if any)\n    path = str(new_path).strip().rstrip('/')\n    if not path:\n        raise SuspiciousFileOperation('Path cannot be empty')\n\n    if path.startswith('/'):\n        raise SuspiciousFileOperation(f\"Path ({path}) must not start with '/'\")\n\n    if path in ('.', '..'):\n        raise SuspiciousFileOperation(f\"Path ({path}) must not be '.' or '..\")\n\n    if any([\"./\" in path, \"../\" in path, \" /\" in path, \"/ \" in path, path.endswith(\"/.\"), path.endswith(\"/..\")]):\n        raise SuspiciousFileOperation(f\"Path ({path}) must not contain './', '../', '/.', or '/..'\")\n\n    return path\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceManager","title":"<code>ResourceManager</code>","text":"<p>               Bases: <code>PageManager</code></p> <p>Extend mezzanine PageManager to manage Resource pages.</p> Source code in <code>hs_core/models.py</code> <pre><code>class ResourceManager(PageManager):\n    \"\"\"Extend mezzanine PageManager to manage Resource pages.\"\"\"\n\n    def __init__(self, resource_type=None, *args, **kwargs):\n        \"\"\"Extend mezzanine PageManager to manage Resource pages based on resource_type.\"\"\"\n        self.resource_type = resource_type\n        super(ResourceManager, self).__init__(*args, **kwargs)\n\n    def create(self, *args, **kwargs):\n        \"\"\"Create new mezzanine page based on resource_type.\"\"\"\n        if self.resource_type is None:\n            kwargs.pop('resource_type', None)\n        return super(ResourceManager, self).create(*args, **kwargs)\n\n    def get_queryset(self):\n        \"\"\"Get mezzanine-like queryset based on resource_type.\"\"\"\n        qs = super(ResourceManager, self).get_queryset()\n        if self.resource_type:\n            qs = qs.filter(resource_type=self.resource_type)\n        return qs\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceManager.__init__","title":"<code>__init__(resource_type=None, *args, **kwargs)</code>","text":"<p>Extend mezzanine PageManager to manage Resource pages based on resource_type.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __init__(self, resource_type=None, *args, **kwargs):\n    \"\"\"Extend mezzanine PageManager to manage Resource pages based on resource_type.\"\"\"\n    self.resource_type = resource_type\n    super(ResourceManager, self).__init__(*args, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceManager.create","title":"<code>create(*args, **kwargs)</code>","text":"<p>Create new mezzanine page based on resource_type.</p> Source code in <code>hs_core/models.py</code> <pre><code>def create(self, *args, **kwargs):\n    \"\"\"Create new mezzanine page based on resource_type.\"\"\"\n    if self.resource_type is None:\n        kwargs.pop('resource_type', None)\n    return super(ResourceManager, self).create(*args, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourceManager.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Get mezzanine-like queryset based on resource_type.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_queryset(self):\n    \"\"\"Get mezzanine-like queryset based on resource_type.\"\"\"\n    qs = super(ResourceManager, self).get_queryset()\n    if self.resource_type:\n        qs = qs.filter(resource_type=self.resource_type)\n    return qs\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourcePermissionsMixin","title":"<code>ResourcePermissionsMixin</code>","text":"<p>               Bases: <code>Ownable</code></p> <p>Mix in can_* permission helper functions between users and resources.</p> Source code in <code>hs_core/models.py</code> <pre><code>class ResourcePermissionsMixin(Ownable):\n    \"\"\"Mix in can_* permission helper functions between users and resources.\"\"\"\n\n    creator = models.ForeignKey(User, on_delete=models.CASCADE,\n                                related_name='creator_of_%(app_label)s_%(class)s',\n                                help_text='This is the person who first uploaded the resource',\n                                )\n\n    class Meta:\n        \"\"\"Define meta properties for ResourcePermissionsMixin, make abstract.\"\"\"\n\n        abstract = True\n\n    @property\n    def permissions_store(self):\n        \"\"\"Use PERMISSIONS_DB constant. Unsure what 's' is here.\"\"\"\n        return s.PERMISSIONS_DB\n\n    def can_add(self, request):\n        \"\"\"Pass through can_change to determine if user can make changes to a resource.\"\"\"\n        return self.can_change(request)\n\n    def can_delete(self, request):\n        \"\"\"Use utils.authorize method to determine if user can delete a resource.\"\"\"\n        # have to do import locally to avoid circular import\n        from hs_core.views.utils import ACTION_TO_AUTHORIZE, authorize\n        return authorize(request, self,\n                         needed_permission=ACTION_TO_AUTHORIZE.DELETE_RESOURCE,\n                         raises_exception=False)[1]\n\n    def can_change(self, request):\n        \"\"\"Use utils.authorize method to determine if user can change a resource.\"\"\"\n        # have to do import locally to avoid circular import\n        from hs_core.views.utils import ACTION_TO_AUTHORIZE, authorize\n        return authorize(request, self,\n                         needed_permission=ACTION_TO_AUTHORIZE.EDIT_RESOURCE,\n                         raises_exception=False)[1]\n\n    def can_view(self, request):\n        \"\"\"Use utils.authorize method to determine if user can view a resource.\"\"\"\n        # have to do import locally to avoid circular import\n        from hs_core.views.utils import ACTION_TO_AUTHORIZE, authorize\n        return authorize(request, self,\n                         needed_permission=ACTION_TO_AUTHORIZE.VIEW_METADATA,\n                         raises_exception=False)[1]\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourcePermissionsMixin.permissions_store","title":"<code>permissions_store</code>  <code>property</code>","text":"<p>Use PERMISSIONS_DB constant. Unsure what 's' is here.</p>"},{"location":"hs_core/models/#hs_core.models.ResourcePermissionsMixin.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for ResourcePermissionsMixin, make abstract.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for ResourcePermissionsMixin, make abstract.\"\"\"\n\n    abstract = True\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourcePermissionsMixin.can_add","title":"<code>can_add(request)</code>","text":"<p>Pass through can_change to determine if user can make changes to a resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>def can_add(self, request):\n    \"\"\"Pass through can_change to determine if user can make changes to a resource.\"\"\"\n    return self.can_change(request)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourcePermissionsMixin.can_change","title":"<code>can_change(request)</code>","text":"<p>Use utils.authorize method to determine if user can change a resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>def can_change(self, request):\n    \"\"\"Use utils.authorize method to determine if user can change a resource.\"\"\"\n    # have to do import locally to avoid circular import\n    from hs_core.views.utils import ACTION_TO_AUTHORIZE, authorize\n    return authorize(request, self,\n                     needed_permission=ACTION_TO_AUTHORIZE.EDIT_RESOURCE,\n                     raises_exception=False)[1]\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourcePermissionsMixin.can_delete","title":"<code>can_delete(request)</code>","text":"<p>Use utils.authorize method to determine if user can delete a resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>def can_delete(self, request):\n    \"\"\"Use utils.authorize method to determine if user can delete a resource.\"\"\"\n    # have to do import locally to avoid circular import\n    from hs_core.views.utils import ACTION_TO_AUTHORIZE, authorize\n    return authorize(request, self,\n                     needed_permission=ACTION_TO_AUTHORIZE.DELETE_RESOURCE,\n                     raises_exception=False)[1]\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.ResourcePermissionsMixin.can_view","title":"<code>can_view(request)</code>","text":"<p>Use utils.authorize method to determine if user can view a resource.</p> Source code in <code>hs_core/models.py</code> <pre><code>def can_view(self, request):\n    \"\"\"Use utils.authorize method to determine if user can view a resource.\"\"\"\n    # have to do import locally to avoid circular import\n    from hs_core.views.utils import ACTION_TO_AUTHORIZE, authorize\n    return authorize(request, self,\n                     needed_permission=ACTION_TO_AUTHORIZE.VIEW_METADATA,\n                     raises_exception=False)[1]\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Rights","title":"<code>Rights</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define Rights custom metadata element model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.rights, statement=HSTERMS.rightsStatement, url=HSTERMS.URL)\nclass Rights(AbstractMetaDataElement):\n    \"\"\"Define Rights custom metadata element model.\"\"\"\n\n    term = 'Rights'\n    statement = models.TextField()\n    url = models.URLField(null=True, blank=True)\n\n    def __unicode__(self):\n        \"\"\"Return either statement or statement + url for unicode representation.\"\"\"\n        value = ''\n        if self.statement:\n            value += self.statement + ' '\n        if self.url:\n            value += self.url\n\n        return value\n\n    class Meta:\n        \"\"\"Define meta properties for Rights model.\"\"\"\n\n        unique_together = (\"content_type\", \"object_id\")\n\n    @classmethod\n    def remove(cls, element_id):\n        \"\"\"Define custom remove method for Rights model.\"\"\"\n        raise ValidationError(\"Rights element of a resource can't be deleted.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Rights.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Rights model.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Rights model.\"\"\"\n\n    unique_together = (\"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Rights.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return either statement or statement + url for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return either statement or statement + url for unicode representation.\"\"\"\n    value = ''\n    if self.statement:\n        value += self.statement + ' '\n    if self.url:\n        value += self.url\n\n    return value\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Rights.remove","title":"<code>remove(element_id)</code>  <code>classmethod</code>","text":"<p>Define custom remove method for Rights model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id):\n    \"\"\"Define custom remove method for Rights model.\"\"\"\n    raise ValidationError(\"Rights element of a resource can't be deleted.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Subject","title":"<code>Subject</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define Subject custom metadata element model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.subject)\nclass Subject(AbstractMetaDataElement):\n    \"\"\"Define Subject custom metadata element model.\"\"\"\n\n    term = 'Subject'\n    value = models.CharField(max_length=100)\n\n    class Meta:\n        \"\"\"Define meta properties for Subject model.\"\"\"\n\n        unique_together = (\"value\", \"content_type\", \"object_id\")\n\n    def __unicode__(self):\n        \"\"\"Return value field for unicode representation.\"\"\"\n        return self.value\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Define custom create method for Subject model.\"\"\"\n        metadata_obj = kwargs['content_object']\n        value = kwargs.get('value', None)\n        if value is not None:\n            if metadata_obj.subjects.filter(value__iexact=value).exists():\n                raise ValidationError(\"Subject element already exists.\")\n\n        return super(Subject, cls).create(**kwargs)\n\n    @classmethod\n    def remove(cls, element_id):\n        \"\"\"Define custom remove method for Subject model.\"\"\"\n        sub = Subject.objects.get(id=element_id)\n        if Subject.objects.filter(object_id=sub.object_id,\n                                  content_type__pk=sub.content_type.id).count() == 1:\n            raise ValidationError(\"The only subject element of the resource can't be deleted.\")\n        sub.delete()\n\n    def rdf_triples(self, subject, graph):\n        graph.add((subject, self.get_class_term(), Literal(self.value)))\n\n    @classmethod\n    def ingest_rdf(cls, graph, subject, content_object):\n        for _, _, o in graph.triples((subject, cls.get_class_term(), None)):\n            Subject.create(value=str(o), content_object=content_object)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Subject.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Subject model.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Subject model.\"\"\"\n\n    unique_together = (\"value\", \"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Subject.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return value field for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return value field for unicode representation.\"\"\"\n    return self.value\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Subject.create","title":"<code>create(**kwargs)</code>  <code>classmethod</code>","text":"<p>Define custom create method for Subject model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef create(cls, **kwargs):\n    \"\"\"Define custom create method for Subject model.\"\"\"\n    metadata_obj = kwargs['content_object']\n    value = kwargs.get('value', None)\n    if value is not None:\n        if metadata_obj.subjects.filter(value__iexact=value).exists():\n            raise ValidationError(\"Subject element already exists.\")\n\n    return super(Subject, cls).create(**kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Subject.remove","title":"<code>remove(element_id)</code>  <code>classmethod</code>","text":"<p>Define custom remove method for Subject model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id):\n    \"\"\"Define custom remove method for Subject model.\"\"\"\n    sub = Subject.objects.get(id=element_id)\n    if Subject.objects.filter(object_id=sub.object_id,\n                              content_type__pk=sub.content_type.id).count() == 1:\n        raise ValidationError(\"The only subject element of the resource can't be deleted.\")\n    sub.delete()\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Title","title":"<code>Title</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define Title metadata element model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.title)\nclass Title(AbstractMetaDataElement):\n    \"\"\"Define Title metadata element model.\"\"\"\n\n    term = 'Title'\n    value = models.CharField(max_length=300)\n\n    def __unicode__(self):\n        \"\"\"Return value field for unicode representation.\"\"\"\n        return self.value\n\n    class Meta:\n        \"\"\"Define meta properties for Title class.\"\"\"\n\n        unique_together = (\"content_type\", \"object_id\")\n\n    @classmethod\n    def remove(cls, element_id):\n        \"\"\"Define custom remove function for Title class.\"\"\"\n        raise ValidationError(\"Title element of a resource can't be deleted.\")\n\n    def rdf_triples(self, subject, graph):\n        graph.add((subject, self.get_class_term(), Literal(self.value)))\n\n    @classmethod\n    def ingest_rdf(cls, graph, subject, content_object):\n        title = graph.value(subject=subject, predicate=cls.get_class_term())\n        if title:\n            Title.create(value=title.value, content_object=content_object)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Title.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Title class.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Title class.\"\"\"\n\n    unique_together = (\"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Title.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return value field for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return value field for unicode representation.\"\"\"\n    return self.value\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Title.remove","title":"<code>remove(element_id)</code>  <code>classmethod</code>","text":"<p>Define custom remove function for Title class.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id):\n    \"\"\"Define custom remove function for Title class.\"\"\"\n    raise ValidationError(\"Title element of a resource can't be deleted.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Type","title":"<code>Type</code>","text":"<p>               Bases: <code>AbstractMetaDataElement</code></p> <p>Define Type metadata element model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@rdf_terms(DC.type)\nclass Type(AbstractMetaDataElement):\n    \"\"\"Define Type metadata element model.\"\"\"\n\n    term = 'Type'\n    url = models.URLField()\n\n    def __unicode__(self):\n        \"\"\"Return url field for unicode representation.\"\"\"\n        return self.url\n\n    class Meta:\n        \"\"\"Define meta properties for Type class.\"\"\"\n\n        unique_together = (\"content_type\", \"object_id\")\n\n    @classmethod\n    def remove(cls, element_id):\n        \"\"\"Define custom remove function for Type model.\"\"\"\n        raise ValidationError(\"Type element of a resource can't be deleted.\")\n\n    def rdf_triples(self, subject, graph):\n        graph.add((subject, self.get_class_term(), URIRef(self.url)))\n\n    @classmethod\n    def ingest_rdf(cls, graph, subject, content_object):\n        url = graph.value(subject=subject, predicate=cls.get_class_term())\n        if url:\n            Type.create(url=str(url), content_object=content_object)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Type.Meta","title":"<code>Meta</code>","text":"<p>Define meta properties for Type class.</p> Source code in <code>hs_core/models.py</code> <pre><code>class Meta:\n    \"\"\"Define meta properties for Type class.\"\"\"\n\n    unique_together = (\"content_type\", \"object_id\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Type.__unicode__","title":"<code>__unicode__()</code>","text":"<p>Return url field for unicode representation.</p> Source code in <code>hs_core/models.py</code> <pre><code>def __unicode__(self):\n    \"\"\"Return url field for unicode representation.\"\"\"\n    return self.url\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.Type.remove","title":"<code>remove(element_id)</code>  <code>classmethod</code>","text":"<p>Define custom remove function for Type model.</p> Source code in <code>hs_core/models.py</code> <pre><code>@classmethod\ndef remove(cls, element_id):\n    \"\"\"Define custom remove function for Type model.\"\"\"\n    raise ValidationError(\"Type element of a resource can't be deleted.\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.clean_abstract","title":"<code>clean_abstract(original_string)</code>","text":"<p>Clean abstract for XML inclusion.</p> <p>This function takes an original string and removes any illegal XML characters from it. It uses regular expressions to identify and remove the illegal characters.</p> <p>Parameters:</p> Name Type Description Default <code>original_string</code> <code>str</code> <p>The original string to be cleaned.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The cleaned string with illegal XML characters removed.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If there is an error cleaning the abstract.</p> Source code in <code>hs_core/models.py</code> <pre><code>def clean_abstract(original_string):\n    \"\"\"Clean abstract for XML inclusion.\n\n    This function takes an original string and removes any illegal XML characters\n    from it. It uses regular expressions to identify and remove the illegal characters.\n\n    Args:\n        original_string (str): The original string to be cleaned.\n\n    Returns:\n        str: The cleaned string with illegal XML characters removed.\n\n    Raises:\n        ValidationError: If there is an error cleaning the abstract.\n\n    \"\"\"\n    # https://stackoverflow.com/a/64570125\n    try:\n        illegal_unichrs = [(0x00, 0x08), (0x0B, 0x0C), (0x0E, 0x1F),\n                           (0x7F, 0x84), (0x86, 0x9F),\n                           (0xFDD0, 0xFDDF), (0xFFFE, 0xFFFF)]\n        if sys.maxunicode &gt;= 0x10000:  # not narrow build\n            illegal_unichrs.extend([(0x1FFFE, 0x1FFFF), (0x2FFFE, 0x2FFFF),\n                                    (0x3FFFE, 0x3FFFF), (0x4FFFE, 0x4FFFF),\n                                    (0x5FFFE, 0x5FFFF), (0x6FFFE, 0x6FFFF),\n                                    (0x7FFFE, 0x7FFFF), (0x8FFFE, 0x8FFFF),\n                                    (0x9FFFE, 0x9FFFF), (0xAFFFE, 0xAFFFF),\n                                    (0xBFFFE, 0xBFFFF), (0xCFFFE, 0xCFFFF),\n                                    (0xDFFFE, 0xDFFFF), (0xEFFFE, 0xEFFFF),\n                                    (0xFFFFE, 0xFFFFF), (0x10FFFE, 0x10FFFF)])\n\n        illegal_ranges = [fr'{chr(low)}-{chr(high)}' for (low, high) in illegal_unichrs]\n        xml_illegal_character_regex = '[' + ''.join(illegal_ranges) + ']'\n        illegal_xml_chars_re = re.compile(xml_illegal_character_regex)\n        filtered_string = illegal_xml_chars_re.sub('', original_string)\n        return filtered_string\n    except (KeyError, TypeError) as ex:\n        raise ValidationError(f\"Error cleaning abstract: {ex}\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.clean_for_xml","title":"<code>clean_for_xml(s)</code>","text":"<pre><code>Remove all control characters from a unicode string in preparation for XML inclusion\n\n* Convert\n</code></pre> <ul> <li>to unicode paragraph<ul> <li>Convert   alone to unicode RETURN (return SYMBOL)</li> <li>Convert control characters to spaces if last character is not space.</li> <li>Space-pad paragraph and NL symbols as necessary</li> </ul> </li> </ul> Source code in <code>hs_core/models.py</code> <pre><code>def clean_for_xml(s):\n    \"\"\"\n    Remove all control characters from a unicode string in preparation for XML inclusion\n\n    * Convert \\n\\n+ to unicode paragraph\n    * Convert \\n alone to unicode RETURN (return SYMBOL)\n    * Convert control characters to spaces if last character is not space.\n    * Space-pad paragraph and NL symbols as necessary\n\n    \"\"\"\n    # https://www.w3.org/TR/REC-xml/#sec-line-ends\n    CR = chr(0x23CE)  # carriage return unicode SYMBOL\n    PARA = chr(0xB6)  # paragraph mark unicode SYMBOL\n    output = ''\n    next = None\n    last = None\n    for ch in s:\n        cat = unicodedata.category(ch)\n        ISCONTROL = cat[0] == 'C'\n        ISSPACE = cat[0] == 'Z'\n        ISNEWLINE = (ord(ch) == 10)\n        if next:\n            if ISNEWLINE:  # linux '\\n'\n                next = PARA  # upgrade to two+ returns\n            elif ISSPACE or ISCONTROL:\n                pass  # ignore spaces in newline sequence\n            else:\n                if last != ' ':\n                    output += ' '\n                output += next + ' ' + ch\n                next = None\n                last = ch\n        else:\n            if ISNEWLINE:\n                next = CR\n            elif ISSPACE:\n                if last != ' ':\n                    output += ch\n                    last = ch\n            elif ISCONTROL:\n                if last != ' ':\n                    output += ' '\n                    last = ' '\n            else:\n                output += ch\n                last = ch\n    return output\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.get_path","title":"<code>get_path(instance, filename, folder='')</code>","text":"<p>Get a path from a ResourceFile, filename, and folder.</p> <p>:param instance: instance of ResourceFile to use :param filename: file name to use (without folder) :param folder: can override folder for ResourceFile instance</p> <p>The filename is only a single name. This routine converts it to an absolute which contains the federation path. The folder in the instance will be used unless overridden.</p> <p>Note: this does not change the default behavior. Thus it can be used to compute a new path for file that one wishes to move.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_path(instance, filename, folder=''):\n    \"\"\"Get a path from a ResourceFile, filename, and folder.\n\n    :param instance: instance of ResourceFile to use\n    :param filename: file name to use (without folder)\n    :param folder: can override folder for ResourceFile instance\n\n    The filename is only a single name. This routine converts it to an absolute\n    which contains the federation path. The folder in the instance will be used unless\n    overridden.\n\n    Note: this does not change the default behavior.\n    Thus it can be used to compute a new path for file that\n    one wishes to move.\n    \"\"\"\n    if not folder:\n        folder = instance.file_folder\n    return get_resource_file_path(instance.resource, filename, folder)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.get_resource_file_path","title":"<code>get_resource_file_path(resource, filename, folder='')</code>","text":"<p>Determine storage path for a FileField</p> <p>:param resource: resource containing the file. :param filename: name of file without folder. :param folder: folder of file</p> <p>The filename is only a single name. This routine converts it to an absolute to do this.</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_resource_file_path(resource, filename, folder=''):\n    \"\"\"Determine storage path for a FileField\n\n    :param resource: resource containing the file.\n    :param filename: name of file without folder.\n    :param folder: folder of file\n\n    The filename is only a single name. This routine converts it to an absolute\n    to do this.\n\n    \"\"\"\n    # folder can be absolute pathname; strip qualifications off of folder if necessary\n    # cannot only test folder string to start with resource.root_path, since a relative folder path\n    # may start with the resource's uuid if the same resource bag is added into the same resource and unzipped\n    # into the resource as in the bug reported in this issue: https://github.com/hydroshare/hydroshare/issues/2984\n\n    res_data_content_path = os.path.join(resource.root_path, 'data', 'contents')\n    if folder is not None and folder.startswith(res_data_content_path):\n        # TODO: does this now start with /?\n        # check if the folder is a path relative to the resource data content path, if yes, no need to strip out\n        # resource.root_path\n        istorage = resource.get_s3_storage()\n        if not istorage.exists(os.path.join(res_data_content_path, folder)):\n            # folder is not a path relative to res_data_content_path, but a path including resource root path,\n            # strip out resource root path to make folder a relative path\n            folder = folder[len(resource.root_path):]\n\n    # retrieve federation path -- if any -- from Resource object containing the file\n    if filename.startswith(resource.file_path):\n        return filename\n\n    # otherwise, it is an unqualified name.\n    if folder:\n        # use subfolder\n        folder = folder.strip('/')\n        return os.path.join(resource.file_path, folder, filename)\n\n    else:\n        # use root folder\n        filename = filename.strip('/')\n        return os.path.join(resource.file_path, filename)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.get_user","title":"<code>get_user(request)</code>","text":"<p>Authorize user based on API key if it was passed, otherwise just use the request's user.</p> <p>NOTE: The API key portion has been removed with TastyPie and will be restored when the new API is built.</p> <p>:param request: :return: django.contrib.auth.User</p> Source code in <code>hs_core/models.py</code> <pre><code>def get_user(request):\n    \"\"\"Authorize user based on API key if it was passed, otherwise just use the request's user.\n\n    NOTE: The API key portion has been removed with TastyPie and will be restored when the\n    new API is built.\n\n    :param request:\n    :return: django.contrib.auth.User\n    \"\"\"\n    if not hasattr(request, 'user'):\n        raise PermissionDenied\n    if request.user.is_authenticated:\n        return User.objects.get(pk=request.user.pk)\n    else:\n        return request.user\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.new_get_content_model","title":"<code>new_get_content_model(self)</code>","text":"<p>Override mezzanine get_content_model function for pages for resources.</p> Source code in <code>hs_core/models.py</code> <pre><code>def new_get_content_model(self):\n    \"\"\"Override mezzanine get_content_model function for pages for resources.\"\"\"\n    from hs_core.hydroshare.utils import get_resource_types\n    content_model = self.content_model\n    if content_model.endswith('resource'):\n        rt = [rt for rt in get_resource_types() if rt._meta.model_name == content_model][0]\n        return rt.objects.filter(id=self.id).select_related('raccess', 'quota_holder', 'rlabels').first()\n    return old_get_content_model(self)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.page_permissions_page_processor","title":"<code>page_permissions_page_processor(request, page)</code>","text":"<p>Return a dict describing permissions for current user.</p> Source code in <code>hs_core/models.py</code> <pre><code>def page_permissions_page_processor(request, page):\n    \"\"\"Return a dict describing permissions for current user.\"\"\"\n    from hs_access_control.models.privilege import PrivilegeCodes\n    from hs_core.hydroshare.utils import get_remaining_user_quota\n\n    # Optimized to only fetch minimal data for initial page load.\n    # Full manage access data is fetched on-demand when the manage access modal is opened.\n\n    cm = page.get_content_model()\n    can_change_resource_flags = False\n    self_access_level = None\n    if request.user.is_authenticated:\n        if request.user.uaccess.can_change_resource_flags(cm):\n            can_change_resource_flags = True\n\n        # this will get resource access privilege even for admin user\n        user_privilege = cm.raccess.get_effective_user_privilege(request.user)\n        if user_privilege == PrivilegeCodes.OWNER:\n            self_access_level = 'owner'\n        elif user_privilege == PrivilegeCodes.CHANGE:\n            self_access_level = 'edit'\n        elif user_privilege == PrivilegeCodes.VIEW:\n            self_access_level = 'view'\n\n    # Only fetch owners for left header display - full access list loaded on-demand\n    owners = cm.raccess.owners.all()\n\n    last_changed_by = cm.last_changed_by\n\n    if request.user.is_authenticated:\n        for owner in owners:\n            owner.can_undo = request.user.uaccess.can_undo_share_resource_with_user(cm, owner)\n            owner.viewable_contributions = request.user.uaccess.can_view_resources_owned_by(owner)\n\n        if last_changed_by.is_active:\n            last_changed_by.viewable_contributions = request.user.uaccess.can_view_resources_owned_by(last_changed_by)\n\n    else:\n        for owner in owners:\n            owner.can_undo = False\n    last_changed_by.can_undo = False\n\n    # Only include owners in initial page load JSON\n    users_json = []\n    for usr in owners:\n        users_json.append(get_access_object(usr, \"user\", \"owner\"))\n\n    if last_changed_by.is_active:\n        lcb_access_level = cm.raccess.get_effective_user_privilege(last_changed_by)\n        if lcb_access_level == PrivilegeCodes.OWNER:\n            lcb_access_level = 'owner'\n        elif lcb_access_level == PrivilegeCodes.CHANGE:\n            lcb_access_level = 'edit'\n        elif lcb_access_level == PrivilegeCodes.VIEW:\n            lcb_access_level = 'view'\n    else:\n        lcb_access_level = 'none'\n\n    last_changed_by = json.dumps(get_access_object(last_changed_by, \"user\", lcb_access_level))\n\n    users_json = json.dumps(users_json)\n\n    is_replaced_by = cm.get_relation_version_res_url(RelationTypes.isReplacedBy)\n    is_version_of = cm.get_relation_version_res_url(RelationTypes.isVersionOf)\n\n    permissions_allow_copy = False\n    if request.user.is_authenticated:\n        permissions_allow_copy = request.user.uaccess.can_view_resource(cm)\n\n    show_manage_access = False\n    is_owner = self_access_level == 'owner'\n    is_edit = self_access_level == 'edit'\n    is_view = self_access_level == 'view'\n    if is_owner or (cm.raccess.shareable and (is_view or is_edit)):\n        show_manage_access = True\n\n    file_upload_max_size = getattr(settings, 'FILE_UPLOAD_MAX_SIZE', 25 * 1024**3)\n    remaining_quota = get_remaining_user_quota(cm.quota_holder, \"MB\")\n    if remaining_quota is not None:\n        remaining_quota = remaining_quota * 1024**2\n\n    # https://docs.djangoproject.com/en/3.2/ref/settings/#data-upload-max-memory-size\n    max_chunk_size = getattr(settings, 'DATA_UPLOAD_MAX_MEMORY_SIZE', 2.5 * 1024**2)\n\n    max_number_of_files_in_single_local_upload = getattr(settings, 'MAX_NUMBER_OF_FILES_IN_SINGLE_LOCAL_UPLOAD', 50)\n    parallel_uploads_limit = getattr(settings, 'PARALLEL_UPLOADS_LIMIT', 10)\n\n    companion_url = getattr(settings, 'COMPANION_URL', 'https://companion.hydroshare.org/')\n    UPPY_UPLOAD_PATH = getattr(settings, 'UPPY_UPLOAD_PATH', 'https://hydroshare.org/django_s3/tus/')\n    google_picker_client_id = getattr(settings, 'GOOGLE_PICKER_CLIENT_ID', '')\n    google_picker_api_key = getattr(settings, 'GOOGLE_PICKER_API_KEY', '')\n    google_picker_app_id = getattr(settings, 'GOOGLE_PICKER_APP_ID', '')\n\n    # get the session id for the current user\n    session = None\n    if request.user.is_authenticated:\n        try:\n            session = request.session.session_key\n        except SessionException:\n            pass\n\n    return {\n        'resource_type': cm._meta.verbose_name,\n        \"users_json\": users_json,\n        \"owners\": owners,\n        \"self_access_level\": self_access_level,\n        \"permissions_allow_copy\": permissions_allow_copy,\n        \"can_change_resource_flags\": can_change_resource_flags,\n        \"is_replaced_by\": is_replaced_by,\n        \"is_version_of\": is_version_of,\n        \"show_manage_access\": show_manage_access,\n        \"last_changed_by\": last_changed_by,\n        \"remaining_quota\": remaining_quota,\n        \"file_upload_max_size\": file_upload_max_size,\n        \"max_chunk_size\": max_chunk_size,\n        \"max_number_of_files_in_single_local_upload\": max_number_of_files_in_single_local_upload,\n        \"parallel_uploads_limit\": parallel_uploads_limit,\n        \"companion_url\": companion_url,\n        \"UPPY_UPLOAD_PATH\": UPPY_UPLOAD_PATH,\n        \"google_picker_client_id\": google_picker_client_id,\n        \"google_picker_api_key\": google_picker_api_key,\n        \"google_picker_app_id\": google_picker_app_id,\n        \"hs_s_id\": session\n    }\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.path_is_allowed","title":"<code>path_is_allowed(path)</code>","text":"<p>Check for suspicious paths containing '/../'.</p> Source code in <code>hs_core/models.py</code> <pre><code>def path_is_allowed(path):\n    \"\"\"Check for suspicious paths containing '/../'.\"\"\"\n    if path == \"\":\n        raise ValidationError(\"Empty file paths are not allowed\")\n    if '/../' in path:\n        raise SuspiciousFileOperation(\"File paths cannot contain '/../'\")\n    if '/./' in path:\n        raise SuspiciousFileOperation(\"File paths cannot contain '/./'\")\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.resource_creation_signal_handler","title":"<code>resource_creation_signal_handler(sender, instance, created, **kwargs)</code>","text":"<p>Return resource update signal handler for newly created resource.</p> <p>For now this is just a placeholder for some actions to be taken when a resource gets saved</p> Source code in <code>hs_core/models.py</code> <pre><code>@receiver(post_save)\ndef resource_creation_signal_handler(sender, instance, created, **kwargs):\n    \"\"\"Return resource update signal handler for newly created resource.\n\n    For now this is just a placeholder for some actions to be taken when a resource gets saved\n    \"\"\"\n    if isinstance(instance, AbstractResource):\n        if created:\n            pass\n        else:\n            resource_update_signal_handler(sender, instance, created, **kwargs)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.resource_processor","title":"<code>resource_processor(request, page)</code>","text":"<p>Return mezzanine page processor for resource page.</p> Source code in <code>hs_core/models.py</code> <pre><code>def resource_processor(request, page):\n    \"\"\"Return mezzanine page processor for resource page.\"\"\"\n    extra = page_permissions_page_processor(request, page)\n    return extra\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.resource_update_signal_handler","title":"<code>resource_update_signal_handler(sender, instance, created, **kwargs)</code>","text":"<p>Do nothing (noop).</p> Source code in <code>hs_core/models.py</code> <pre><code>def resource_update_signal_handler(sender, instance, created, **kwargs):\n    \"\"\"Do nothing (noop).\"\"\"\n    pass\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.short_id","title":"<code>short_id()</code>","text":"<p>Generate a uuid4 hex to be used as a resource or element short_id.</p> Source code in <code>hs_core/models.py</code> <pre><code>def short_id():\n    \"\"\"Generate a uuid4 hex to be used as a resource or element short_id.\"\"\"\n    return uuid4().hex\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.validate_abstract","title":"<code>validate_abstract(value)</code>","text":"<p>Validates the abstract value by ensuring it can serialize as XML.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The abstract value to be validated.</p> required <p>Raises:</p> Type Description <code>ValidationError</code> <p>If there is an error parsing the abstract as XML.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>hs_core/models.py</code> <pre><code>def validate_abstract(value):\n    \"\"\"\n    Validates the abstract value by ensuring it can serialize as XML.\n\n    Args:\n        value (str): The abstract value to be validated.\n\n    Raises:\n        ValidationError: If there is an error parsing the abstract as XML.\n\n    Returns:\n        None\n    \"\"\"\n    err_message = 'Error parsing abstract as XML.'\n    if value:\n        try:\n            ROOT = etree.Element('root')\n            body_node = etree.SubElement(ROOT, 'body')\n            c_abstract = clean_abstract(value)\n            etree.SubElement(body_node, 'description').text = c_abstract\n        except Exception as ex:\n            raise ValidationError(f'{err_message}, more info: {ex}')\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.validate_hydroshare_user_id","title":"<code>validate_hydroshare_user_id(value)</code>","text":"<p>Validate that a hydroshare_user_id is valid for a hydroshare user.</p> Source code in <code>hs_core/models.py</code> <pre><code>def validate_hydroshare_user_id(value):\n    \"\"\"Validate that a hydroshare_user_id is valid for a hydroshare user.\"\"\"\n    err_message = '%s is not a valid id for hydroshare user' % value\n    if value:\n        try:\n            value = int(value)\n        except ValueError:\n            raise ValidationError(err_message)\n\n        # check the user exists for the provided user id\n        if not User.objects.filter(pk=value).exists():\n            raise UserValidationError(err_message)\n</code></pre>"},{"location":"hs_core/models/#hs_core.models.validate_user_url","title":"<code>validate_user_url(value)</code>","text":"<p>Validate that a URL is a valid URL for a hydroshare user.</p> Source code in <code>hs_core/models.py</code> <pre><code>def validate_user_url(value):\n    \"\"\"Validate that a URL is a valid URL for a hydroshare user.\"\"\"\n    err_message = '%s is not a valid url for hydroshare user' % value\n    if value:\n        url_parts = value.split('/')\n        if len(url_parts) != 4:\n            raise ValidationError(err_message)\n        if url_parts[1] != 'user':\n            raise ValidationError(err_message)\n\n        try:\n            user_id = int(url_parts[2])\n        except ValueError:\n            raise ValidationError(err_message)\n\n        # check the user exists for the provided user id\n        if not User.objects.filter(pk=user_id).exists():\n            raise ValidationError(err_message)\n</code></pre>"},{"location":"hs_core/views/","title":"Views","text":""},{"location":"hs_core/views/#hs_core.views.act_on_group_membership_request","title":"<code>act_on_group_membership_request(request, membership_request_id, action, *args, **kwargs)</code>","text":"<p>Take action (accept or decline) on group membership request</p> <p>:param request: requesting user is either owner of the group taking action on a request from a user                 or a user taking action on a invitation to join a group from a group owner :param membership_request_id: id of the membership request object (an instance of GroupMembershipRequest)                               to act on :param action: need to have a value of either 'accept' or 'decline' :return:</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@login_required\ndef act_on_group_membership_request(\n    request, membership_request_id, action, *args, **kwargs\n):\n    \"\"\"\n    Take action (accept or decline) on group membership request\n\n    :param request: requesting user is either owner of the group taking action on a request from a user\n                    or a user taking action on a invitation to join a group from a group owner\n    :param membership_request_id: id of the membership request object (an instance of GroupMembershipRequest)\n                                  to act on\n    :param action: need to have a value of either 'accept' or 'decline'\n    :return:\n    \"\"\"\n\n    accept_request = action == \"accept\"\n    user_acting = request.user\n\n    try:\n        membership_request = GroupMembershipRequest.objects.get(\n            pk=membership_request_id\n        )\n    except ObjectDoesNotExist:\n        messages.error(request, \"No matching group membership request was found\")\n    else:\n        if membership_request.group_to_join.gaccess.active:\n            try:\n                user_acting.uaccess.act_on_group_membership_request(\n                    membership_request, accept_request\n                )\n                if accept_request:\n                    message = \"Membership request accepted\"\n                    messages.success(request, message)\n                    # send email to notify membership acceptance\n                    _send_email_on_group_membership_acceptance(membership_request)\n                else:\n                    message = \"Membership request declined\"\n                    messages.success(request, message)\n\n            except PermissionDenied as ex:\n                messages.error(request, str(ex))\n        else:\n            messages.error(request, \"Group is not active\")\n\n    return HttpResponseRedirect(request.headers[\"referer\"])\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.add_files_to_resource","title":"<code>add_files_to_resource(request, shortkey, *args, **kwargs)</code>","text":"<p>This view function is called by AJAX in the folder implementation :param request: AJAX request :param shortkey: resource uuid :param args: :param kwargs: :return: HTTP response with status code indicating success or failure</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def add_files_to_resource(request, shortkey, *args, **kwargs):\n    \"\"\"\n    This view function is called by AJAX in the folder implementation\n    :param request: AJAX request\n    :param shortkey: resource uuid\n    :param args:\n    :param kwargs:\n    :return: HTTP response with status code indicating success or failure\n    \"\"\"\n    resource, _, user = authorize(\n        request, shortkey, needed_permission=ACTION_TO_AUTHORIZE.EDIT_RESOURCE\n    )\n    if resource.raccess.published and not user.is_superuser:\n        msg = {\"validation_error\": \"Only admin can add files to a published resource\"}\n        return JsonResponse(msg, status=500)\n\n    res_files, full_paths = extract_files_with_paths(request)\n    auto_aggregate = request.POST.get(\"auto_aggregate\", \"true\").lower() == \"true\"\n    extract_metadata = request.GET.get(\"extract-metadata\", \"No\")\n    extract_metadata = True if extract_metadata.lower() == \"yes\" else False\n    file_folder = request.POST.get(\"file_folder\", \"\")\n    if file_folder == \"data/contents\":\n        file_folder = \"\"\n    elif file_folder.startswith(\"data/contents/\"):\n        file_folder = file_folder[len(\"data/contents/\"):]\n\n    try:\n        hydroshare.utils.resource_file_add_pre_process(\n            resource=resource,\n            files=res_files,\n            user=request.user,\n            extract_metadata=extract_metadata,\n            folder=file_folder,\n        )\n\n    except hydroshare.utils.ResourceFileSizeException as ex:\n        msg = {\"file_size_error\": str(ex)}\n        return JsonResponse(msg, status=500)\n\n    except (hydroshare.utils.ResourceFileValidationException, Exception) as ex:\n        msg = {\"validation_error\": str(ex)}\n        return JsonResponse(msg, status=500)\n\n    aggregations_pre = [aggr for aggr in resource.logical_files]\n    aggr_count_pre = len(aggregations_pre)\n\n    try:\n        hydroshare.utils.resource_file_add_process(\n            resource=resource,\n            files=res_files,\n            user=request.user,\n            extract_metadata=extract_metadata,\n            folder=file_folder,\n            full_paths=full_paths,\n            auto_aggregate=auto_aggregate,\n        )\n\n    except (hydroshare.utils.ResourceFileValidationException, Exception) as ex:\n        msg = {\"validation_error\": str(ex)}\n        return JsonResponse(msg, status=500)\n\n    aggregations_post = [aggr for aggr in resource.logical_files]\n    aggr_count_post = len(aggregations_post)\n\n    res_public_status = \"public\" if resource.raccess.public else \"not public\"\n    res_discoverable_status = (\n        \"discoverable\" if resource.raccess.discoverable else \"not discoverable\"\n    )\n\n    show_meta_status = False\n    if \"meta-status\" not in request.session:\n        request.session[\"meta-status\"] = \"\"\n\n    if \"meta-status-res-id\" not in request.session:\n        request.session[\"meta-status-res-id\"] = resource.short_id\n        show_meta_status = True\n    elif request.session[\"meta-status-res-id\"] != resource.short_id:\n        request.session[\"meta-status-res-id\"] = resource.short_id\n        show_meta_status = True\n\n    if resource.can_be_public_or_discoverable:\n        metadata_status = METADATA_STATUS_SUFFICIENT\n    else:\n        metadata_status = METADATA_STATUS_INSUFFICIENT\n\n    if request.session[\"meta-status\"] != metadata_status:\n        request.session[\"meta-status\"] = metadata_status\n        show_meta_status = True\n\n    response_data = {\n        \"res_public_status\": res_public_status,\n        \"res_discoverable_status\": res_discoverable_status,\n        \"number_new_aggregations\": aggr_count_post - aggr_count_pre,\n        \"metadata_status\": metadata_status,\n        \"show_meta_status\": show_meta_status,\n    }\n\n    return JsonResponse(data=response_data, status=200)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.add_metadata_element","title":"<code>add_metadata_element(request, shortkey, element_name, *args, **kwargs)</code>","text":"<p>This function is normally for adding/creating new resource level metadata elements. However, for the metadata element 'subject' (keyword) this function allows for creating, updating and deleting metadata elements.</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def add_metadata_element(request, shortkey, element_name, *args, **kwargs):\n    \"\"\"This function is normally for adding/creating new resource level metadata elements.\n    However, for the metadata element 'subject' (keyword) this function allows for creating,\n    updating and deleting metadata elements.\n    \"\"\"\n    res, _, _ = authorize(\n        request, shortkey, needed_permission=ACTION_TO_AUTHORIZE.EDIT_RESOURCE\n    )\n\n    is_add_success = False\n    err_msg = \"Failed to create metadata element '{}'. {}.\"\n    element = None\n    sender_resource = _get_resource_sender(element_name, res)\n    if element_name.lower() == \"subject\" and len(request.POST[\"value\"]) == 0:\n        # seems the user wants to delete all keywords - no need for pre-check in signal handler\n        if res.raccess.published:\n            err_msg = err_msg.format(\n                element_name, \"Published resource needs to have at least one subject\"\n            )\n        else:\n            res.metadata.subjects.all().delete()\n            is_add_success = True\n            res.update_public_and_discoverable()\n            resource_modified(res, request.user, overwrite_bag=False)\n    else:\n\n        if element_name.lower() == \"subject\":\n            original_value = request.POST.get(\"value\", \"\")\n            keywords = original_value.replace(\",\", \"\")\n            # Make request.POST mutable\n            mutable_post = request.POST.copy()\n            mutable_post[\"value\"] = keywords\n            request.POST = mutable_post\n\n        handler_response = signals.pre_metadata_element_create.send(\n            sender=sender_resource, element_name=element_name, request=request\n        )\n\n        if element_name.lower() == \"subject\" and original_value:\n            mutable_post[\"value\"] = original_value  # Reassign original value\n            request.POST = mutable_post\n\n        for receiver, response in handler_response:\n            if \"is_valid\" in response:\n                if response[\"is_valid\"]:\n                    element_data_dict = response[\"element_data_dict\"]\n\n                    if element_name == \"subject\":\n                        if original_value:\n                            element_data_dict[\"value\"] = original_value\n\n                        # using set() to remove any duplicate keywords\n                        keywords = set(\n                            [k.strip() for k in element_data_dict[\"value\"].split(\",\")]\n                        )\n                        keyword_maxlength = Subject._meta.get_field(\"value\").max_length\n                        keywords_to_add = []\n                        for kw in keywords:\n                            if len(kw) &gt; keyword_maxlength:\n                                kw = kw[:keyword_maxlength]\n\n                            # skip any duplicate keywords (case insensitive)\n                            if (\n                                kw not in keywords_to_add\n                                and kw.lower() not in keywords_to_add\n                            ):\n                                keywords_to_add.append(kw)\n\n                        if len(keywords_to_add) &gt; 0:\n                            res.metadata.subjects.all().delete()\n                            for kw in keywords_to_add:\n                                res.metadata.create_element(element_name, value=kw)\n                        is_add_success = True\n                    else:\n                        try:\n                            element = res.metadata.create_element(\n                                element_name, **element_data_dict\n                            )\n                            res.refresh_from_db()\n                            is_add_success = True\n                        except ValidationError as exp:\n                            err_msg = err_msg.format(element_name, str(exp))\n                            request.session[\"validation_error\"] = err_msg\n                            logger.warning(err_msg)\n                        except Error as exp:\n                            # some database error occurred\n                            err_msg = err_msg.format(element_name, str(exp))\n                            request.session[\"validation_error\"] = err_msg\n                            logger.warning(err_msg)\n                        except Exception as exp:\n                            # some other error occurred\n                            err_msg = err_msg.format(element_name, str(exp))\n                            request.session[\"validation_error\"] = err_msg\n                            logger.warning(err_msg)\n\n                    if is_add_success:\n                        resource_modified(res, request.user, overwrite_bag=False)\n                elif \"errors\" in response:\n                    err_msg = err_msg.format(element_name, response[\"errors\"])\n\n    update_doi_metadata_with_datacite(short_id=shortkey, element_name=element_name,\n                                      payload={element_name: request.POST.get(\"value\", \"\")})\n    if is_ajax(request):\n        if is_add_success:\n            res_public_status = \"public\" if res.raccess.public else \"not public\"\n            res_discoverable_status = (\n                \"discoverable\" if res.raccess.discoverable else \"not discoverable\"\n            )\n            if res.can_be_public_or_discoverable:\n                metadata_status = METADATA_STATUS_SUFFICIENT\n            else:\n                metadata_status = METADATA_STATUS_INSUFFICIENT\n\n            if element_name == \"subject\":\n                ajax_response_data = {\n                    \"status\": \"success\",\n                    \"element_name\": element_name,\n                    \"metadata_status\": metadata_status,\n                    \"res_public_status\": res_public_status,\n                    \"res_discoverable_status\": res_discoverable_status,\n                }\n            else:\n                ajax_response_data = {\n                    \"status\": \"success\",\n                    \"element_type\": getattr(element, 'type', None),\n                    \"element_name\": element_name,\n                    \"spatial_coverage\": get_coverage_data_dict(res),\n                    \"temporal_coverage\": get_coverage_data_dict(res, \"temporal\"),\n                    \"has_logical_temporal_coverage\": res.has_logical_temporal_coverage,\n                    \"has_logical_spatial_coverage\": res.has_logical_spatial_coverage,\n                    \"metadata_status\": metadata_status,\n                    \"res_public_status\": res_public_status,\n                    \"res_discoverable_status\": res_discoverable_status,\n                }\n                if element is not None:\n                    ajax_response_data[\"element_id\"] = element.id\n\n            ajax_response_data[\"is_dirty\"] = (\n                res.metadata.is_dirty if hasattr(res.metadata, \"is_dirty\") else False\n            )\n\n            return JsonResponse(ajax_response_data)\n        else:\n            ajax_response_data = {\"status\": \"error\", \"message\": err_msg}\n            return JsonResponse(ajax_response_data)\n\n    if \"resource-mode\" in request.POST:\n        request.session[\"resource-mode\"] = \"edit\"\n\n    return HttpResponseRedirect(request.headers[\"referer\"])\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.copy_resource_public","title":"<code>copy_resource_public(request, pk)</code>","text":"<p>Copy a resource</p> <p>:param request: :param pk: id of the resource to be copied</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@swagger_auto_schema(\n    method=\"post\",\n    operation_description=\"Copy a resource\",\n    responses={202: \"Returns the resource ID of the newly created resource\"},\n    manual_parameters=[res_id],\n)\n@api_view([\"POST\"])\ndef copy_resource_public(request, pk):\n    \"\"\"\n    Copy a resource\n\n    :param request:\n    :param pk: id of the resource to be copied\n    \"\"\"\n    response = copy_resource(request, pk)\n    if not response.status_code &lt; 400:\n        return response\n    return HttpResponse(response.url.split(\"/\")[2], status=202)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.create_new_version_resource_public","title":"<code>create_new_version_resource_public(request, pk)</code>","text":"<p>Create a new version of a resource</p> <p>:param request: :param pk: id of the resource to be versioned :return: HttpResponse with status code</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@swagger_auto_schema(\n    method=\"post\",\n    operation_description=\"Create a new version of a resource\",\n    responses={202: \"Returns the resource ID of the new version\"},\n    manual_parameters=[res_id],\n)\n@api_view([\"POST\"])\ndef create_new_version_resource_public(request, pk):\n    \"\"\"\n    Create a new version of a resource\n\n    :param request:\n    :param pk: id of the resource to be versioned\n    :return: HttpResponse with status code\n    \"\"\"\n    redirect = create_new_version_resource(request, pk)\n    if not redirect.status_code &lt; 400:\n        return redirect\n    return HttpResponse(redirect.url.split(\"/\")[2], status=202)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.delete_user_group","title":"<code>delete_user_group(request, group_id, *args, **kwargs)</code>","text":"<p>This one is not really deleting the group object, rather setting the active status to False (delete) which can be later restored (undelete) )</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@login_required\ndef delete_user_group(request, group_id, *args, **kwargs):\n    \"\"\"This one is not really deleting the group object, rather setting the active status\n    to False (delete) which can be later restored (undelete) )\"\"\"\n    try:\n        hydroshare.set_group_active_status(request.user, group_id, False)\n        messages.success(request, \"Group delete was successful.\")\n    except PermissionDenied:\n        messages.error(\n            request,\n            \"Group delete errors: You don't have permission to delete\" \" this group.\",\n        )\n\n    return HttpResponseRedirect(request.headers[\"referer\"])\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.file_download_url_mapper","title":"<code>file_download_url_mapper(request, shortkey, filepath)</code>","text":"<p>maps the file URIs in resourcemap document to django_s3 download view function</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@swagger_auto_schema(method=\"get\", auto_schema=None)\n@api_view([\"GET\"])\ndef file_download_url_mapper(request, shortkey, filepath):\n    \"\"\"maps the file URIs in resourcemap document to django_s3 download view function\"\"\"\n    try:\n        res, _, _ = authorize(\n            request,\n            shortkey,\n            needed_permission=ACTION_TO_AUTHORIZE.VIEW_RESOURCE,\n            raises_exception=False,\n        )\n    except ObjectDoesNotExist:\n        return HttpResponse(\"resource not found\", status=status.HTTP_404_NOT_FOUND)\n    except PermissionDenied:\n        return HttpResponse(\n            \"access not authorized\", status=status.HTTP_401_UNAUTHORIZED\n        )\n\n    path_split = request.path.split(\"/\")[2:]  # strip /resource/\n    public_file_path = \"/\".join(path_split)\n    public_file_path = public_file_path.strip(\"/\")\n\n    zipped = True if request.GET.get(\"zipped\", \"false\").lower() == \"true\" else False\n    aggregation = (\n        True if request.GET.get(\"aggregation\", \"false\").lower() == \"true\" else False\n    )\n\n    istorage = res.get_s3_storage()\n    if istorage.isDir(public_file_path):\n        zipped = True\n    aggregation_name = None\n    if aggregation:\n        aggregation_name = filepath\n        zipped = True\n    if zipped:\n        zip_path = f\"tmp/{uuid.uuid4()}.zip\"\n        user_id = get_task_user_id(request)\n        is_single_file = istorage.exists(public_file_path)\n        task = create_temp_zip.apply_async((res.short_id, public_file_path, zip_path, aggregation_name, is_single_file))\n        api_request = request.META.get('CSRF_COOKIE', None) is None\n        if api_request:\n            return JsonResponse({\n                'zip_status': 'Not ready',\n                'task_id': task.task_id,\n                'download_path': zip_path})\n        else:\n            # return status to the task notification App AJAX call\n            task_dict = get_or_create_task_notification(\n                task.task_id, name='zip download', payload=zip_path, username=user_id)\n            return JsonResponse(task_dict)\n    return HttpResponseRedirect(\n        istorage.url(public_file_path)\n    )\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.get_manage_access_data","title":"<code>get_manage_access_data(request, shortkey)</code>","text":"<p>API endpoint to fetch manage access modal data on demand. This reduces the initial page load queries by deferring this data until needed.</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def get_manage_access_data(request, shortkey):\n    \"\"\"\n    API endpoint to fetch manage access modal data on demand.\n    This reduces the initial page load queries by deferring this data until needed.\n    \"\"\"\n    from hs_core.models import get_access_object\n\n    try:\n        resource, _, user = authorize(\n            request, shortkey, needed_permission=ACTION_TO_AUTHORIZE.VIEW_RESOURCE\n        )\n    except ObjectDoesNotExist:\n        return JsonResponse(\n            {\"status\": \"error\", \"message\": f\"No resource was found for id:{shortkey}\"},\n            status=404\n        )\n    except PermissionDenied:\n        return JsonResponse(\n            {\"status\": \"error\", \"message\": \"You don't have permission for this resource\"},\n            status=403\n        )\n\n    can_change_resource_flags = False\n    self_access_level = None\n    if request.user.is_authenticated:\n        if request.user.uaccess.can_change_resource_flags(resource):\n            can_change_resource_flags = True\n\n        # this will get resource access privilege even for admin user\n        user_privilege = resource.raccess.get_effective_user_privilege(request.user)\n        if user_privilege == PrivilegeCodes.OWNER:\n            self_access_level = 'owner'\n        elif user_privilege == PrivilegeCodes.CHANGE:\n            self_access_level = 'edit'\n        elif user_privilege == PrivilegeCodes.VIEW:\n            self_access_level = 'view'\n\n    owners = resource.raccess.owners.all()\n    editors = resource.raccess.get_users_with_explicit_access(\n        PrivilegeCodes.CHANGE, include_group_granted_access=False\n    )\n    viewers = resource.raccess.get_users_with_explicit_access(\n        PrivilegeCodes.VIEW, include_group_granted_access=False\n    )\n    edit_groups = resource.raccess.edit_groups\n    view_groups = resource.raccess.view_groups.exclude(pk__in=edit_groups)\n\n    if request.user.is_authenticated:\n        # N+1 queries\n        for owner in owners:\n            owner.can_undo = request.user.uaccess.can_undo_share_resource_with_user(\n                resource, owner\n            )\n\n        for viewer in viewers:\n            viewer.can_undo = request.user.uaccess.can_undo_share_resource_with_user(\n                resource, viewer\n            )\n\n        for editor in editors:\n            editor.can_undo = request.user.uaccess.can_undo_share_resource_with_user(\n                resource, editor\n            )\n\n        for view_grp in view_groups:\n            view_grp.can_undo = request.user.uaccess.can_undo_share_resource_with_group(\n                resource, view_grp\n            )\n\n        for edit_grp in edit_groups:\n            edit_grp.can_undo = request.user.uaccess.can_undo_share_resource_with_group(\n                resource, edit_grp\n            )\n    else:\n        for owner in owners:\n            owner.can_undo = False\n        for viewer in viewers:\n            viewer.can_undo = False\n        for editor in editors:\n            editor.can_undo = False\n        for view_grp in view_groups:\n            view_grp.can_undo = False\n        for edit_grp in edit_groups:\n            edit_grp.can_undo = False\n\n    users_json = []\n\n    for usr in owners:\n        users_json.append(get_access_object(usr, \"user\", \"owner\"))\n\n    for usr in editors:\n        users_json.append(get_access_object(usr, \"user\", \"edit\"))\n\n    for usr in viewers:\n        users_json.append(get_access_object(usr, \"user\", \"view\"))\n\n    for usr in edit_groups:\n        users_json.append(get_access_object(usr, \"group\", \"edit\"))\n\n    for usr in view_groups:\n        users_json.append(get_access_object(usr, \"group\", \"view\"))\n\n    data = {\n        \"status\": \"success\",\n        \"users_json\": users_json,\n        \"current_user_id\": request.user.id if request.user.is_authenticated else None,\n        \"self_access_level\": self_access_level,\n        \"quota_holder_pk\": resource.quota_holder.pk,\n        \"can_change_resource_flags\": can_change_resource_flags,\n        \"can_be_public_or_discoverable\": resource.can_be_public_or_discoverable,\n        \"resource_access\": {\n            \"isPublic\": resource.raccess.public,\n            \"isDiscoverable\": resource.raccess.discoverable,\n            \"isShareable\": resource.raccess.shareable,\n            \"isPrivateLinkSharing\": resource.raccess.allow_private_sharing,\n        },\n    }\n\n    return JsonResponse(data)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.get_resource_metadata","title":"<code>get_resource_metadata(request, shortkey, *args, **kwargs)</code>","text":"<p>Returns resource level metadata that is needed to update UI Only the following resource level metadata is returned for now: title abstract keywords creators spatial coverage temporal coverage</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def get_resource_metadata(request, shortkey, *args, **kwargs):\n    \"\"\"Returns resource level metadata that is needed to update UI\n    Only the following resource level metadata is returned for now:\n    title\n    abstract\n    keywords\n    creators\n    spatial coverage\n    temporal coverage\n    \"\"\"\n    resource, _, _ = authorize(\n        request, shortkey, needed_permission=ACTION_TO_AUTHORIZE.VIEW_RESOURCE\n    )\n    res_metadata = dict()\n    res_metadata[\"title\"] = resource.metadata.title.value\n    description = resource.metadata.description\n    if description:\n        res_metadata[\"abstract\"] = description.abstract\n    else:\n        res_metadata[\"abstract\"] = None\n    creators = []\n    for creator in resource.metadata.creators.all():\n        creators.append(model_to_dict(creator))\n    res_metadata[\"creators\"] = creators\n    res_metadata[\"keywords\"] = [sub.value for sub in resource.metadata.subjects.all()]\n    res_metadata[\"spatial_coverage\"] = get_coverage_data_dict(resource)\n    res_metadata[\"temporal_coverage\"] = get_coverage_data_dict(\n        resource, coverage_type=\"temporal\"\n    )\n    return JsonResponse(res_metadata, status=200)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.get_user_or_group_data","title":"<code>get_user_or_group_data(request, user_or_group_id, is_group, *args, **kwargs)</code>","text":"<p>This view function must be called as an AJAX call</p> <p>:param user_or_group_id: id of the user or group for which data is needed :param is_group : (string) 'false' if the id is for a group, 'true' if id is for a user :return: JsonResponse() containing user data</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@login_required\ndef get_user_or_group_data(request, user_or_group_id, is_group, *args, **kwargs):\n    \"\"\"\n    This view function must be called as an AJAX call\n\n    :param user_or_group_id: id of the user or group for which data is needed\n    :param is_group : (string) 'false' if the id is for a group, 'true' if id is for a user\n    :return: JsonResponse() containing user data\n    \"\"\"\n    user_data = {}\n    if is_group == \"false\":\n        user = hydroshare.utils.user_from_id(user_or_group_id)\n        user_data[\"name\"] = hydroshare.utils.get_user_party_name(user)\n        user_data[\"email\"] = user.email\n        user_data[\"url\"] = \"{domain}/user/{uid}/\".format(\n            domain=hydroshare.utils.current_site_url(), uid=user.pk\n        )\n        if user.userprofile.phone_1:\n            user_data[\"phone\"] = user.userprofile.phone_1\n        elif user.userprofile.phone_2:\n            user_data[\"phone\"] = user.userprofile.phone_2\n        else:\n            user_data[\"phone\"] = \"\"\n\n        address = \"\"\n        if user.userprofile.state and user.userprofile.state.lower() != \"unspecified\":\n            address = user.userprofile.state\n        if (\n            user.userprofile.country\n            and user.userprofile.country.lower() != \"unspecified\"\n        ):\n            if len(address) &gt; 0:\n                address += \", \" + user.userprofile.country\n            else:\n                address = user.userprofile.country\n\n        user_data[\"address\"] = address\n        user_data[\"organization\"] = (\n            user.userprofile.organization if user.userprofile.organization else \"\"\n        )\n        user_data[\"website\"] = (\n            user.userprofile.website if user.userprofile.website else \"\"\n        )\n        user_data[\"identifiers\"] = user.userprofile.identifiers\n        user_data[\"type\"] = user.userprofile.user_type\n        user_data[\"date_joined\"] = user.date_joined\n        user_data[\"subject_areas\"] = user.userprofile.subject_areas\n        if user.userprofile.state:\n            user_data[\"state\"] = user.userprofile.state\n        if user.userprofile.country:\n            user_data[\"country\"] = user.userprofile.country\n    else:\n        group = hydroshare.utils.group_from_id(user_or_group_id)\n        user_data[\"organization\"] = group.name\n        user_data[\"url\"] = \"{domain}/user/{uid}/\".format(\n            domain=hydroshare.utils.current_site_url(), uid=group.pk\n        )\n        user_data[\"description\"] = group.gaccess.description\n\n    return JsonResponse(user_data)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.group_membership","title":"<code>group_membership(request, uidb36, token, membership_request_id, **kwargs)</code>","text":"<p>View for the link in the verification email that was sent to a user when they request/invite to join a group. User is logged in and the request to join a group is accepted. Then the user is redirected to the group profile page of the group for which the membership got accepted.</p> <p>:param uidb36: ID of the user to whom the email was sent (part of the link in the email) :param token: token that was part of the link in the email :param membership_request_id: ID of the GroupMembershipRequest object (part of the link in the email)</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def group_membership(request, uidb36, token, membership_request_id, **kwargs):\n    \"\"\"\n    View for the link in the verification email that was sent to a user\n    when they request/invite to join a group.\n    User is logged in and the request to join a group is accepted. Then the user is redirected to the group\n    profile page of the group for which the membership got accepted.\n\n    :param uidb36: ID of the user to whom the email was sent (part of the link in the email)\n    :param token: token that was part of the link in the email\n    :param membership_request_id: ID of the GroupMembershipRequest object (part of the link in the email)\n    \"\"\"\n    membership_request = GroupMembershipRequest.objects.filter(\n        id=membership_request_id\n    ).first()\n    if membership_request is None:\n        messages.error(request, \"This group membership link is not valid\")\n    elif membership_request.redeemed:\n        messages.error(\n            request, \"This group membership link has previously been redeemed\"\n        )\n    elif not membership_request.group_to_join.gaccess.active:\n        messages.error(\n            request,\n            \"The group associated with this group membership link is no longer active.\",\n        )\n    else:\n        user = authenticate(uidb36=uidb36, token=token, is_active=True)\n        if user is None:\n            messages.error(\n                request,\n                \"The link you clicked has expired. Please ask to \"\n                \"join the group again.\",\n            )\n        else:\n            user.uaccess.act_on_group_membership_request(\n                membership_request, accept_request=True\n            )\n            auth_login(request, user)\n            # send email to notify membership acceptance\n            _send_email_on_group_membership_acceptance(membership_request)\n            if membership_request.invitation_to is not None:\n                message = \"You just joined the group '{}'\".format(\n                    membership_request.group_to_join.name\n                )\n            else:\n                message = \"User '{}' just joined the group '{}'\".format(\n                    membership_request.request_from.first_name,\n                    membership_request.group_to_join.name,\n                )\n\n            messages.info(request, message)\n            # redirect to group profile page\n            return HttpResponseRedirect(\n                \"/group/{}/\".format(membership_request.group_to_join.id)\n            )\n    return redirect(\"/\")\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.hsapi_get_user","title":"<code>hsapi_get_user(request, user_identifier)</code>","text":"<p>Get user data</p> <p>:param user_identifier: id of the user for which data is needed :return: JsonResponse containing user data</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@swagger_auto_schema(\n    method=\"get\",\n    operation_description=\"Get user data\",\n    responses={200: \"Returns JsonResponse containing user data\"},\n    manual_parameters=[uid],\n)\n@api_view([\"GET\"])\ndef hsapi_get_user(request, user_identifier):\n    \"\"\"\n    Get user data\n\n    :param user_identifier: id of the user for which data is needed\n    :return: JsonResponse containing user data\n    \"\"\"\n    return get_user_or_group_data(request, user_identifier, \"false\")\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.hsapi_get_user_for_keycloak","title":"<code>hsapi_get_user_for_keycloak(request, user_identifier)</code>","text":"<p>Get user data</p> <p>:param user_identifier: id of the user for which data is needed :return: JsonResponse containing user data</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@swagger_auto_schema(\n    method=\"post\",\n    operation_description=\"Check user password for Keycloak Migration\",\n    responses={200: \"Password is valid\", 400: \"Password is invalid\"},\n    manual_parameters=[uid],\n    request_body=openapi.Schema(type=openapi.TYPE_OBJECT,\n                                properties={'password': openapi.Schema(type=openapi.TYPE_STRING,\n                                                                       description=\"raw password to validate\")}\n                                )\n)\n@swagger_auto_schema(\n    method=\"get\",\n    operation_description=\"Get user data for Keycloak Migration\",\n    responses={200: \"Returns JsonResponse containing user data\"},\n    manual_parameters=[uid],\n)\n@api_view([\"GET\", \"POST\"])\ndef hsapi_get_user_for_keycloak(request, user_identifier):\n    \"\"\"\n    Get user data\n\n    :param user_identifier: id of the user for which data is needed\n    :return: JsonResponse containing user data\n    \"\"\"\n\n    if not request.user.is_superuser:\n        msg = {\"message\": \"Unauthorized\"}\n        return JsonResponse(msg, status=401)\n\n    if request.method == \"POST\":\n        return hsapi_post_user_for_keycloak(request, user_identifier)\n\n    user: User = hydroshare.utils.user_from_id(user_identifier)\n    user_profile = UserProfile.objects.filter(user=user).first()\n    user_attributes = {}\n    if user_profile.organization:\n        user_attributes['cuahsi-organizations'] = [org for org in user_profile.organization.split(\";\")]\n    if user_profile.state and user_profile.state.strip() and user_profile.state != 'Unspecified':\n        user_attributes['cuahsi-region'] = [user_profile.state.strip()]\n    if user_profile.country and user_profile.country != 'Unspecified':\n        user_attributes['cuahsi-country'] = [user_profile.country]\n    if user_profile.user_type and user_profile.user_type.strip() and user_profile.user_type != 'Unspecified':\n        user_attributes['cuahsi-user-type'] = [user_profile.user_type.strip()]\n    if user_profile.subject_areas:\n        user_attributes['cuahsi-subject-areas'] = [subject for subject in user_profile.subject_areas]\n    keycloak_dict = {\n        \"username\": user.username,\n        \"email\": user.email,\n        \"firstName\": user.first_name,\n        \"lastName\": user.last_name,\n        \"enabled\": True,\n        \"emailVerified\": user.is_active,\n        \"attributes\": user_attributes,\n        \"roles\": [\n            \"hydroshare-imported\"\n        ],\n    }\n    return JsonResponse(keycloak_dict)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.hsapi_post_user_for_keycloak","title":"<code>hsapi_post_user_for_keycloak(request, user_identifier)</code>","text":"<p>Check the user password</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def hsapi_post_user_for_keycloak(request, user_identifier):\n    \"\"\"\n    Check the user password\n    \"\"\"\n    password = json.loads(request.body.decode('utf-8'))['password']\n    user: User = hydroshare.utils.user_from_id(user_identifier)\n    return HttpResponse() if user.check_password(password) else HttpResponseBadRequest()\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.make_group_membership_request","title":"<code>make_group_membership_request(request, group_id, user_id=None, *args, **kwargs)</code>","text":"<p>Allows either an owner of the group to invite a user to join a group or a user to make a request to join a group :param request: the user who is making the request :param group_id: ID of the group for which the join request/invitation to me made :param user_id: needed only when an owner is inviting a user to join a group. This is the id of the user the owner                 is inviting :return:</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@login_required\ndef make_group_membership_request(request, group_id, user_id=None, *args, **kwargs):\n    \"\"\"\n    Allows either an owner of the group to invite a user to join a group or a user to make a request\n    to join a group\n    :param request: the user who is making the request\n    :param group_id: ID of the group for which the join request/invitation to me made\n    :param user_id: needed only when an owner is inviting a user to join a group. This is the id of the user the owner\n                    is inviting\n    :return:\n    \"\"\"\n    requesting_user = request.user\n    group_to_join = hydroshare.utils.group_from_id(group_id)\n    user_to_join = None\n    if request.method == \"POST\":\n        explanation = request.POST.get(\"explanation\", None)\n    else:\n        explanation = None\n    if user_id is not None:\n        user_to_join = hydroshare.utils.user_from_id(user_id)\n    try:\n        membership_request = requesting_user.uaccess.create_group_membership_request(\n            group_to_join, user_to_join, explanation=explanation\n        )\n        if user_to_join is not None:\n            message = \"Group membership invitation was successful\"\n            # send mail to the user who was invited to join group\n\n            send_action_to_take_email(\n                request,\n                user=user_to_join,\n                action_type=\"group_membership\",\n                group=group_to_join,\n                membership_request=membership_request,\n                explanation=explanation,\n            )\n        else:\n            message = \"You are now a member of this group\"\n            # membership_request is None in case where group allows auto approval of membership\n            # request. no need send email notification to group owners for membership approval\n            if membership_request is not None:\n                message = \"Group membership request was successful\"\n                # send mail to all owners of the group for approval of the request\n                for grp_owner in group_to_join.gaccess.owners:\n                    send_action_to_take_email(\n                        request,\n                        user=requesting_user,\n                        action_type=\"group_membership\",\n                        group=group_to_join,\n                        group_owner=grp_owner,\n                        membership_request=membership_request,\n                        explanation=explanation,\n                    )\n            else:\n                # send mail to all owners of the group to let them know that someone has\n                # joined this group\n                for grp_owner in group_to_join.gaccess.owners:\n                    send_action_to_take_email(\n                        request,\n                        user=requesting_user,\n                        action_type=\"group_auto_membership\",\n                        group=group_to_join,\n                        group_owner=grp_owner,\n                        explanation=explanation,\n                    )\n        return JsonResponse({\n            \"status\": \"success\",\n            \"message\": message\n        })\n    except PermissionDenied as ex:\n        return JsonResponse({\n            \"status\": \"error\",\n            \"message\": str(ex),\n        })\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.metadata_review","title":"<code>metadata_review(request, shortkey, action, uidb64=None, token=None, **kwargs)</code>","text":"<p>View for the link in the verification email that was sent to a user when they request publication/metadata review. User is logged in and the request for review is approved. Then the user is redirected to the resource landing page for the resource that they just approved.</p> <p>:param uidb36: ID of the user to whom the email was sent (part of the link in the email) :param token: token that was part of the link in the email</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def metadata_review(request, shortkey, action, uidb64=None, token=None, **kwargs):\n    \"\"\"\n    View for the link in the verification email that was sent to a user\n    when they request publication/metadata review.\n    User is logged in and the request for review is approved. Then the user is redirected to the resource landing page\n    for the resource that they just approved.\n\n    :param uidb36: ID of the user to whom the email was sent (part of the link in the email)\n    :param token: token that was part of the link in the email\n    \"\"\"\n    if uidb64:\n        user = authenticate(uidb64=uidb64, token=token, is_active=True)\n        if user is None:\n            messages.error(\n                request,\n                \"The link you clicked has expired. Please manually navigate to the resouce \"\n                \"to complete the metadata review.\",\n            )\n    else:\n        user = request.user\n\n    res = hydroshare.utils.get_resource_by_shortkey(shortkey)\n    if not res.raccess.review_pending:\n        messages.error(\n            request,\n            f\"This resource does not have a pending metadata review for you to {action}.\",\n        )\n    else:\n        res.raccess.alter_review_pending_flags(initiating_review=False)\n        if action == \"approve\":\n            hydroshare.publish_resource(user, shortkey)\n            messages.success(\n                request,\n                \"Publication request was accepted. \"\n                \"An email will be sent notifiying the resource owner(s) once the DOI activates.\",\n            )\n        else:\n            messages.warning(\n                request,\n                \"Publication request was rejected. Please send an email to the resource owner indicating why.\",\n            )\n        res.metadata.dates.all().filter(type=\"reviewStarted\").delete()\n    return HttpResponseRedirect(f\"/resource/{res.short_id}/\")\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.my_resources","title":"<code>my_resources(request, *args, **kwargs)</code>","text":"<p>View for listing resources that belong to a given user.</p> <p>Renders either a full my-resources page, or just a table of new resources</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@login_required\ndef my_resources(request, *args, **kwargs):\n    \"\"\"\n    View for listing resources that belong to a given user.\n\n    Renders either a full my-resources page, or just a table of new resources\n    \"\"\"\n\n    if not is_ajax(request):\n        filter = request.GET.getlist(\"f\", default=[])\n    else:\n        filter = [request.GET[\"new_filter\"]]\n    u = User.objects.select_related(\"uaccess\").get(pk=request.user.id)\n\n    if not filter:\n        # add default filters\n        filter = [\"owned\", \"discovered\", \"favorites\"]\n\n    if \"shared\" in filter:\n        filter.remove(\"shared\")\n        filter.append(\"viewable\")\n        filter.append(\"editable\")\n\n    resource_collection = get_my_resources_list(u, annotate=True, filter=filter)\n\n    context = {\"collection\": resource_collection}\n\n    if not is_ajax(request):\n        return render(request, \"pages/my-resources.html\", context)\n    else:\n        from django.template.loader import render_to_string\n\n        trows = render_to_string(\"includes/my-resources-trows.html\", context, request)\n        return JsonResponse(\n            {\n                \"trows\": trows,\n            }\n        )\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.my_resources_filter_counts","title":"<code>my_resources_filter_counts(request, *args, **kwargs)</code>","text":"<p>View for counting resources that belong to a given user.</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@login_required\ndef my_resources_filter_counts(request, *args, **kwargs):\n    \"\"\"\n    View for counting resources that belong to a given user.\n    \"\"\"\n    _ = request.GET.getlist(\"filter\", default=None)\n    u = User.objects.select_related(\"uaccess\").get(pk=request.user.id)\n\n    filter_counts = get_my_resources_filter_counts(u)\n\n    return JsonResponse({\"filter_counts\": filter_counts})\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.request_new_community","title":"<code>request_new_community(request, *args, **kwargs)</code>","text":"<p>A view function to server request for creating a new community</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@login_required\ndef request_new_community(request, *args, **kwargs):\n    \"\"\" A view function to server request for creating a new community \"\"\"\n\n    community_form = RequestNewCommunityForm(request.POST, request.FILES)\n    if community_form.is_valid():\n        try:\n            new_community_request = community_form.save(request)\n            new_community_name = new_community_request.community_to_approve.name\n            msg = f\"New community ({new_community_name}) request was successful.\"\n            messages.success(request, msg)\n            # send email to hydroshare support\n            CommunityRequestEmailNotification(request=request, community_request=new_community_request,\n                                              on_event=CommunityRequestEvents.CREATED).send()\n            return HttpResponseRedirect(reverse('my_communities'))\n        except PermissionDenied:\n            err_msg = \"You don't have permission to request new community\"\n            messages.error(request, err_msg)\n        except Exception as ex:\n            messages.error(request, f\"Community request errors:{str(ex)}.\")\n\n    else:\n        messages.error(request, \"Community request errors:{}.\".format(community_form.errors.as_json))\n\n    return HttpResponseRedirect(request.headers[\"referer\"])\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.restore_user_group","title":"<code>restore_user_group(request, group_id, *args, **kwargs)</code>","text":"<p>This one is for setting the active status of the group back to True</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@login_required\ndef restore_user_group(request, group_id, *args, **kwargs):\n    \"\"\"This one is for setting the active status of the group back to True\"\"\"\n    try:\n        hydroshare.set_group_active_status(request.user, group_id, True)\n        messages.success(request, \"Group restore was successful.\")\n    except PermissionDenied:\n        messages.error(\n            request,\n            \"Group restore errors: You don't have permission to restore\" \" this group.\",\n        )\n\n    return HttpResponseRedirect(request.headers[\"referer\"])\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.set_resource_flag_public","title":"<code>set_resource_flag_public(request, pk)</code>","text":"<p>Set resource flag to \"Public\"</p> <p>:param request: :param pk: id of the resource to be modified :return: HttpResponse with status code</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@swagger_auto_schema(\n    method=\"post\",\n    operation_description=\"Set resource flag to 'Public'\",\n    responses={202: \"Returns the resource ID of the new version\"},\n    manual_parameters=[res_id],\n)\n@api_view([\"POST\"])\ndef set_resource_flag_public(request, pk):\n    \"\"\"\n    Set resource flag to \"Public\"\n\n    :param request:\n    :param pk: id of the resource to be modified\n    :return: HttpResponse with status code\n    \"\"\"\n    http_request = request._request\n    http_request.data = request.data.copy()\n    js_response = set_resource_flag(http_request, pk)\n    data = json.loads(js_response.content)\n    if data[\"status\"] == \"error\":\n        return HttpResponse(data[\"message\"], status=400)\n    return HttpResponse(status=202)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.share_resource_with_group","title":"<code>share_resource_with_group(request, shortkey, privilege, group_id, *args, **kwargs)</code>","text":"<p>this view function is expected to be called by ajax</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def share_resource_with_group(request, shortkey, privilege, group_id, *args, **kwargs):\n    \"\"\"this view function is expected to be called by ajax\"\"\"\n    return _share_resource(\n        request, shortkey, privilege, group_id, user_or_group=\"group\"\n    )\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.share_resource_with_user","title":"<code>share_resource_with_user(request, shortkey, privilege, user_id, *args, **kwargs)</code>","text":"<p>this view function is expected to be called by ajax</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def share_resource_with_user(request, shortkey, privilege, user_id, *args, **kwargs):\n    \"\"\"this view function is expected to be called by ajax\"\"\"\n    return _share_resource(request, shortkey, privilege, user_id, user_or_group=\"user\")\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.spam_allowlist","title":"<code>spam_allowlist(request, shortkey, action, **kwargs)</code>","text":"<p>Allow a resource to be discoverable even if it matches spam patterns</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def spam_allowlist(request, shortkey, action, **kwargs):\n    \"\"\"\n    Allow a resource to be discoverable even if it matches spam patterns\n    \"\"\"\n    user = request.user\n    if not user.is_superuser:\n        return HttpResponseForbidden(\n            \"not authorized to perform this action\"\n        )\n\n    res = hydroshare.utils.get_resource_by_shortkey(shortkey)\n    if action == \"allow\":\n        res.spam_allowlisted = True\n        res.save()\n        messages.success(\n            request,\n            \"Resource has been allowlisted. \"\n            \"This means that it can show in Discover even if it contains spam patterns.\",\n        )\n    else:\n        res.spam_allowlisted = False\n        res.save()\n        messages.warning(\n            request,\n            \"Resource has been removed from allowlist. \"\n            \"It will not show in Discover if it contains spam patterns.\",\n        )\n    # update the index\n    signals.post_spam_whitelist_change.send(sender=BaseResource, instance=res)\n    return HttpResponseRedirect(f\"/resource/{res.short_id}/\")\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.undo_share_resource_with_group","title":"<code>undo_share_resource_with_group(request, shortkey, group_id, *args, **kwargs)</code>","text":"<p>this view function is expected to be called by ajax</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def undo_share_resource_with_group(request, shortkey, group_id, *args, **kwargs):\n    \"\"\"this view function is expected to be called by ajax\"\"\"\n\n    res, _, user = authorize(\n        request, shortkey, needed_permission=ACTION_TO_AUTHORIZE.VIEW_RESOURCE\n    )\n    group_to_unshare_with = hydroshare.utils.group_from_id(group_id)\n    ajax_response_data = {\"status\": \"success\"}\n    try:\n        user.uaccess.undo_share_resource_with_group(res, group_to_unshare_with)\n        if group_to_unshare_with in res.raccess.edit_groups:\n            undo_group_privilege = \"edit\"\n        elif group_to_unshare_with in res.raccess.view_groups:\n            undo_group_privilege = \"view\"\n        else:\n            undo_group_privilege = \"none\"\n        ajax_response_data[\"undo_group_privilege\"] = undo_group_privilege\n\n        if user not in res.raccess.view_users:\n            # user has no explicit access to the resource - redirect to resource listing page\n            ajax_response_data[\"redirect_to\"] = \"/my-resources/\"\n    except PermissionDenied as exp:\n        ajax_response_data[\"status\"] = \"error\"\n        ajax_response_data[\"message\"] = str(exp)\n\n    return JsonResponse(ajax_response_data)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.undo_share_resource_with_user","title":"<code>undo_share_resource_with_user(request, shortkey, user_id, *args, **kwargs)</code>","text":"<p>this view function is expected to be called by ajax</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def undo_share_resource_with_user(request, shortkey, user_id, *args, **kwargs):\n    \"\"\"this view function is expected to be called by ajax\"\"\"\n\n    res, _, user = authorize(\n        request, shortkey, needed_permission=ACTION_TO_AUTHORIZE.VIEW_RESOURCE\n    )\n    user_to_unshare_with = hydroshare.utils.user_from_id(user_id)\n    ajax_response_data = {\"status\": \"success\"}\n    try:\n        user.uaccess.undo_share_resource_with_user(res, user_to_unshare_with)\n        undo_user_privilege = res.raccess.get_effective_privilege(user_to_unshare_with)\n        if undo_user_privilege == PrivilegeCodes.VIEW:\n            undo_user_privilege = \"view\"\n        elif undo_user_privilege == PrivilegeCodes.CHANGE:\n            undo_user_privilege = \"edit\"\n        elif undo_user_privilege == PrivilegeCodes.OWNER:\n            undo_user_privilege = \"owner\"\n        else:\n            undo_user_privilege = \"none\"\n        ajax_response_data[\"undo_user_privilege\"] = undo_user_privilege\n\n        if user not in res.raccess.view_users:\n            # user has no explict access to the resource - redirect to resource listing page\n            ajax_response_data[\"redirect_to\"] = \"/my-resources/\"\n\n    except PermissionDenied as exp:\n        ajax_response_data[\"status\"] = \"error\"\n        ajax_response_data[\"message\"] = str(exp)\n\n    return JsonResponse(ajax_response_data)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.unshare_group_with_user","title":"<code>unshare_group_with_user(request, group_id, user_id, *args, **kwargs)</code>","text":"<p>Remove a user from a group</p> <p>:param request: group owner who is removing the user from the group :param group_id: id of the user being removed from the group :param user_id: id of the group from which the user to be removed :return:</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@login_required\ndef unshare_group_with_user(request, group_id, user_id, *args, **kwargs):\n    \"\"\"\n    Remove a user from a group\n\n    :param request: group owner who is removing the user from the group\n    :param group_id: id of the user being removed from the group\n    :param user_id: id of the group from which the user to be removed\n    :return:\n    \"\"\"\n    requesting_user = request.user\n    group_to_unshare = hydroshare.utils.group_from_id(group_id)\n    user_to_unshare_with = hydroshare.utils.user_from_id(user_id)\n\n    try:\n        requesting_user.uaccess.unshare_group_with_user(\n            group_to_unshare, user_to_unshare_with\n        )\n        if requesting_user == user_to_unshare_with:\n            success_msg = \"You successfully left the group.\"\n        else:\n            success_msg = \"User successfully removed from the group.\"\n        messages.success(request, success_msg)\n    except PermissionDenied as ex:\n        messages.error(request, str(ex))\n\n    if requesting_user == user_to_unshare_with:\n        return HttpResponseRedirect(reverse(\"my_groups\"))\n    else:\n        return HttpResponseRedirect(request.headers[\"referer\"])\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.unshare_resource_with_group","title":"<code>unshare_resource_with_group(request, shortkey, group_id, *args, **kwargs)</code>","text":"<p>this view function is expected to be called by ajax</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def unshare_resource_with_group(request, shortkey, group_id, *args, **kwargs):\n    \"\"\"this view function is expected to be called by ajax\"\"\"\n\n    res, _, user = authorize(\n        request, shortkey, needed_permission=ACTION_TO_AUTHORIZE.VIEW_RESOURCE\n    )\n    group_to_unshare_with = hydroshare.utils.group_from_id(group_id)\n    ajax_response_data = {\"status\": \"success\"}\n    try:\n        user.uaccess.unshare_resource_with_group(res, group_to_unshare_with)\n        if user not in res.raccess.view_users:\n            # user has no explicit access to the resource - redirect to resource listing page\n            ajax_response_data[\"redirect_to\"] = \"/my-resources/\"\n    except PermissionDenied as exp:\n        ajax_response_data[\"status\"] = \"error\"\n        ajax_response_data[\"message\"] = str(exp)\n\n    return JsonResponse(ajax_response_data)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.unshare_resource_with_user","title":"<code>unshare_resource_with_user(request, shortkey, user_id, *args, **kwargs)</code>","text":"<p>this view function is expected to be called by ajax</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def unshare_resource_with_user(request, shortkey, user_id, *args, **kwargs):\n    \"\"\"this view function is expected to be called by ajax\"\"\"\n\n    res, _, user = authorize(\n        request, shortkey, needed_permission=ACTION_TO_AUTHORIZE.VIEW_RESOURCE\n    )\n    user_to_unshare_with = hydroshare.utils.user_from_id(user_id)\n    ajax_response_data = {\"status\": \"success\"}\n    try:\n        user.uaccess.unshare_resource_with_user(res, user_to_unshare_with)\n        if user not in res.raccess.view_users:\n            # user has no explict access to the resource - redirect to resource listing page\n            ajax_response_data[\"redirect_to\"] = \"/my-resources/\"\n\n    except PermissionDenied as exp:\n        ajax_response_data[\"status\"] = \"error\"\n        ajax_response_data[\"message\"] = str(exp)\n\n    return JsonResponse(ajax_response_data)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.update_key_value_metadata","title":"<code>update_key_value_metadata(request, shortkey, *args, **kwargs)</code>","text":"<p>This one view function is for CRUD operation for resource key/value arbitrary metadata. key/value data in request.POST is assigned to the resource.extra_metadata field</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>def update_key_value_metadata(request, shortkey, *args, **kwargs):\n    \"\"\"\n    This one view function is for CRUD operation for resource key/value arbitrary metadata.\n    key/value data in request.POST is assigned to the resource.extra_metadata field\n    \"\"\"\n    res, _, _ = authorize(\n        request, shortkey, needed_permission=ACTION_TO_AUTHORIZE.EDIT_RESOURCE\n    )\n    post_data = request.POST.copy()\n    resource_mode = post_data.pop(\"resource-mode\", None)\n    extra_metadata = post_data.dict()\n    extra_metadata_copy = extra_metadata.copy()\n    for key in extra_metadata_copy:\n        if not key:\n            extra_metadata.pop(key)\n\n    res.extra_metadata = extra_metadata\n    is_update_success = True\n    err_message = \"\"\n    try:\n        res.save()\n    except Error as ex:\n        is_update_success = False\n        err_message = str(ex)\n\n    if is_update_success:\n        resource_modified(res, request.user, overwrite_bag=False)\n        res_metadata = res.metadata\n        res_metadata.set_dirty(True)\n\n    if is_ajax(request):\n        if is_update_success:\n            ajax_response_data = {\n                \"status\": \"success\",\n                \"is_dirty\": res.metadata.is_dirty\n                if hasattr(res.metadata, \"is_dirty\")\n                else False,\n            }\n        else:\n            ajax_response_data = {\"status\": \"error\", \"message\": err_message}\n        return HttpResponse(json.dumps(ajax_response_data))\n\n    if resource_mode is not None:\n        request.session[\"resource-mode\"] = \"edit\"\n\n    if is_update_success:\n        messages.success(request, \"Metadata update successful\")\n    else:\n        messages.error(request, err_message)\n\n    return HttpResponseRedirect(request.headers[\"referer\"])\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.update_key_value_metadata_public","title":"<code>update_key_value_metadata_public(request, id)</code>","text":"<p>Update resource key/value metadata pair</p> <p>Metadata to be updated should be included as key/value pairs in the REST request</p> <p>:param request: :param id: id of the resource to be updated :return: HttpResponse with status code</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@swagger_auto_schema(\n    method=\"get\",\n    operation_description=\"Gets all key/value metadata for the resource\",\n    responses={200: \"key/value metadata\"},\n    manual_parameters=[res_id],\n)\n@swagger_auto_schema(\n    method=\"post\",\n    operation_description=\"Updates key/value metadata for the resource\",\n    responses={200: \"\"},\n    manual_parameters=[res_id],\n)\n@api_view([\"POST\", \"GET\"])\ndef update_key_value_metadata_public(request, id):\n    \"\"\"\n    Update resource key/value metadata pair\n\n    Metadata to be updated should be included as key/value pairs in the REST request\n\n    :param request:\n    :param id: id of the resource to be updated\n    :return: HttpResponse with status code\n    \"\"\"\n    if request.method == \"GET\":\n        res, _, _ = authorize(\n            request, id, needed_permission=ACTION_TO_AUTHORIZE.VIEW_RESOURCE\n        )\n        return HttpResponse(status=200, content=json.dumps(res.extra_metadata))\n\n    res, _, _ = authorize(\n        request, id, needed_permission=ACTION_TO_AUTHORIZE.EDIT_RESOURCE\n    )\n\n    post_data = request.data.copy()\n    res.extra_metadata = post_data\n\n    is_update_success = True\n\n    try:\n        res.save()\n    except Error:\n        is_update_success = False\n\n    if is_update_success:\n        resource_modified(res, request.user, overwrite_bag=False)\n\n    if is_update_success:\n        return HttpResponse(status=200)\n    else:\n        return HttpResponse(status=400)\n</code></pre>"},{"location":"hs_core/views/#hs_core.views.update_user_community","title":"<code>update_user_community(request, community_id, *args, **kwargs)</code>","text":"<p>Updates metadata for a community</p> Source code in <code>hs_core/views/__init__.py</code> <pre><code>@login_required\ndef update_user_community(request, community_id, *args, **kwargs):\n    \"\"\"Updates metadata for a community\"\"\"\n\n    community_to_update = hydroshare.utils.community_from_id(community_id)\n\n    community_form = UpdateCommunityForm(request.POST, request.FILES)\n    if community_form.is_valid():\n        try:\n            community_form.update(community_to_update, request)\n            messages.success(request, \"Community update was successful.\")\n        except PermissionDenied:\n            err_msg = \"You don't have permission to update community\"\n            messages.error(request, err_msg)\n        except Exception as ex:\n            messages.error(request, f\"Community update errors:{str(ex)}.\")\n\n    else:\n        messages.error(request, \"Community update errors:{}.\".format(community_form.errors.as_json))\n\n    return HttpResponseRedirect(request.headers[\"referer\"])\n</code></pre>"}]}